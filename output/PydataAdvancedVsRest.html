<!-- some code adapted from www.degeneratestate.org/static/metal_lyrics/metal_line.html -->
<!-- <!DOCTYPE html>
<meta content="utf-8"> -->
<style> /* set the CSS */

body {
  font: 12px Arial;
}


svg {
  font: 12px Helvetica;
}

path {
  stroke: steelblue;
  stroke-width: 2;
  fill: none;
}

.axis path,
.axis lineper {
  fill: none;
  stroke: grey;
  stroke-width: 1;
  shape-rendering: crispEdges;
}

div.tooltip {
  position: absolute;
  text-align: center;
  width: 150px;
  height: 28px;
  padding: 2px;
  font: 12px sans-serif;
  background: lightsteelblue;
  border: 0px;
  border-radius: 8px;
  pointer-events: none;
}

div.tooltipscore {
  position: absolute;
  text-align: center;
  width: 150px;
  height: 42px;
  padding: 2px;
  font: 10px sans-serif;
  background: lightsteelblue;
  border: 0px;
  border-radius: 8px;
  pointer-events: none;
}


.category_header {
  font: 12px sans-serif;
  font-weight: bolder;
  text-decoration: underline;
}

div.label {
  color: rgb(252, 251, 253);
  color: rgb(63, 0, 125);
  color: rgb(158, 155, 201);

  position: absolute;
  text-align: left;
  padding: 1px;
  border-spacing: 1px;
  font: 10px sans-serif;
  font-family: Sans-Serif;
  border: 0;
  pointer-events: none;
}

input {
  border: 1px dotted #ccc;
  background: white;
  font-family: monospace;
  padding: 10px 20px;
  font-size: 14px;
  margin: 20px 10px 30px 0;
  color: darkred;
}

.alert {
  font-family: monospace;
  padding: 10px 20px;
  font-size: 14px;
  margin: 20px 10px 30px 0;
  color: darkred;
}

ul.top_terms li {
  padding-right: 20px;
  font-size: 30pt;
  color: red;
}

input:focus {
  background-color: lightyellow;
  outline: none;
}

.snippet {
  padding-bottom: 10px;
  padding-left: 5px;
  padding-right: 5px;
  white-space: pre-wrap;
}

.snippet_header {
  font-size: 20px;
  font-family: Helvetica, Arial, Sans-Serif;
  font-weight: bolder;
  #text-decoration: underline;
  text-align: center;
  border-bottom-width: 10px;
  border-bottom-color: #888888;
  padding-bottom: 10px;
}


#title-div {
  font-size: 20px;
  font-family: Helvetica, Arial, Sans-Serif;
  text-align: center;
}


.text_header {
  font: 18px sans-serif;
  font-size: 18px;
  font-family: Helvetica, Arial, Sans-Serif;

  font-weight: bolder;
  text-decoration: underline;
  text-align: center;
  color: darkblue;
  padding-bottom: 10px;
}

.text_subheader {
  font-size: 14px;
  font-family: Helvetica, Arial, Sans-Serif;

  text-align: center;
}


.snippet_meta {
  border-top: 3px solid #4588ba;
  font-size: 12px;
  font-family: Helvetica, Arial, Sans-Serif;
color: darkblue;
}

.left_contexts {
  width: 45%;
  float: left;
}

.right_contexts {
  width: 45%;
  float: left;
}

.scattertext {
  font-size: 10px;
  font-family: Helvetica, Arial, Sans-Serif;
}

.label {
  font-size: 10px;
  font-family: Helvetica, Arial, Sans-Serif;
}


.small_label {
  font-size: 10px;
}

#corpus-stats {
  text-align: center;
}

#cat {
}

#notcat {
}

</style>
<script src="https://cdnjs.cloudflare.com/ajax/libs/d3/4.6.0/d3.min.js" charset="utf-8"></script>
<script src="https://d3js.org/d3-scale-chromatic.v1.min.js" charset="utf-8"></script>


<span id="title-div"></span>
<div class="scattertext" id="d3-div-1"> </div>
<div id="corpus-stats"> </div>
<form name="termForm" onSubmit="handleSearch(); return false;">
  <input name="Submit" type="submit" value="Search for term">
  <input type="text" id="searchTerm" placeholder="Type a word or two&hellip;">
  <span id="alertMessage" class="alert"></span>
</form>
<a name="snippets"></a>
<a name="snippetsalt"></a>
<div id="termstats"></div>
<div id="d3-div-2">
 <div class="snippet_header left_contexts" id="cathead"></div>
 <div class="snippet_header right_contexts" id="notcathead"></div>
 <div class="snippet left_contexts" id="cat"></div>
 <div class="snippet right_contexts" id="notcat"></div>
</div>
<script charset="utf-8">
  // Created using Cozy: github.com/uwplse/cozy
function Rectangle(ax1, ay1, ax2, ay2) {
    this.ax1 = ax1;
    this.ay1 = ay1;
    this.ax2 = ax2;
    this.ay2 = ay2;
    this._left7 = undefined;
    this._right8 = undefined;
    this._parent9 = undefined;
    this._min_ax12 = undefined;
    this._min_ay13 = undefined;
    this._max_ay24 = undefined;
    this._height10 = undefined;
}
function RectangleHolder() {
    this.my_size = 0;
    (this)._root1 = null;
}
RectangleHolder.prototype.size = function () {
    return this.my_size;
};
RectangleHolder.prototype.add = function (x) {
    ++this.my_size;
    var _idx69 = (x).ax2;
    (x)._left7 = null;
    (x)._right8 = null;
    (x)._min_ax12 = (x).ax1;
    (x)._min_ay13 = (x).ay1;
    (x)._max_ay24 = (x).ay2;
    (x)._height10 = 0;
    var _previous70 = null;
    var _current71 = (this)._root1;
    var _is_left72 = false;
    while (!((_current71) == null)) {
        _previous70 = _current71;
        if ((_idx69) < ((_current71).ax2)) {
            _current71 = (_current71)._left7;
            _is_left72 = true;
        } else {
            _current71 = (_current71)._right8;
            _is_left72 = false;
        }
    }
    if ((_previous70) == null) {
        (this)._root1 = x;
    } else {
        (x)._parent9 = _previous70;
        if (_is_left72) {
            (_previous70)._left7 = x;
        } else {
            (_previous70)._right8 = x;
        }
    }
    var _cursor73 = (x)._parent9;
    var _changed74 = true;
    while ((_changed74) && (!((_cursor73) == (null)))) {
        var _old__min_ax1275 = (_cursor73)._min_ax12;
        var _old__min_ay1376 = (_cursor73)._min_ay13;
        var _old__max_ay2477 = (_cursor73)._max_ay24;
        var _old_height78 = (_cursor73)._height10;
        /* _min_ax12 is min of ax1 */
        var _augval79 = (_cursor73).ax1;
        var _child80 = (_cursor73)._left7;
        if (!((_child80) == null)) {
            var _val81 = (_child80)._min_ax12;
            _augval79 = ((_augval79) < (_val81)) ? (_augval79) : (_val81);
        }
        var _child82 = (_cursor73)._right8;
        if (!((_child82) == null)) {
            var _val83 = (_child82)._min_ax12;
            _augval79 = ((_augval79) < (_val83)) ? (_augval79) : (_val83);
        }
        (_cursor73)._min_ax12 = _augval79;
        /* _min_ay13 is min of ay1 */
        var _augval84 = (_cursor73).ay1;
        var _child85 = (_cursor73)._left7;
        if (!((_child85) == null)) {
            var _val86 = (_child85)._min_ay13;
            _augval84 = ((_augval84) < (_val86)) ? (_augval84) : (_val86);
        }
        var _child87 = (_cursor73)._right8;
        if (!((_child87) == null)) {
            var _val88 = (_child87)._min_ay13;
            _augval84 = ((_augval84) < (_val88)) ? (_augval84) : (_val88);
        }
        (_cursor73)._min_ay13 = _augval84;
        /* _max_ay24 is max of ay2 */
        var _augval89 = (_cursor73).ay2;
        var _child90 = (_cursor73)._left7;
        if (!((_child90) == null)) {
            var _val91 = (_child90)._max_ay24;
            _augval89 = ((_augval89) < (_val91)) ? (_val91) : (_augval89);
        }
        var _child92 = (_cursor73)._right8;
        if (!((_child92) == null)) {
            var _val93 = (_child92)._max_ay24;
            _augval89 = ((_augval89) < (_val93)) ? (_val93) : (_augval89);
        }
        (_cursor73)._max_ay24 = _augval89;
        (_cursor73)._height10 = 1 + ((((((_cursor73)._left7) == null) ? (-1) : (((_cursor73)._left7)._height10)) > ((((_cursor73)._right8) == null) ? (-1) : (((_cursor73)._right8)._height10))) ? ((((_cursor73)._left7) == null) ? (-1) : (((_cursor73)._left7)._height10)) : ((((_cursor73)._right8) == null) ? (-1) : (((_cursor73)._right8)._height10)));
        _changed74 = false;
        _changed74 = (_changed74) || (!((_old__min_ax1275) == ((_cursor73)._min_ax12)));
        _changed74 = (_changed74) || (!((_old__min_ay1376) == ((_cursor73)._min_ay13)));
        _changed74 = (_changed74) || (!((_old__max_ay2477) == ((_cursor73)._max_ay24)));
        _changed74 = (_changed74) || (!((_old_height78) == ((_cursor73)._height10)));
        _cursor73 = (_cursor73)._parent9;
    }
    /* rebalance AVL tree */
    var _cursor94 = x;
    var _imbalance95;
    while (!(((_cursor94)._parent9) == null)) {
        _cursor94 = (_cursor94)._parent9;
        (_cursor94)._height10 = 1 + ((((((_cursor94)._left7) == null) ? (-1) : (((_cursor94)._left7)._height10)) > ((((_cursor94)._right8) == null) ? (-1) : (((_cursor94)._right8)._height10))) ? ((((_cursor94)._left7) == null) ? (-1) : (((_cursor94)._left7)._height10)) : ((((_cursor94)._right8) == null) ? (-1) : (((_cursor94)._right8)._height10)));
        _imbalance95 = ((((_cursor94)._left7) == null) ? (-1) : (((_cursor94)._left7)._height10)) - ((((_cursor94)._right8) == null) ? (-1) : (((_cursor94)._right8)._height10));
        if ((_imbalance95) > (1)) {
            if ((((((_cursor94)._left7)._left7) == null) ? (-1) : ((((_cursor94)._left7)._left7)._height10)) < (((((_cursor94)._left7)._right8) == null) ? (-1) : ((((_cursor94)._left7)._right8)._height10))) {
                /* rotate ((_cursor94)._left7)._right8 */
                var _a96 = (_cursor94)._left7;
                var _b97 = (_a96)._right8;
                var _c98 = (_b97)._left7;
                /* replace _a96 with _b97 in (_a96)._parent9 */
                if (!(((_a96)._parent9) == null)) {
                    if ((((_a96)._parent9)._left7) == (_a96)) {
                        ((_a96)._parent9)._left7 = _b97;
                    } else {
                        ((_a96)._parent9)._right8 = _b97;
                    }
                }
                if (!((_b97) == null)) {
                    (_b97)._parent9 = (_a96)._parent9;
                }
                /* replace _c98 with _a96 in _b97 */
                (_b97)._left7 = _a96;
                if (!((_a96) == null)) {
                    (_a96)._parent9 = _b97;
                }
                /* replace _b97 with _c98 in _a96 */
                (_a96)._right8 = _c98;
                if (!((_c98) == null)) {
                    (_c98)._parent9 = _a96;
                }
                /* _min_ax12 is min of ax1 */
                var _augval99 = (_a96).ax1;
                var _child100 = (_a96)._left7;
                if (!((_child100) == null)) {
                    var _val101 = (_child100)._min_ax12;
                    _augval99 = ((_augval99) < (_val101)) ? (_augval99) : (_val101);
                }
                var _child102 = (_a96)._right8;
                if (!((_child102) == null)) {
                    var _val103 = (_child102)._min_ax12;
                    _augval99 = ((_augval99) < (_val103)) ? (_augval99) : (_val103);
                }
                (_a96)._min_ax12 = _augval99;
                /* _min_ay13 is min of ay1 */
                var _augval104 = (_a96).ay1;
                var _child105 = (_a96)._left7;
                if (!((_child105) == null)) {
                    var _val106 = (_child105)._min_ay13;
                    _augval104 = ((_augval104) < (_val106)) ? (_augval104) : (_val106);
                }
                var _child107 = (_a96)._right8;
                if (!((_child107) == null)) {
                    var _val108 = (_child107)._min_ay13;
                    _augval104 = ((_augval104) < (_val108)) ? (_augval104) : (_val108);
                }
                (_a96)._min_ay13 = _augval104;
                /* _max_ay24 is max of ay2 */
                var _augval109 = (_a96).ay2;
                var _child110 = (_a96)._left7;
                if (!((_child110) == null)) {
                    var _val111 = (_child110)._max_ay24;
                    _augval109 = ((_augval109) < (_val111)) ? (_val111) : (_augval109);
                }
                var _child112 = (_a96)._right8;
                if (!((_child112) == null)) {
                    var _val113 = (_child112)._max_ay24;
                    _augval109 = ((_augval109) < (_val113)) ? (_val113) : (_augval109);
                }
                (_a96)._max_ay24 = _augval109;
                (_a96)._height10 = 1 + ((((((_a96)._left7) == null) ? (-1) : (((_a96)._left7)._height10)) > ((((_a96)._right8) == null) ? (-1) : (((_a96)._right8)._height10))) ? ((((_a96)._left7) == null) ? (-1) : (((_a96)._left7)._height10)) : ((((_a96)._right8) == null) ? (-1) : (((_a96)._right8)._height10)));
                /* _min_ax12 is min of ax1 */
                var _augval114 = (_b97).ax1;
                var _child115 = (_b97)._left7;
                if (!((_child115) == null)) {
                    var _val116 = (_child115)._min_ax12;
                    _augval114 = ((_augval114) < (_val116)) ? (_augval114) : (_val116);
                }
                var _child117 = (_b97)._right8;
                if (!((_child117) == null)) {
                    var _val118 = (_child117)._min_ax12;
                    _augval114 = ((_augval114) < (_val118)) ? (_augval114) : (_val118);
                }
                (_b97)._min_ax12 = _augval114;
                /* _min_ay13 is min of ay1 */
                var _augval119 = (_b97).ay1;
                var _child120 = (_b97)._left7;
                if (!((_child120) == null)) {
                    var _val121 = (_child120)._min_ay13;
                    _augval119 = ((_augval119) < (_val121)) ? (_augval119) : (_val121);
                }
                var _child122 = (_b97)._right8;
                if (!((_child122) == null)) {
                    var _val123 = (_child122)._min_ay13;
                    _augval119 = ((_augval119) < (_val123)) ? (_augval119) : (_val123);
                }
                (_b97)._min_ay13 = _augval119;
                /* _max_ay24 is max of ay2 */
                var _augval124 = (_b97).ay2;
                var _child125 = (_b97)._left7;
                if (!((_child125) == null)) {
                    var _val126 = (_child125)._max_ay24;
                    _augval124 = ((_augval124) < (_val126)) ? (_val126) : (_augval124);
                }
                var _child127 = (_b97)._right8;
                if (!((_child127) == null)) {
                    var _val128 = (_child127)._max_ay24;
                    _augval124 = ((_augval124) < (_val128)) ? (_val128) : (_augval124);
                }
                (_b97)._max_ay24 = _augval124;
                (_b97)._height10 = 1 + ((((((_b97)._left7) == null) ? (-1) : (((_b97)._left7)._height10)) > ((((_b97)._right8) == null) ? (-1) : (((_b97)._right8)._height10))) ? ((((_b97)._left7) == null) ? (-1) : (((_b97)._left7)._height10)) : ((((_b97)._right8) == null) ? (-1) : (((_b97)._right8)._height10)));
                if (!(((_b97)._parent9) == null)) {
                    /* _min_ax12 is min of ax1 */
                    var _augval129 = ((_b97)._parent9).ax1;
                    var _child130 = ((_b97)._parent9)._left7;
                    if (!((_child130) == null)) {
                        var _val131 = (_child130)._min_ax12;
                        _augval129 = ((_augval129) < (_val131)) ? (_augval129) : (_val131);
                    }
                    var _child132 = ((_b97)._parent9)._right8;
                    if (!((_child132) == null)) {
                        var _val133 = (_child132)._min_ax12;
                        _augval129 = ((_augval129) < (_val133)) ? (_augval129) : (_val133);
                    }
                    ((_b97)._parent9)._min_ax12 = _augval129;
                    /* _min_ay13 is min of ay1 */
                    var _augval134 = ((_b97)._parent9).ay1;
                    var _child135 = ((_b97)._parent9)._left7;
                    if (!((_child135) == null)) {
                        var _val136 = (_child135)._min_ay13;
                        _augval134 = ((_augval134) < (_val136)) ? (_augval134) : (_val136);
                    }
                    var _child137 = ((_b97)._parent9)._right8;
                    if (!((_child137) == null)) {
                        var _val138 = (_child137)._min_ay13;
                        _augval134 = ((_augval134) < (_val138)) ? (_augval134) : (_val138);
                    }
                    ((_b97)._parent9)._min_ay13 = _augval134;
                    /* _max_ay24 is max of ay2 */
                    var _augval139 = ((_b97)._parent9).ay2;
                    var _child140 = ((_b97)._parent9)._left7;
                    if (!((_child140) == null)) {
                        var _val141 = (_child140)._max_ay24;
                        _augval139 = ((_augval139) < (_val141)) ? (_val141) : (_augval139);
                    }
                    var _child142 = ((_b97)._parent9)._right8;
                    if (!((_child142) == null)) {
                        var _val143 = (_child142)._max_ay24;
                        _augval139 = ((_augval139) < (_val143)) ? (_val143) : (_augval139);
                    }
                    ((_b97)._parent9)._max_ay24 = _augval139;
                    ((_b97)._parent9)._height10 = 1 + (((((((_b97)._parent9)._left7) == null) ? (-1) : ((((_b97)._parent9)._left7)._height10)) > (((((_b97)._parent9)._right8) == null) ? (-1) : ((((_b97)._parent9)._right8)._height10))) ? (((((_b97)._parent9)._left7) == null) ? (-1) : ((((_b97)._parent9)._left7)._height10)) : (((((_b97)._parent9)._right8) == null) ? (-1) : ((((_b97)._parent9)._right8)._height10)));
                } else {
                    (this)._root1 = _b97;
                }
            }
            /* rotate (_cursor94)._left7 */
            var _a144 = _cursor94;
            var _b145 = (_a144)._left7;
            var _c146 = (_b145)._right8;
            /* replace _a144 with _b145 in (_a144)._parent9 */
            if (!(((_a144)._parent9) == null)) {
                if ((((_a144)._parent9)._left7) == (_a144)) {
                    ((_a144)._parent9)._left7 = _b145;
                } else {
                    ((_a144)._parent9)._right8 = _b145;
                }
            }
            if (!((_b145) == null)) {
                (_b145)._parent9 = (_a144)._parent9;
            }
            /* replace _c146 with _a144 in _b145 */
            (_b145)._right8 = _a144;
            if (!((_a144) == null)) {
                (_a144)._parent9 = _b145;
            }
            /* replace _b145 with _c146 in _a144 */
            (_a144)._left7 = _c146;
            if (!((_c146) == null)) {
                (_c146)._parent9 = _a144;
            }
            /* _min_ax12 is min of ax1 */
            var _augval147 = (_a144).ax1;
            var _child148 = (_a144)._left7;
            if (!((_child148) == null)) {
                var _val149 = (_child148)._min_ax12;
                _augval147 = ((_augval147) < (_val149)) ? (_augval147) : (_val149);
            }
            var _child150 = (_a144)._right8;
            if (!((_child150) == null)) {
                var _val151 = (_child150)._min_ax12;
                _augval147 = ((_augval147) < (_val151)) ? (_augval147) : (_val151);
            }
            (_a144)._min_ax12 = _augval147;
            /* _min_ay13 is min of ay1 */
            var _augval152 = (_a144).ay1;
            var _child153 = (_a144)._left7;
            if (!((_child153) == null)) {
                var _val154 = (_child153)._min_ay13;
                _augval152 = ((_augval152) < (_val154)) ? (_augval152) : (_val154);
            }
            var _child155 = (_a144)._right8;
            if (!((_child155) == null)) {
                var _val156 = (_child155)._min_ay13;
                _augval152 = ((_augval152) < (_val156)) ? (_augval152) : (_val156);
            }
            (_a144)._min_ay13 = _augval152;
            /* _max_ay24 is max of ay2 */
            var _augval157 = (_a144).ay2;
            var _child158 = (_a144)._left7;
            if (!((_child158) == null)) {
                var _val159 = (_child158)._max_ay24;
                _augval157 = ((_augval157) < (_val159)) ? (_val159) : (_augval157);
            }
            var _child160 = (_a144)._right8;
            if (!((_child160) == null)) {
                var _val161 = (_child160)._max_ay24;
                _augval157 = ((_augval157) < (_val161)) ? (_val161) : (_augval157);
            }
            (_a144)._max_ay24 = _augval157;
            (_a144)._height10 = 1 + ((((((_a144)._left7) == null) ? (-1) : (((_a144)._left7)._height10)) > ((((_a144)._right8) == null) ? (-1) : (((_a144)._right8)._height10))) ? ((((_a144)._left7) == null) ? (-1) : (((_a144)._left7)._height10)) : ((((_a144)._right8) == null) ? (-1) : (((_a144)._right8)._height10)));
            /* _min_ax12 is min of ax1 */
            var _augval162 = (_b145).ax1;
            var _child163 = (_b145)._left7;
            if (!((_child163) == null)) {
                var _val164 = (_child163)._min_ax12;
                _augval162 = ((_augval162) < (_val164)) ? (_augval162) : (_val164);
            }
            var _child165 = (_b145)._right8;
            if (!((_child165) == null)) {
                var _val166 = (_child165)._min_ax12;
                _augval162 = ((_augval162) < (_val166)) ? (_augval162) : (_val166);
            }
            (_b145)._min_ax12 = _augval162;
            /* _min_ay13 is min of ay1 */
            var _augval167 = (_b145).ay1;
            var _child168 = (_b145)._left7;
            if (!((_child168) == null)) {
                var _val169 = (_child168)._min_ay13;
                _augval167 = ((_augval167) < (_val169)) ? (_augval167) : (_val169);
            }
            var _child170 = (_b145)._right8;
            if (!((_child170) == null)) {
                var _val171 = (_child170)._min_ay13;
                _augval167 = ((_augval167) < (_val171)) ? (_augval167) : (_val171);
            }
            (_b145)._min_ay13 = _augval167;
            /* _max_ay24 is max of ay2 */
            var _augval172 = (_b145).ay2;
            var _child173 = (_b145)._left7;
            if (!((_child173) == null)) {
                var _val174 = (_child173)._max_ay24;
                _augval172 = ((_augval172) < (_val174)) ? (_val174) : (_augval172);
            }
            var _child175 = (_b145)._right8;
            if (!((_child175) == null)) {
                var _val176 = (_child175)._max_ay24;
                _augval172 = ((_augval172) < (_val176)) ? (_val176) : (_augval172);
            }
            (_b145)._max_ay24 = _augval172;
            (_b145)._height10 = 1 + ((((((_b145)._left7) == null) ? (-1) : (((_b145)._left7)._height10)) > ((((_b145)._right8) == null) ? (-1) : (((_b145)._right8)._height10))) ? ((((_b145)._left7) == null) ? (-1) : (((_b145)._left7)._height10)) : ((((_b145)._right8) == null) ? (-1) : (((_b145)._right8)._height10)));
            if (!(((_b145)._parent9) == null)) {
                /* _min_ax12 is min of ax1 */
                var _augval177 = ((_b145)._parent9).ax1;
                var _child178 = ((_b145)._parent9)._left7;
                if (!((_child178) == null)) {
                    var _val179 = (_child178)._min_ax12;
                    _augval177 = ((_augval177) < (_val179)) ? (_augval177) : (_val179);
                }
                var _child180 = ((_b145)._parent9)._right8;
                if (!((_child180) == null)) {
                    var _val181 = (_child180)._min_ax12;
                    _augval177 = ((_augval177) < (_val181)) ? (_augval177) : (_val181);
                }
                ((_b145)._parent9)._min_ax12 = _augval177;
                /* _min_ay13 is min of ay1 */
                var _augval182 = ((_b145)._parent9).ay1;
                var _child183 = ((_b145)._parent9)._left7;
                if (!((_child183) == null)) {
                    var _val184 = (_child183)._min_ay13;
                    _augval182 = ((_augval182) < (_val184)) ? (_augval182) : (_val184);
                }
                var _child185 = ((_b145)._parent9)._right8;
                if (!((_child185) == null)) {
                    var _val186 = (_child185)._min_ay13;
                    _augval182 = ((_augval182) < (_val186)) ? (_augval182) : (_val186);
                }
                ((_b145)._parent9)._min_ay13 = _augval182;
                /* _max_ay24 is max of ay2 */
                var _augval187 = ((_b145)._parent9).ay2;
                var _child188 = ((_b145)._parent9)._left7;
                if (!((_child188) == null)) {
                    var _val189 = (_child188)._max_ay24;
                    _augval187 = ((_augval187) < (_val189)) ? (_val189) : (_augval187);
                }
                var _child190 = ((_b145)._parent9)._right8;
                if (!((_child190) == null)) {
                    var _val191 = (_child190)._max_ay24;
                    _augval187 = ((_augval187) < (_val191)) ? (_val191) : (_augval187);
                }
                ((_b145)._parent9)._max_ay24 = _augval187;
                ((_b145)._parent9)._height10 = 1 + (((((((_b145)._parent9)._left7) == null) ? (-1) : ((((_b145)._parent9)._left7)._height10)) > (((((_b145)._parent9)._right8) == null) ? (-1) : ((((_b145)._parent9)._right8)._height10))) ? (((((_b145)._parent9)._left7) == null) ? (-1) : ((((_b145)._parent9)._left7)._height10)) : (((((_b145)._parent9)._right8) == null) ? (-1) : ((((_b145)._parent9)._right8)._height10)));
            } else {
                (this)._root1 = _b145;
            }
            _cursor94 = (_cursor94)._parent9;
        } else if ((_imbalance95) < (-1)) {
            if ((((((_cursor94)._right8)._left7) == null) ? (-1) : ((((_cursor94)._right8)._left7)._height10)) > (((((_cursor94)._right8)._right8) == null) ? (-1) : ((((_cursor94)._right8)._right8)._height10))) {
                /* rotate ((_cursor94)._right8)._left7 */
                var _a192 = (_cursor94)._right8;
                var _b193 = (_a192)._left7;
                var _c194 = (_b193)._right8;
                /* replace _a192 with _b193 in (_a192)._parent9 */
                if (!(((_a192)._parent9) == null)) {
                    if ((((_a192)._parent9)._left7) == (_a192)) {
                        ((_a192)._parent9)._left7 = _b193;
                    } else {
                        ((_a192)._parent9)._right8 = _b193;
                    }
                }
                if (!((_b193) == null)) {
                    (_b193)._parent9 = (_a192)._parent9;
                }
                /* replace _c194 with _a192 in _b193 */
                (_b193)._right8 = _a192;
                if (!((_a192) == null)) {
                    (_a192)._parent9 = _b193;
                }
                /* replace _b193 with _c194 in _a192 */
                (_a192)._left7 = _c194;
                if (!((_c194) == null)) {
                    (_c194)._parent9 = _a192;
                }
                /* _min_ax12 is min of ax1 */
                var _augval195 = (_a192).ax1;
                var _child196 = (_a192)._left7;
                if (!((_child196) == null)) {
                    var _val197 = (_child196)._min_ax12;
                    _augval195 = ((_augval195) < (_val197)) ? (_augval195) : (_val197);
                }
                var _child198 = (_a192)._right8;
                if (!((_child198) == null)) {
                    var _val199 = (_child198)._min_ax12;
                    _augval195 = ((_augval195) < (_val199)) ? (_augval195) : (_val199);
                }
                (_a192)._min_ax12 = _augval195;
                /* _min_ay13 is min of ay1 */
                var _augval200 = (_a192).ay1;
                var _child201 = (_a192)._left7;
                if (!((_child201) == null)) {
                    var _val202 = (_child201)._min_ay13;
                    _augval200 = ((_augval200) < (_val202)) ? (_augval200) : (_val202);
                }
                var _child203 = (_a192)._right8;
                if (!((_child203) == null)) {
                    var _val204 = (_child203)._min_ay13;
                    _augval200 = ((_augval200) < (_val204)) ? (_augval200) : (_val204);
                }
                (_a192)._min_ay13 = _augval200;
                /* _max_ay24 is max of ay2 */
                var _augval205 = (_a192).ay2;
                var _child206 = (_a192)._left7;
                if (!((_child206) == null)) {
                    var _val207 = (_child206)._max_ay24;
                    _augval205 = ((_augval205) < (_val207)) ? (_val207) : (_augval205);
                }
                var _child208 = (_a192)._right8;
                if (!((_child208) == null)) {
                    var _val209 = (_child208)._max_ay24;
                    _augval205 = ((_augval205) < (_val209)) ? (_val209) : (_augval205);
                }
                (_a192)._max_ay24 = _augval205;
                (_a192)._height10 = 1 + ((((((_a192)._left7) == null) ? (-1) : (((_a192)._left7)._height10)) > ((((_a192)._right8) == null) ? (-1) : (((_a192)._right8)._height10))) ? ((((_a192)._left7) == null) ? (-1) : (((_a192)._left7)._height10)) : ((((_a192)._right8) == null) ? (-1) : (((_a192)._right8)._height10)));
                /* _min_ax12 is min of ax1 */
                var _augval210 = (_b193).ax1;
                var _child211 = (_b193)._left7;
                if (!((_child211) == null)) {
                    var _val212 = (_child211)._min_ax12;
                    _augval210 = ((_augval210) < (_val212)) ? (_augval210) : (_val212);
                }
                var _child213 = (_b193)._right8;
                if (!((_child213) == null)) {
                    var _val214 = (_child213)._min_ax12;
                    _augval210 = ((_augval210) < (_val214)) ? (_augval210) : (_val214);
                }
                (_b193)._min_ax12 = _augval210;
                /* _min_ay13 is min of ay1 */
                var _augval215 = (_b193).ay1;
                var _child216 = (_b193)._left7;
                if (!((_child216) == null)) {
                    var _val217 = (_child216)._min_ay13;
                    _augval215 = ((_augval215) < (_val217)) ? (_augval215) : (_val217);
                }
                var _child218 = (_b193)._right8;
                if (!((_child218) == null)) {
                    var _val219 = (_child218)._min_ay13;
                    _augval215 = ((_augval215) < (_val219)) ? (_augval215) : (_val219);
                }
                (_b193)._min_ay13 = _augval215;
                /* _max_ay24 is max of ay2 */
                var _augval220 = (_b193).ay2;
                var _child221 = (_b193)._left7;
                if (!((_child221) == null)) {
                    var _val222 = (_child221)._max_ay24;
                    _augval220 = ((_augval220) < (_val222)) ? (_val222) : (_augval220);
                }
                var _child223 = (_b193)._right8;
                if (!((_child223) == null)) {
                    var _val224 = (_child223)._max_ay24;
                    _augval220 = ((_augval220) < (_val224)) ? (_val224) : (_augval220);
                }
                (_b193)._max_ay24 = _augval220;
                (_b193)._height10 = 1 + ((((((_b193)._left7) == null) ? (-1) : (((_b193)._left7)._height10)) > ((((_b193)._right8) == null) ? (-1) : (((_b193)._right8)._height10))) ? ((((_b193)._left7) == null) ? (-1) : (((_b193)._left7)._height10)) : ((((_b193)._right8) == null) ? (-1) : (((_b193)._right8)._height10)));
                if (!(((_b193)._parent9) == null)) {
                    /* _min_ax12 is min of ax1 */
                    var _augval225 = ((_b193)._parent9).ax1;
                    var _child226 = ((_b193)._parent9)._left7;
                    if (!((_child226) == null)) {
                        var _val227 = (_child226)._min_ax12;
                        _augval225 = ((_augval225) < (_val227)) ? (_augval225) : (_val227);
                    }
                    var _child228 = ((_b193)._parent9)._right8;
                    if (!((_child228) == null)) {
                        var _val229 = (_child228)._min_ax12;
                        _augval225 = ((_augval225) < (_val229)) ? (_augval225) : (_val229);
                    }
                    ((_b193)._parent9)._min_ax12 = _augval225;
                    /* _min_ay13 is min of ay1 */
                    var _augval230 = ((_b193)._parent9).ay1;
                    var _child231 = ((_b193)._parent9)._left7;
                    if (!((_child231) == null)) {
                        var _val232 = (_child231)._min_ay13;
                        _augval230 = ((_augval230) < (_val232)) ? (_augval230) : (_val232);
                    }
                    var _child233 = ((_b193)._parent9)._right8;
                    if (!((_child233) == null)) {
                        var _val234 = (_child233)._min_ay13;
                        _augval230 = ((_augval230) < (_val234)) ? (_augval230) : (_val234);
                    }
                    ((_b193)._parent9)._min_ay13 = _augval230;
                    /* _max_ay24 is max of ay2 */
                    var _augval235 = ((_b193)._parent9).ay2;
                    var _child236 = ((_b193)._parent9)._left7;
                    if (!((_child236) == null)) {
                        var _val237 = (_child236)._max_ay24;
                        _augval235 = ((_augval235) < (_val237)) ? (_val237) : (_augval235);
                    }
                    var _child238 = ((_b193)._parent9)._right8;
                    if (!((_child238) == null)) {
                        var _val239 = (_child238)._max_ay24;
                        _augval235 = ((_augval235) < (_val239)) ? (_val239) : (_augval235);
                    }
                    ((_b193)._parent9)._max_ay24 = _augval235;
                    ((_b193)._parent9)._height10 = 1 + (((((((_b193)._parent9)._left7) == null) ? (-1) : ((((_b193)._parent9)._left7)._height10)) > (((((_b193)._parent9)._right8) == null) ? (-1) : ((((_b193)._parent9)._right8)._height10))) ? (((((_b193)._parent9)._left7) == null) ? (-1) : ((((_b193)._parent9)._left7)._height10)) : (((((_b193)._parent9)._right8) == null) ? (-1) : ((((_b193)._parent9)._right8)._height10)));
                } else {
                    (this)._root1 = _b193;
                }
            }
            /* rotate (_cursor94)._right8 */
            var _a240 = _cursor94;
            var _b241 = (_a240)._right8;
            var _c242 = (_b241)._left7;
            /* replace _a240 with _b241 in (_a240)._parent9 */
            if (!(((_a240)._parent9) == null)) {
                if ((((_a240)._parent9)._left7) == (_a240)) {
                    ((_a240)._parent9)._left7 = _b241;
                } else {
                    ((_a240)._parent9)._right8 = _b241;
                }
            }
            if (!((_b241) == null)) {
                (_b241)._parent9 = (_a240)._parent9;
            }
            /* replace _c242 with _a240 in _b241 */
            (_b241)._left7 = _a240;
            if (!((_a240) == null)) {
                (_a240)._parent9 = _b241;
            }
            /* replace _b241 with _c242 in _a240 */
            (_a240)._right8 = _c242;
            if (!((_c242) == null)) {
                (_c242)._parent9 = _a240;
            }
            /* _min_ax12 is min of ax1 */
            var _augval243 = (_a240).ax1;
            var _child244 = (_a240)._left7;
            if (!((_child244) == null)) {
                var _val245 = (_child244)._min_ax12;
                _augval243 = ((_augval243) < (_val245)) ? (_augval243) : (_val245);
            }
            var _child246 = (_a240)._right8;
            if (!((_child246) == null)) {
                var _val247 = (_child246)._min_ax12;
                _augval243 = ((_augval243) < (_val247)) ? (_augval243) : (_val247);
            }
            (_a240)._min_ax12 = _augval243;
            /* _min_ay13 is min of ay1 */
            var _augval248 = (_a240).ay1;
            var _child249 = (_a240)._left7;
            if (!((_child249) == null)) {
                var _val250 = (_child249)._min_ay13;
                _augval248 = ((_augval248) < (_val250)) ? (_augval248) : (_val250);
            }
            var _child251 = (_a240)._right8;
            if (!((_child251) == null)) {
                var _val252 = (_child251)._min_ay13;
                _augval248 = ((_augval248) < (_val252)) ? (_augval248) : (_val252);
            }
            (_a240)._min_ay13 = _augval248;
            /* _max_ay24 is max of ay2 */
            var _augval253 = (_a240).ay2;
            var _child254 = (_a240)._left7;
            if (!((_child254) == null)) {
                var _val255 = (_child254)._max_ay24;
                _augval253 = ((_augval253) < (_val255)) ? (_val255) : (_augval253);
            }
            var _child256 = (_a240)._right8;
            if (!((_child256) == null)) {
                var _val257 = (_child256)._max_ay24;
                _augval253 = ((_augval253) < (_val257)) ? (_val257) : (_augval253);
            }
            (_a240)._max_ay24 = _augval253;
            (_a240)._height10 = 1 + ((((((_a240)._left7) == null) ? (-1) : (((_a240)._left7)._height10)) > ((((_a240)._right8) == null) ? (-1) : (((_a240)._right8)._height10))) ? ((((_a240)._left7) == null) ? (-1) : (((_a240)._left7)._height10)) : ((((_a240)._right8) == null) ? (-1) : (((_a240)._right8)._height10)));
            /* _min_ax12 is min of ax1 */
            var _augval258 = (_b241).ax1;
            var _child259 = (_b241)._left7;
            if (!((_child259) == null)) {
                var _val260 = (_child259)._min_ax12;
                _augval258 = ((_augval258) < (_val260)) ? (_augval258) : (_val260);
            }
            var _child261 = (_b241)._right8;
            if (!((_child261) == null)) {
                var _val262 = (_child261)._min_ax12;
                _augval258 = ((_augval258) < (_val262)) ? (_augval258) : (_val262);
            }
            (_b241)._min_ax12 = _augval258;
            /* _min_ay13 is min of ay1 */
            var _augval263 = (_b241).ay1;
            var _child264 = (_b241)._left7;
            if (!((_child264) == null)) {
                var _val265 = (_child264)._min_ay13;
                _augval263 = ((_augval263) < (_val265)) ? (_augval263) : (_val265);
            }
            var _child266 = (_b241)._right8;
            if (!((_child266) == null)) {
                var _val267 = (_child266)._min_ay13;
                _augval263 = ((_augval263) < (_val267)) ? (_augval263) : (_val267);
            }
            (_b241)._min_ay13 = _augval263;
            /* _max_ay24 is max of ay2 */
            var _augval268 = (_b241).ay2;
            var _child269 = (_b241)._left7;
            if (!((_child269) == null)) {
                var _val270 = (_child269)._max_ay24;
                _augval268 = ((_augval268) < (_val270)) ? (_val270) : (_augval268);
            }
            var _child271 = (_b241)._right8;
            if (!((_child271) == null)) {
                var _val272 = (_child271)._max_ay24;
                _augval268 = ((_augval268) < (_val272)) ? (_val272) : (_augval268);
            }
            (_b241)._max_ay24 = _augval268;
            (_b241)._height10 = 1 + ((((((_b241)._left7) == null) ? (-1) : (((_b241)._left7)._height10)) > ((((_b241)._right8) == null) ? (-1) : (((_b241)._right8)._height10))) ? ((((_b241)._left7) == null) ? (-1) : (((_b241)._left7)._height10)) : ((((_b241)._right8) == null) ? (-1) : (((_b241)._right8)._height10)));
            if (!(((_b241)._parent9) == null)) {
                /* _min_ax12 is min of ax1 */
                var _augval273 = ((_b241)._parent9).ax1;
                var _child274 = ((_b241)._parent9)._left7;
                if (!((_child274) == null)) {
                    var _val275 = (_child274)._min_ax12;
                    _augval273 = ((_augval273) < (_val275)) ? (_augval273) : (_val275);
                }
                var _child276 = ((_b241)._parent9)._right8;
                if (!((_child276) == null)) {
                    var _val277 = (_child276)._min_ax12;
                    _augval273 = ((_augval273) < (_val277)) ? (_augval273) : (_val277);
                }
                ((_b241)._parent9)._min_ax12 = _augval273;
                /* _min_ay13 is min of ay1 */
                var _augval278 = ((_b241)._parent9).ay1;
                var _child279 = ((_b241)._parent9)._left7;
                if (!((_child279) == null)) {
                    var _val280 = (_child279)._min_ay13;
                    _augval278 = ((_augval278) < (_val280)) ? (_augval278) : (_val280);
                }
                var _child281 = ((_b241)._parent9)._right8;
                if (!((_child281) == null)) {
                    var _val282 = (_child281)._min_ay13;
                    _augval278 = ((_augval278) < (_val282)) ? (_augval278) : (_val282);
                }
                ((_b241)._parent9)._min_ay13 = _augval278;
                /* _max_ay24 is max of ay2 */
                var _augval283 = ((_b241)._parent9).ay2;
                var _child284 = ((_b241)._parent9)._left7;
                if (!((_child284) == null)) {
                    var _val285 = (_child284)._max_ay24;
                    _augval283 = ((_augval283) < (_val285)) ? (_val285) : (_augval283);
                }
                var _child286 = ((_b241)._parent9)._right8;
                if (!((_child286) == null)) {
                    var _val287 = (_child286)._max_ay24;
                    _augval283 = ((_augval283) < (_val287)) ? (_val287) : (_augval283);
                }
                ((_b241)._parent9)._max_ay24 = _augval283;
                ((_b241)._parent9)._height10 = 1 + (((((((_b241)._parent9)._left7) == null) ? (-1) : ((((_b241)._parent9)._left7)._height10)) > (((((_b241)._parent9)._right8) == null) ? (-1) : ((((_b241)._parent9)._right8)._height10))) ? (((((_b241)._parent9)._left7) == null) ? (-1) : ((((_b241)._parent9)._left7)._height10)) : (((((_b241)._parent9)._right8) == null) ? (-1) : ((((_b241)._parent9)._right8)._height10)));
            } else {
                (this)._root1 = _b241;
            }
            _cursor94 = (_cursor94)._parent9;
        }
    }
};
RectangleHolder.prototype.remove = function (x) {
    --this.my_size;
    var _parent288 = (x)._parent9;
    var _left289 = (x)._left7;
    var _right290 = (x)._right8;
    var _new_x291;
    if (((_left289) == null) && ((_right290) == null)) {
        _new_x291 = null;
        /* replace x with _new_x291 in _parent288 */
        if (!((_parent288) == null)) {
            if (((_parent288)._left7) == (x)) {
                (_parent288)._left7 = _new_x291;
            } else {
                (_parent288)._right8 = _new_x291;
            }
        }
        if (!((_new_x291) == null)) {
            (_new_x291)._parent9 = _parent288;
        }
    } else if ((!((_left289) == null)) && ((_right290) == null)) {
        _new_x291 = _left289;
        /* replace x with _new_x291 in _parent288 */
        if (!((_parent288) == null)) {
            if (((_parent288)._left7) == (x)) {
                (_parent288)._left7 = _new_x291;
            } else {
                (_parent288)._right8 = _new_x291;
            }
        }
        if (!((_new_x291) == null)) {
            (_new_x291)._parent9 = _parent288;
        }
    } else if (((_left289) == null) && (!((_right290) == null))) {
        _new_x291 = _right290;
        /* replace x with _new_x291 in _parent288 */
        if (!((_parent288) == null)) {
            if (((_parent288)._left7) == (x)) {
                (_parent288)._left7 = _new_x291;
            } else {
                (_parent288)._right8 = _new_x291;
            }
        }
        if (!((_new_x291) == null)) {
            (_new_x291)._parent9 = _parent288;
        }
    } else {
        var _root292 = (x)._right8;
        var _x293 = _root292;
        var _descend294 = true;
        var _from_left295 = true;
        while (true) {
            if ((_x293) == null) {
                _x293 = null;
                break;
            }
            if (_descend294) {
                /* too small? */
                if (false) {
                    if ((!(((_x293)._right8) == null)) && (true)) {
                        if ((_x293) == (_root292)) {
                            _root292 = (_x293)._right8;
                        }
                        _x293 = (_x293)._right8;
                    } else if ((_x293) == (_root292)) {
                        _x293 = null;
                        break;
                    } else {
                        _descend294 = false;
                        _from_left295 = (!(((_x293)._parent9) == null)) && ((_x293) == (((_x293)._parent9)._left7));
                        _x293 = (_x293)._parent9;
                    }
                } else if ((!(((_x293)._left7) == null)) && (true)) {
                    _x293 = (_x293)._left7;
                    /* too large? */
                } else if (false) {
                    if ((_x293) == (_root292)) {
                        _x293 = null;
                        break;
                    } else {
                        _descend294 = false;
                        _from_left295 = (!(((_x293)._parent9) == null)) && ((_x293) == (((_x293)._parent9)._left7));
                        _x293 = (_x293)._parent9;
                    }
                    /* node ok? */
                } else if (true) {
                    break;
                } else if ((_x293) == (_root292)) {
                    _root292 = (_x293)._right8;
                    _x293 = (_x293)._right8;
                } else {
                    if ((!(((_x293)._right8) == null)) && (true)) {
                        if ((_x293) == (_root292)) {
                            _root292 = (_x293)._right8;
                        }
                        _x293 = (_x293)._right8;
                    } else {
                        _descend294 = false;
                        _from_left295 = (!(((_x293)._parent9) == null)) && ((_x293) == (((_x293)._parent9)._left7));
                        _x293 = (_x293)._parent9;
                    }
                }
            } else if (_from_left295) {
                if (false) {
                    _x293 = null;
                    break;
                } else if (true) {
                    break;
                } else if ((!(((_x293)._right8) == null)) && (true)) {
                    _descend294 = true;
                    if ((_x293) == (_root292)) {
                        _root292 = (_x293)._right8;
                    }
                    _x293 = (_x293)._right8;
                } else if ((_x293) == (_root292)) {
                    _x293 = null;
                    break;
                } else {
                    _descend294 = false;
                    _from_left295 = (!(((_x293)._parent9) == null)) && ((_x293) == (((_x293)._parent9)._left7));
                    _x293 = (_x293)._parent9;
                }
            } else {
                if ((_x293) == (_root292)) {
                    _x293 = null;
                    break;
                } else {
                    _descend294 = false;
                    _from_left295 = (!(((_x293)._parent9) == null)) && ((_x293) == (((_x293)._parent9)._left7));
                    _x293 = (_x293)._parent9;
                }
            }
        }
        _new_x291 = _x293;
        var _mp296 = (_x293)._parent9;
        var _mr297 = (_x293)._right8;
        /* replace _x293 with _mr297 in _mp296 */
        if (!((_mp296) == null)) {
            if (((_mp296)._left7) == (_x293)) {
                (_mp296)._left7 = _mr297;
            } else {
                (_mp296)._right8 = _mr297;
            }
        }
        if (!((_mr297) == null)) {
            (_mr297)._parent9 = _mp296;
        }
        /* replace x with _x293 in _parent288 */
        if (!((_parent288) == null)) {
            if (((_parent288)._left7) == (x)) {
                (_parent288)._left7 = _x293;
            } else {
                (_parent288)._right8 = _x293;
            }
        }
        if (!((_x293) == null)) {
            (_x293)._parent9 = _parent288;
        }
        /* replace null with _left289 in _x293 */
        (_x293)._left7 = _left289;
        if (!((_left289) == null)) {
            (_left289)._parent9 = _x293;
        }
        /* replace _mr297 with (x)._right8 in _x293 */
        (_x293)._right8 = (x)._right8;
        if (!(((x)._right8) == null)) {
            ((x)._right8)._parent9 = _x293;
        }
        /* _min_ax12 is min of ax1 */
        var _augval298 = (_x293).ax1;
        var _child299 = (_x293)._left7;
        if (!((_child299) == null)) {
            var _val300 = (_child299)._min_ax12;
            _augval298 = ((_augval298) < (_val300)) ? (_augval298) : (_val300);
        }
        var _child301 = (_x293)._right8;
        if (!((_child301) == null)) {
            var _val302 = (_child301)._min_ax12;
            _augval298 = ((_augval298) < (_val302)) ? (_augval298) : (_val302);
        }
        (_x293)._min_ax12 = _augval298;
        /* _min_ay13 is min of ay1 */
        var _augval303 = (_x293).ay1;
        var _child304 = (_x293)._left7;
        if (!((_child304) == null)) {
            var _val305 = (_child304)._min_ay13;
            _augval303 = ((_augval303) < (_val305)) ? (_augval303) : (_val305);
        }
        var _child306 = (_x293)._right8;
        if (!((_child306) == null)) {
            var _val307 = (_child306)._min_ay13;
            _augval303 = ((_augval303) < (_val307)) ? (_augval303) : (_val307);
        }
        (_x293)._min_ay13 = _augval303;
        /* _max_ay24 is max of ay2 */
        var _augval308 = (_x293).ay2;
        var _child309 = (_x293)._left7;
        if (!((_child309) == null)) {
            var _val310 = (_child309)._max_ay24;
            _augval308 = ((_augval308) < (_val310)) ? (_val310) : (_augval308);
        }
        var _child311 = (_x293)._right8;
        if (!((_child311) == null)) {
            var _val312 = (_child311)._max_ay24;
            _augval308 = ((_augval308) < (_val312)) ? (_val312) : (_augval308);
        }
        (_x293)._max_ay24 = _augval308;
        (_x293)._height10 = 1 + ((((((_x293)._left7) == null) ? (-1) : (((_x293)._left7)._height10)) > ((((_x293)._right8) == null) ? (-1) : (((_x293)._right8)._height10))) ? ((((_x293)._left7) == null) ? (-1) : (((_x293)._left7)._height10)) : ((((_x293)._right8) == null) ? (-1) : (((_x293)._right8)._height10)));
        var _cursor313 = _mp296;
        var _changed314 = true;
        while ((_changed314) && (!((_cursor313) == (_parent288)))) {
            var _old__min_ax12315 = (_cursor313)._min_ax12;
            var _old__min_ay13316 = (_cursor313)._min_ay13;
            var _old__max_ay24317 = (_cursor313)._max_ay24;
            var _old_height318 = (_cursor313)._height10;
            /* _min_ax12 is min of ax1 */
            var _augval319 = (_cursor313).ax1;
            var _child320 = (_cursor313)._left7;
            if (!((_child320) == null)) {
                var _val321 = (_child320)._min_ax12;
                _augval319 = ((_augval319) < (_val321)) ? (_augval319) : (_val321);
            }
            var _child322 = (_cursor313)._right8;
            if (!((_child322) == null)) {
                var _val323 = (_child322)._min_ax12;
                _augval319 = ((_augval319) < (_val323)) ? (_augval319) : (_val323);
            }
            (_cursor313)._min_ax12 = _augval319;
            /* _min_ay13 is min of ay1 */
            var _augval324 = (_cursor313).ay1;
            var _child325 = (_cursor313)._left7;
            if (!((_child325) == null)) {
                var _val326 = (_child325)._min_ay13;
                _augval324 = ((_augval324) < (_val326)) ? (_augval324) : (_val326);
            }
            var _child327 = (_cursor313)._right8;
            if (!((_child327) == null)) {
                var _val328 = (_child327)._min_ay13;
                _augval324 = ((_augval324) < (_val328)) ? (_augval324) : (_val328);
            }
            (_cursor313)._min_ay13 = _augval324;
            /* _max_ay24 is max of ay2 */
            var _augval329 = (_cursor313).ay2;
            var _child330 = (_cursor313)._left7;
            if (!((_child330) == null)) {
                var _val331 = (_child330)._max_ay24;
                _augval329 = ((_augval329) < (_val331)) ? (_val331) : (_augval329);
            }
            var _child332 = (_cursor313)._right8;
            if (!((_child332) == null)) {
                var _val333 = (_child332)._max_ay24;
                _augval329 = ((_augval329) < (_val333)) ? (_val333) : (_augval329);
            }
            (_cursor313)._max_ay24 = _augval329;
            (_cursor313)._height10 = 1 + ((((((_cursor313)._left7) == null) ? (-1) : (((_cursor313)._left7)._height10)) > ((((_cursor313)._right8) == null) ? (-1) : (((_cursor313)._right8)._height10))) ? ((((_cursor313)._left7) == null) ? (-1) : (((_cursor313)._left7)._height10)) : ((((_cursor313)._right8) == null) ? (-1) : (((_cursor313)._right8)._height10)));
            _changed314 = false;
            _changed314 = (_changed314) || (!((_old__min_ax12315) == ((_cursor313)._min_ax12)));
            _changed314 = (_changed314) || (!((_old__min_ay13316) == ((_cursor313)._min_ay13)));
            _changed314 = (_changed314) || (!((_old__max_ay24317) == ((_cursor313)._max_ay24)));
            _changed314 = (_changed314) || (!((_old_height318) == ((_cursor313)._height10)));
            _cursor313 = (_cursor313)._parent9;
        }
    }
    var _cursor334 = _parent288;
    var _changed335 = true;
    while ((_changed335) && (!((_cursor334) == (null)))) {
        var _old__min_ax12336 = (_cursor334)._min_ax12;
        var _old__min_ay13337 = (_cursor334)._min_ay13;
        var _old__max_ay24338 = (_cursor334)._max_ay24;
        var _old_height339 = (_cursor334)._height10;
        /* _min_ax12 is min of ax1 */
        var _augval340 = (_cursor334).ax1;
        var _child341 = (_cursor334)._left7;
        if (!((_child341) == null)) {
            var _val342 = (_child341)._min_ax12;
            _augval340 = ((_augval340) < (_val342)) ? (_augval340) : (_val342);
        }
        var _child343 = (_cursor334)._right8;
        if (!((_child343) == null)) {
            var _val344 = (_child343)._min_ax12;
            _augval340 = ((_augval340) < (_val344)) ? (_augval340) : (_val344);
        }
        (_cursor334)._min_ax12 = _augval340;
        /* _min_ay13 is min of ay1 */
        var _augval345 = (_cursor334).ay1;
        var _child346 = (_cursor334)._left7;
        if (!((_child346) == null)) {
            var _val347 = (_child346)._min_ay13;
            _augval345 = ((_augval345) < (_val347)) ? (_augval345) : (_val347);
        }
        var _child348 = (_cursor334)._right8;
        if (!((_child348) == null)) {
            var _val349 = (_child348)._min_ay13;
            _augval345 = ((_augval345) < (_val349)) ? (_augval345) : (_val349);
        }
        (_cursor334)._min_ay13 = _augval345;
        /* _max_ay24 is max of ay2 */
        var _augval350 = (_cursor334).ay2;
        var _child351 = (_cursor334)._left7;
        if (!((_child351) == null)) {
            var _val352 = (_child351)._max_ay24;
            _augval350 = ((_augval350) < (_val352)) ? (_val352) : (_augval350);
        }
        var _child353 = (_cursor334)._right8;
        if (!((_child353) == null)) {
            var _val354 = (_child353)._max_ay24;
            _augval350 = ((_augval350) < (_val354)) ? (_val354) : (_augval350);
        }
        (_cursor334)._max_ay24 = _augval350;
        (_cursor334)._height10 = 1 + ((((((_cursor334)._left7) == null) ? (-1) : (((_cursor334)._left7)._height10)) > ((((_cursor334)._right8) == null) ? (-1) : (((_cursor334)._right8)._height10))) ? ((((_cursor334)._left7) == null) ? (-1) : (((_cursor334)._left7)._height10)) : ((((_cursor334)._right8) == null) ? (-1) : (((_cursor334)._right8)._height10)));
        _changed335 = false;
        _changed335 = (_changed335) || (!((_old__min_ax12336) == ((_cursor334)._min_ax12)));
        _changed335 = (_changed335) || (!((_old__min_ay13337) == ((_cursor334)._min_ay13)));
        _changed335 = (_changed335) || (!((_old__max_ay24338) == ((_cursor334)._max_ay24)));
        _changed335 = (_changed335) || (!((_old_height339) == ((_cursor334)._height10)));
        _cursor334 = (_cursor334)._parent9;
    }
    if (((this)._root1) == (x)) {
        (this)._root1 = _new_x291;
    }
};
RectangleHolder.prototype.updateAx1 = function (__x, new_val) {
    if ((__x).ax1 != new_val) {
        /* _min_ax12 is min of ax1 */
        var _augval355 = new_val;
        var _child356 = (__x)._left7;
        if (!((_child356) == null)) {
            var _val357 = (_child356)._min_ax12;
            _augval355 = ((_augval355) < (_val357)) ? (_augval355) : (_val357);
        }
        var _child358 = (__x)._right8;
        if (!((_child358) == null)) {
            var _val359 = (_child358)._min_ax12;
            _augval355 = ((_augval355) < (_val359)) ? (_augval355) : (_val359);
        }
        (__x)._min_ax12 = _augval355;
        var _cursor360 = (__x)._parent9;
        var _changed361 = true;
        while ((_changed361) && (!((_cursor360) == (null)))) {
            var _old__min_ax12362 = (_cursor360)._min_ax12;
            var _old_height363 = (_cursor360)._height10;
            /* _min_ax12 is min of ax1 */
            var _augval364 = (_cursor360).ax1;
            var _child365 = (_cursor360)._left7;
            if (!((_child365) == null)) {
                var _val366 = (_child365)._min_ax12;
                _augval364 = ((_augval364) < (_val366)) ? (_augval364) : (_val366);
            }
            var _child367 = (_cursor360)._right8;
            if (!((_child367) == null)) {
                var _val368 = (_child367)._min_ax12;
                _augval364 = ((_augval364) < (_val368)) ? (_augval364) : (_val368);
            }
            (_cursor360)._min_ax12 = _augval364;
            (_cursor360)._height10 = 1 + ((((((_cursor360)._left7) == null) ? (-1) : (((_cursor360)._left7)._height10)) > ((((_cursor360)._right8) == null) ? (-1) : (((_cursor360)._right8)._height10))) ? ((((_cursor360)._left7) == null) ? (-1) : (((_cursor360)._left7)._height10)) : ((((_cursor360)._right8) == null) ? (-1) : (((_cursor360)._right8)._height10)));
            _changed361 = false;
            _changed361 = (_changed361) || (!((_old__min_ax12362) == ((_cursor360)._min_ax12)));
            _changed361 = (_changed361) || (!((_old_height363) == ((_cursor360)._height10)));
            _cursor360 = (_cursor360)._parent9;
        }
        (__x).ax1 = new_val;
    }
}
RectangleHolder.prototype.updateAy1 = function (__x, new_val) {
    if ((__x).ay1 != new_val) {
        /* _min_ay13 is min of ay1 */
        var _augval369 = new_val;
        var _child370 = (__x)._left7;
        if (!((_child370) == null)) {
            var _val371 = (_child370)._min_ay13;
            _augval369 = ((_augval369) < (_val371)) ? (_augval369) : (_val371);
        }
        var _child372 = (__x)._right8;
        if (!((_child372) == null)) {
            var _val373 = (_child372)._min_ay13;
            _augval369 = ((_augval369) < (_val373)) ? (_augval369) : (_val373);
        }
        (__x)._min_ay13 = _augval369;
        var _cursor374 = (__x)._parent9;
        var _changed375 = true;
        while ((_changed375) && (!((_cursor374) == (null)))) {
            var _old__min_ay13376 = (_cursor374)._min_ay13;
            var _old_height377 = (_cursor374)._height10;
            /* _min_ay13 is min of ay1 */
            var _augval378 = (_cursor374).ay1;
            var _child379 = (_cursor374)._left7;
            if (!((_child379) == null)) {
                var _val380 = (_child379)._min_ay13;
                _augval378 = ((_augval378) < (_val380)) ? (_augval378) : (_val380);
            }
            var _child381 = (_cursor374)._right8;
            if (!((_child381) == null)) {
                var _val382 = (_child381)._min_ay13;
                _augval378 = ((_augval378) < (_val382)) ? (_augval378) : (_val382);
            }
            (_cursor374)._min_ay13 = _augval378;
            (_cursor374)._height10 = 1 + ((((((_cursor374)._left7) == null) ? (-1) : (((_cursor374)._left7)._height10)) > ((((_cursor374)._right8) == null) ? (-1) : (((_cursor374)._right8)._height10))) ? ((((_cursor374)._left7) == null) ? (-1) : (((_cursor374)._left7)._height10)) : ((((_cursor374)._right8) == null) ? (-1) : (((_cursor374)._right8)._height10)));
            _changed375 = false;
            _changed375 = (_changed375) || (!((_old__min_ay13376) == ((_cursor374)._min_ay13)));
            _changed375 = (_changed375) || (!((_old_height377) == ((_cursor374)._height10)));
            _cursor374 = (_cursor374)._parent9;
        }
        (__x).ay1 = new_val;
    }
}
RectangleHolder.prototype.updateAx2 = function (__x, new_val) {
    if ((__x).ax2 != new_val) {
        var _parent383 = (__x)._parent9;
        var _left384 = (__x)._left7;
        var _right385 = (__x)._right8;
        var _new_x386;
        if (((_left384) == null) && ((_right385) == null)) {
            _new_x386 = null;
            /* replace __x with _new_x386 in _parent383 */
            if (!((_parent383) == null)) {
                if (((_parent383)._left7) == (__x)) {
                    (_parent383)._left7 = _new_x386;
                } else {
                    (_parent383)._right8 = _new_x386;
                }
            }
            if (!((_new_x386) == null)) {
                (_new_x386)._parent9 = _parent383;
            }
        } else if ((!((_left384) == null)) && ((_right385) == null)) {
            _new_x386 = _left384;
            /* replace __x with _new_x386 in _parent383 */
            if (!((_parent383) == null)) {
                if (((_parent383)._left7) == (__x)) {
                    (_parent383)._left7 = _new_x386;
                } else {
                    (_parent383)._right8 = _new_x386;
                }
            }
            if (!((_new_x386) == null)) {
                (_new_x386)._parent9 = _parent383;
            }
        } else if (((_left384) == null) && (!((_right385) == null))) {
            _new_x386 = _right385;
            /* replace __x with _new_x386 in _parent383 */
            if (!((_parent383) == null)) {
                if (((_parent383)._left7) == (__x)) {
                    (_parent383)._left7 = _new_x386;
                } else {
                    (_parent383)._right8 = _new_x386;
                }
            }
            if (!((_new_x386) == null)) {
                (_new_x386)._parent9 = _parent383;
            }
        } else {
            var _root387 = (__x)._right8;
            var _x388 = _root387;
            var _descend389 = true;
            var _from_left390 = true;
            while (true) {
                if ((_x388) == null) {
                    _x388 = null;
                    break;
                }
                if (_descend389) {
                    /* too small? */
                    if (false) {
                        if ((!(((_x388)._right8) == null)) && (true)) {
                            if ((_x388) == (_root387)) {
                                _root387 = (_x388)._right8;
                            }
                            _x388 = (_x388)._right8;
                        } else if ((_x388) == (_root387)) {
                            _x388 = null;
                            break;
                        } else {
                            _descend389 = false;
                            _from_left390 = (!(((_x388)._parent9) == null)) && ((_x388) == (((_x388)._parent9)._left7));
                            _x388 = (_x388)._parent9;
                        }
                    } else if ((!(((_x388)._left7) == null)) && (true)) {
                        _x388 = (_x388)._left7;
                        /* too large? */
                    } else if (false) {
                        if ((_x388) == (_root387)) {
                            _x388 = null;
                            break;
                        } else {
                            _descend389 = false;
                            _from_left390 = (!(((_x388)._parent9) == null)) && ((_x388) == (((_x388)._parent9)._left7));
                            _x388 = (_x388)._parent9;
                        }
                        /* node ok? */
                    } else if (true) {
                        break;
                    } else if ((_x388) == (_root387)) {
                        _root387 = (_x388)._right8;
                        _x388 = (_x388)._right8;
                    } else {
                        if ((!(((_x388)._right8) == null)) && (true)) {
                            if ((_x388) == (_root387)) {
                                _root387 = (_x388)._right8;
                            }
                            _x388 = (_x388)._right8;
                        } else {
                            _descend389 = false;
                            _from_left390 = (!(((_x388)._parent9) == null)) && ((_x388) == (((_x388)._parent9)._left7));
                            _x388 = (_x388)._parent9;
                        }
                    }
                } else if (_from_left390) {
                    if (false) {
                        _x388 = null;
                        break;
                    } else if (true) {
                        break;
                    } else if ((!(((_x388)._right8) == null)) && (true)) {
                        _descend389 = true;
                        if ((_x388) == (_root387)) {
                            _root387 = (_x388)._right8;
                        }
                        _x388 = (_x388)._right8;
                    } else if ((_x388) == (_root387)) {
                        _x388 = null;
                        break;
                    } else {
                        _descend389 = false;
                        _from_left390 = (!(((_x388)._parent9) == null)) && ((_x388) == (((_x388)._parent9)._left7));
                        _x388 = (_x388)._parent9;
                    }
                } else {
                    if ((_x388) == (_root387)) {
                        _x388 = null;
                        break;
                    } else {
                        _descend389 = false;
                        _from_left390 = (!(((_x388)._parent9) == null)) && ((_x388) == (((_x388)._parent9)._left7));
                        _x388 = (_x388)._parent9;
                    }
                }
            }
            _new_x386 = _x388;
            var _mp391 = (_x388)._parent9;
            var _mr392 = (_x388)._right8;
            /* replace _x388 with _mr392 in _mp391 */
            if (!((_mp391) == null)) {
                if (((_mp391)._left7) == (_x388)) {
                    (_mp391)._left7 = _mr392;
                } else {
                    (_mp391)._right8 = _mr392;
                }
            }
            if (!((_mr392) == null)) {
                (_mr392)._parent9 = _mp391;
            }
            /* replace __x with _x388 in _parent383 */
            if (!((_parent383) == null)) {
                if (((_parent383)._left7) == (__x)) {
                    (_parent383)._left7 = _x388;
                } else {
                    (_parent383)._right8 = _x388;
                }
            }
            if (!((_x388) == null)) {
                (_x388)._parent9 = _parent383;
            }
            /* replace null with _left384 in _x388 */
            (_x388)._left7 = _left384;
            if (!((_left384) == null)) {
                (_left384)._parent9 = _x388;
            }
            /* replace _mr392 with (__x)._right8 in _x388 */
            (_x388)._right8 = (__x)._right8;
            if (!(((__x)._right8) == null)) {
                ((__x)._right8)._parent9 = _x388;
            }
            /* _min_ax12 is min of ax1 */
            var _augval393 = (_x388).ax1;
            var _child394 = (_x388)._left7;
            if (!((_child394) == null)) {
                var _val395 = (_child394)._min_ax12;
                _augval393 = ((_augval393) < (_val395)) ? (_augval393) : (_val395);
            }
            var _child396 = (_x388)._right8;
            if (!((_child396) == null)) {
                var _val397 = (_child396)._min_ax12;
                _augval393 = ((_augval393) < (_val397)) ? (_augval393) : (_val397);
            }
            (_x388)._min_ax12 = _augval393;
            /* _min_ay13 is min of ay1 */
            var _augval398 = (_x388).ay1;
            var _child399 = (_x388)._left7;
            if (!((_child399) == null)) {
                var _val400 = (_child399)._min_ay13;
                _augval398 = ((_augval398) < (_val400)) ? (_augval398) : (_val400);
            }
            var _child401 = (_x388)._right8;
            if (!((_child401) == null)) {
                var _val402 = (_child401)._min_ay13;
                _augval398 = ((_augval398) < (_val402)) ? (_augval398) : (_val402);
            }
            (_x388)._min_ay13 = _augval398;
            /* _max_ay24 is max of ay2 */
            var _augval403 = (_x388).ay2;
            var _child404 = (_x388)._left7;
            if (!((_child404) == null)) {
                var _val405 = (_child404)._max_ay24;
                _augval403 = ((_augval403) < (_val405)) ? (_val405) : (_augval403);
            }
            var _child406 = (_x388)._right8;
            if (!((_child406) == null)) {
                var _val407 = (_child406)._max_ay24;
                _augval403 = ((_augval403) < (_val407)) ? (_val407) : (_augval403);
            }
            (_x388)._max_ay24 = _augval403;
            (_x388)._height10 = 1 + ((((((_x388)._left7) == null) ? (-1) : (((_x388)._left7)._height10)) > ((((_x388)._right8) == null) ? (-1) : (((_x388)._right8)._height10))) ? ((((_x388)._left7) == null) ? (-1) : (((_x388)._left7)._height10)) : ((((_x388)._right8) == null) ? (-1) : (((_x388)._right8)._height10)));
            var _cursor408 = _mp391;
            var _changed409 = true;
            while ((_changed409) && (!((_cursor408) == (_parent383)))) {
                var _old__min_ax12410 = (_cursor408)._min_ax12;
                var _old__min_ay13411 = (_cursor408)._min_ay13;
                var _old__max_ay24412 = (_cursor408)._max_ay24;
                var _old_height413 = (_cursor408)._height10;
                /* _min_ax12 is min of ax1 */
                var _augval414 = (_cursor408).ax1;
                var _child415 = (_cursor408)._left7;
                if (!((_child415) == null)) {
                    var _val416 = (_child415)._min_ax12;
                    _augval414 = ((_augval414) < (_val416)) ? (_augval414) : (_val416);
                }
                var _child417 = (_cursor408)._right8;
                if (!((_child417) == null)) {
                    var _val418 = (_child417)._min_ax12;
                    _augval414 = ((_augval414) < (_val418)) ? (_augval414) : (_val418);
                }
                (_cursor408)._min_ax12 = _augval414;
                /* _min_ay13 is min of ay1 */
                var _augval419 = (_cursor408).ay1;
                var _child420 = (_cursor408)._left7;
                if (!((_child420) == null)) {
                    var _val421 = (_child420)._min_ay13;
                    _augval419 = ((_augval419) < (_val421)) ? (_augval419) : (_val421);
                }
                var _child422 = (_cursor408)._right8;
                if (!((_child422) == null)) {
                    var _val423 = (_child422)._min_ay13;
                    _augval419 = ((_augval419) < (_val423)) ? (_augval419) : (_val423);
                }
                (_cursor408)._min_ay13 = _augval419;
                /* _max_ay24 is max of ay2 */
                var _augval424 = (_cursor408).ay2;
                var _child425 = (_cursor408)._left7;
                if (!((_child425) == null)) {
                    var _val426 = (_child425)._max_ay24;
                    _augval424 = ((_augval424) < (_val426)) ? (_val426) : (_augval424);
                }
                var _child427 = (_cursor408)._right8;
                if (!((_child427) == null)) {
                    var _val428 = (_child427)._max_ay24;
                    _augval424 = ((_augval424) < (_val428)) ? (_val428) : (_augval424);
                }
                (_cursor408)._max_ay24 = _augval424;
                (_cursor408)._height10 = 1 + ((((((_cursor408)._left7) == null) ? (-1) : (((_cursor408)._left7)._height10)) > ((((_cursor408)._right8) == null) ? (-1) : (((_cursor408)._right8)._height10))) ? ((((_cursor408)._left7) == null) ? (-1) : (((_cursor408)._left7)._height10)) : ((((_cursor408)._right8) == null) ? (-1) : (((_cursor408)._right8)._height10)));
                _changed409 = false;
                _changed409 = (_changed409) || (!((_old__min_ax12410) == ((_cursor408)._min_ax12)));
                _changed409 = (_changed409) || (!((_old__min_ay13411) == ((_cursor408)._min_ay13)));
                _changed409 = (_changed409) || (!((_old__max_ay24412) == ((_cursor408)._max_ay24)));
                _changed409 = (_changed409) || (!((_old_height413) == ((_cursor408)._height10)));
                _cursor408 = (_cursor408)._parent9;
            }
        }
        var _cursor429 = _parent383;
        var _changed430 = true;
        while ((_changed430) && (!((_cursor429) == (null)))) {
            var _old__min_ax12431 = (_cursor429)._min_ax12;
            var _old__min_ay13432 = (_cursor429)._min_ay13;
            var _old__max_ay24433 = (_cursor429)._max_ay24;
            var _old_height434 = (_cursor429)._height10;
            /* _min_ax12 is min of ax1 */
            var _augval435 = (_cursor429).ax1;
            var _child436 = (_cursor429)._left7;
            if (!((_child436) == null)) {
                var _val437 = (_child436)._min_ax12;
                _augval435 = ((_augval435) < (_val437)) ? (_augval435) : (_val437);
            }
            var _child438 = (_cursor429)._right8;
            if (!((_child438) == null)) {
                var _val439 = (_child438)._min_ax12;
                _augval435 = ((_augval435) < (_val439)) ? (_augval435) : (_val439);
            }
            (_cursor429)._min_ax12 = _augval435;
            /* _min_ay13 is min of ay1 */
            var _augval440 = (_cursor429).ay1;
            var _child441 = (_cursor429)._left7;
            if (!((_child441) == null)) {
                var _val442 = (_child441)._min_ay13;
                _augval440 = ((_augval440) < (_val442)) ? (_augval440) : (_val442);
            }
            var _child443 = (_cursor429)._right8;
            if (!((_child443) == null)) {
                var _val444 = (_child443)._min_ay13;
                _augval440 = ((_augval440) < (_val444)) ? (_augval440) : (_val444);
            }
            (_cursor429)._min_ay13 = _augval440;
            /* _max_ay24 is max of ay2 */
            var _augval445 = (_cursor429).ay2;
            var _child446 = (_cursor429)._left7;
            if (!((_child446) == null)) {
                var _val447 = (_child446)._max_ay24;
                _augval445 = ((_augval445) < (_val447)) ? (_val447) : (_augval445);
            }
            var _child448 = (_cursor429)._right8;
            if (!((_child448) == null)) {
                var _val449 = (_child448)._max_ay24;
                _augval445 = ((_augval445) < (_val449)) ? (_val449) : (_augval445);
            }
            (_cursor429)._max_ay24 = _augval445;
            (_cursor429)._height10 = 1 + ((((((_cursor429)._left7) == null) ? (-1) : (((_cursor429)._left7)._height10)) > ((((_cursor429)._right8) == null) ? (-1) : (((_cursor429)._right8)._height10))) ? ((((_cursor429)._left7) == null) ? (-1) : (((_cursor429)._left7)._height10)) : ((((_cursor429)._right8) == null) ? (-1) : (((_cursor429)._right8)._height10)));
            _changed430 = false;
            _changed430 = (_changed430) || (!((_old__min_ax12431) == ((_cursor429)._min_ax12)));
            _changed430 = (_changed430) || (!((_old__min_ay13432) == ((_cursor429)._min_ay13)));
            _changed430 = (_changed430) || (!((_old__max_ay24433) == ((_cursor429)._max_ay24)));
            _changed430 = (_changed430) || (!((_old_height434) == ((_cursor429)._height10)));
            _cursor429 = (_cursor429)._parent9;
        }
        if (((this)._root1) == (__x)) {
            (this)._root1 = _new_x386;
        }
        (__x)._left7 = null;
        (__x)._right8 = null;
        (__x)._min_ax12 = (__x).ax1;
        (__x)._min_ay13 = (__x).ay1;
        (__x)._max_ay24 = (__x).ay2;
        (__x)._height10 = 0;
        var _previous450 = null;
        var _current451 = (this)._root1;
        var _is_left452 = false;
        while (!((_current451) == null)) {
            _previous450 = _current451;
            if ((new_val) < ((_current451).ax2)) {
                _current451 = (_current451)._left7;
                _is_left452 = true;
            } else {
                _current451 = (_current451)._right8;
                _is_left452 = false;
            }
        }
        if ((_previous450) == null) {
            (this)._root1 = __x;
        } else {
            (__x)._parent9 = _previous450;
            if (_is_left452) {
                (_previous450)._left7 = __x;
            } else {
                (_previous450)._right8 = __x;
            }
        }
        var _cursor453 = (__x)._parent9;
        var _changed454 = true;
        while ((_changed454) && (!((_cursor453) == (null)))) {
            var _old__min_ax12455 = (_cursor453)._min_ax12;
            var _old__min_ay13456 = (_cursor453)._min_ay13;
            var _old__max_ay24457 = (_cursor453)._max_ay24;
            var _old_height458 = (_cursor453)._height10;
            /* _min_ax12 is min of ax1 */
            var _augval459 = (_cursor453).ax1;
            var _child460 = (_cursor453)._left7;
            if (!((_child460) == null)) {
                var _val461 = (_child460)._min_ax12;
                _augval459 = ((_augval459) < (_val461)) ? (_augval459) : (_val461);
            }
            var _child462 = (_cursor453)._right8;
            if (!((_child462) == null)) {
                var _val463 = (_child462)._min_ax12;
                _augval459 = ((_augval459) < (_val463)) ? (_augval459) : (_val463);
            }
            (_cursor453)._min_ax12 = _augval459;
            /* _min_ay13 is min of ay1 */
            var _augval464 = (_cursor453).ay1;
            var _child465 = (_cursor453)._left7;
            if (!((_child465) == null)) {
                var _val466 = (_child465)._min_ay13;
                _augval464 = ((_augval464) < (_val466)) ? (_augval464) : (_val466);
            }
            var _child467 = (_cursor453)._right8;
            if (!((_child467) == null)) {
                var _val468 = (_child467)._min_ay13;
                _augval464 = ((_augval464) < (_val468)) ? (_augval464) : (_val468);
            }
            (_cursor453)._min_ay13 = _augval464;
            /* _max_ay24 is max of ay2 */
            var _augval469 = (_cursor453).ay2;
            var _child470 = (_cursor453)._left7;
            if (!((_child470) == null)) {
                var _val471 = (_child470)._max_ay24;
                _augval469 = ((_augval469) < (_val471)) ? (_val471) : (_augval469);
            }
            var _child472 = (_cursor453)._right8;
            if (!((_child472) == null)) {
                var _val473 = (_child472)._max_ay24;
                _augval469 = ((_augval469) < (_val473)) ? (_val473) : (_augval469);
            }
            (_cursor453)._max_ay24 = _augval469;
            (_cursor453)._height10 = 1 + ((((((_cursor453)._left7) == null) ? (-1) : (((_cursor453)._left7)._height10)) > ((((_cursor453)._right8) == null) ? (-1) : (((_cursor453)._right8)._height10))) ? ((((_cursor453)._left7) == null) ? (-1) : (((_cursor453)._left7)._height10)) : ((((_cursor453)._right8) == null) ? (-1) : (((_cursor453)._right8)._height10)));
            _changed454 = false;
            _changed454 = (_changed454) || (!((_old__min_ax12455) == ((_cursor453)._min_ax12)));
            _changed454 = (_changed454) || (!((_old__min_ay13456) == ((_cursor453)._min_ay13)));
            _changed454 = (_changed454) || (!((_old__max_ay24457) == ((_cursor453)._max_ay24)));
            _changed454 = (_changed454) || (!((_old_height458) == ((_cursor453)._height10)));
            _cursor453 = (_cursor453)._parent9;
        }
        /* rebalance AVL tree */
        var _cursor474 = __x;
        var _imbalance475;
        while (!(((_cursor474)._parent9) == null)) {
            _cursor474 = (_cursor474)._parent9;
            (_cursor474)._height10 = 1 + ((((((_cursor474)._left7) == null) ? (-1) : (((_cursor474)._left7)._height10)) > ((((_cursor474)._right8) == null) ? (-1) : (((_cursor474)._right8)._height10))) ? ((((_cursor474)._left7) == null) ? (-1) : (((_cursor474)._left7)._height10)) : ((((_cursor474)._right8) == null) ? (-1) : (((_cursor474)._right8)._height10)));
            _imbalance475 = ((((_cursor474)._left7) == null) ? (-1) : (((_cursor474)._left7)._height10)) - ((((_cursor474)._right8) == null) ? (-1) : (((_cursor474)._right8)._height10));
            if ((_imbalance475) > (1)) {
                if ((((((_cursor474)._left7)._left7) == null) ? (-1) : ((((_cursor474)._left7)._left7)._height10)) < (((((_cursor474)._left7)._right8) == null) ? (-1) : ((((_cursor474)._left7)._right8)._height10))) {
                    /* rotate ((_cursor474)._left7)._right8 */
                    var _a476 = (_cursor474)._left7;
                    var _b477 = (_a476)._right8;
                    var _c478 = (_b477)._left7;
                    /* replace _a476 with _b477 in (_a476)._parent9 */
                    if (!(((_a476)._parent9) == null)) {
                        if ((((_a476)._parent9)._left7) == (_a476)) {
                            ((_a476)._parent9)._left7 = _b477;
                        } else {
                            ((_a476)._parent9)._right8 = _b477;
                        }
                    }
                    if (!((_b477) == null)) {
                        (_b477)._parent9 = (_a476)._parent9;
                    }
                    /* replace _c478 with _a476 in _b477 */
                    (_b477)._left7 = _a476;
                    if (!((_a476) == null)) {
                        (_a476)._parent9 = _b477;
                    }
                    /* replace _b477 with _c478 in _a476 */
                    (_a476)._right8 = _c478;
                    if (!((_c478) == null)) {
                        (_c478)._parent9 = _a476;
                    }
                    /* _min_ax12 is min of ax1 */
                    var _augval479 = (_a476).ax1;
                    var _child480 = (_a476)._left7;
                    if (!((_child480) == null)) {
                        var _val481 = (_child480)._min_ax12;
                        _augval479 = ((_augval479) < (_val481)) ? (_augval479) : (_val481);
                    }
                    var _child482 = (_a476)._right8;
                    if (!((_child482) == null)) {
                        var _val483 = (_child482)._min_ax12;
                        _augval479 = ((_augval479) < (_val483)) ? (_augval479) : (_val483);
                    }
                    (_a476)._min_ax12 = _augval479;
                    /* _min_ay13 is min of ay1 */
                    var _augval484 = (_a476).ay1;
                    var _child485 = (_a476)._left7;
                    if (!((_child485) == null)) {
                        var _val486 = (_child485)._min_ay13;
                        _augval484 = ((_augval484) < (_val486)) ? (_augval484) : (_val486);
                    }
                    var _child487 = (_a476)._right8;
                    if (!((_child487) == null)) {
                        var _val488 = (_child487)._min_ay13;
                        _augval484 = ((_augval484) < (_val488)) ? (_augval484) : (_val488);
                    }
                    (_a476)._min_ay13 = _augval484;
                    /* _max_ay24 is max of ay2 */
                    var _augval489 = (_a476).ay2;
                    var _child490 = (_a476)._left7;
                    if (!((_child490) == null)) {
                        var _val491 = (_child490)._max_ay24;
                        _augval489 = ((_augval489) < (_val491)) ? (_val491) : (_augval489);
                    }
                    var _child492 = (_a476)._right8;
                    if (!((_child492) == null)) {
                        var _val493 = (_child492)._max_ay24;
                        _augval489 = ((_augval489) < (_val493)) ? (_val493) : (_augval489);
                    }
                    (_a476)._max_ay24 = _augval489;
                    (_a476)._height10 = 1 + ((((((_a476)._left7) == null) ? (-1) : (((_a476)._left7)._height10)) > ((((_a476)._right8) == null) ? (-1) : (((_a476)._right8)._height10))) ? ((((_a476)._left7) == null) ? (-1) : (((_a476)._left7)._height10)) : ((((_a476)._right8) == null) ? (-1) : (((_a476)._right8)._height10)));
                    /* _min_ax12 is min of ax1 */
                    var _augval494 = (_b477).ax1;
                    var _child495 = (_b477)._left7;
                    if (!((_child495) == null)) {
                        var _val496 = (_child495)._min_ax12;
                        _augval494 = ((_augval494) < (_val496)) ? (_augval494) : (_val496);
                    }
                    var _child497 = (_b477)._right8;
                    if (!((_child497) == null)) {
                        var _val498 = (_child497)._min_ax12;
                        _augval494 = ((_augval494) < (_val498)) ? (_augval494) : (_val498);
                    }
                    (_b477)._min_ax12 = _augval494;
                    /* _min_ay13 is min of ay1 */
                    var _augval499 = (_b477).ay1;
                    var _child500 = (_b477)._left7;
                    if (!((_child500) == null)) {
                        var _val501 = (_child500)._min_ay13;
                        _augval499 = ((_augval499) < (_val501)) ? (_augval499) : (_val501);
                    }
                    var _child502 = (_b477)._right8;
                    if (!((_child502) == null)) {
                        var _val503 = (_child502)._min_ay13;
                        _augval499 = ((_augval499) < (_val503)) ? (_augval499) : (_val503);
                    }
                    (_b477)._min_ay13 = _augval499;
                    /* _max_ay24 is max of ay2 */
                    var _augval504 = (_b477).ay2;
                    var _child505 = (_b477)._left7;
                    if (!((_child505) == null)) {
                        var _val506 = (_child505)._max_ay24;
                        _augval504 = ((_augval504) < (_val506)) ? (_val506) : (_augval504);
                    }
                    var _child507 = (_b477)._right8;
                    if (!((_child507) == null)) {
                        var _val508 = (_child507)._max_ay24;
                        _augval504 = ((_augval504) < (_val508)) ? (_val508) : (_augval504);
                    }
                    (_b477)._max_ay24 = _augval504;
                    (_b477)._height10 = 1 + ((((((_b477)._left7) == null) ? (-1) : (((_b477)._left7)._height10)) > ((((_b477)._right8) == null) ? (-1) : (((_b477)._right8)._height10))) ? ((((_b477)._left7) == null) ? (-1) : (((_b477)._left7)._height10)) : ((((_b477)._right8) == null) ? (-1) : (((_b477)._right8)._height10)));
                    if (!(((_b477)._parent9) == null)) {
                        /* _min_ax12 is min of ax1 */
                        var _augval509 = ((_b477)._parent9).ax1;
                        var _child510 = ((_b477)._parent9)._left7;
                        if (!((_child510) == null)) {
                            var _val511 = (_child510)._min_ax12;
                            _augval509 = ((_augval509) < (_val511)) ? (_augval509) : (_val511);
                        }
                        var _child512 = ((_b477)._parent9)._right8;
                        if (!((_child512) == null)) {
                            var _val513 = (_child512)._min_ax12;
                            _augval509 = ((_augval509) < (_val513)) ? (_augval509) : (_val513);
                        }
                        ((_b477)._parent9)._min_ax12 = _augval509;
                        /* _min_ay13 is min of ay1 */
                        var _augval514 = ((_b477)._parent9).ay1;
                        var _child515 = ((_b477)._parent9)._left7;
                        if (!((_child515) == null)) {
                            var _val516 = (_child515)._min_ay13;
                            _augval514 = ((_augval514) < (_val516)) ? (_augval514) : (_val516);
                        }
                        var _child517 = ((_b477)._parent9)._right8;
                        if (!((_child517) == null)) {
                            var _val518 = (_child517)._min_ay13;
                            _augval514 = ((_augval514) < (_val518)) ? (_augval514) : (_val518);
                        }
                        ((_b477)._parent9)._min_ay13 = _augval514;
                        /* _max_ay24 is max of ay2 */
                        var _augval519 = ((_b477)._parent9).ay2;
                        var _child520 = ((_b477)._parent9)._left7;
                        if (!((_child520) == null)) {
                            var _val521 = (_child520)._max_ay24;
                            _augval519 = ((_augval519) < (_val521)) ? (_val521) : (_augval519);
                        }
                        var _child522 = ((_b477)._parent9)._right8;
                        if (!((_child522) == null)) {
                            var _val523 = (_child522)._max_ay24;
                            _augval519 = ((_augval519) < (_val523)) ? (_val523) : (_augval519);
                        }
                        ((_b477)._parent9)._max_ay24 = _augval519;
                        ((_b477)._parent9)._height10 = 1 + (((((((_b477)._parent9)._left7) == null) ? (-1) : ((((_b477)._parent9)._left7)._height10)) > (((((_b477)._parent9)._right8) == null) ? (-1) : ((((_b477)._parent9)._right8)._height10))) ? (((((_b477)._parent9)._left7) == null) ? (-1) : ((((_b477)._parent9)._left7)._height10)) : (((((_b477)._parent9)._right8) == null) ? (-1) : ((((_b477)._parent9)._right8)._height10)));
                    } else {
                        (this)._root1 = _b477;
                    }
                }
                /* rotate (_cursor474)._left7 */
                var _a524 = _cursor474;
                var _b525 = (_a524)._left7;
                var _c526 = (_b525)._right8;
                /* replace _a524 with _b525 in (_a524)._parent9 */
                if (!(((_a524)._parent9) == null)) {
                    if ((((_a524)._parent9)._left7) == (_a524)) {
                        ((_a524)._parent9)._left7 = _b525;
                    } else {
                        ((_a524)._parent9)._right8 = _b525;
                    }
                }
                if (!((_b525) == null)) {
                    (_b525)._parent9 = (_a524)._parent9;
                }
                /* replace _c526 with _a524 in _b525 */
                (_b525)._right8 = _a524;
                if (!((_a524) == null)) {
                    (_a524)._parent9 = _b525;
                }
                /* replace _b525 with _c526 in _a524 */
                (_a524)._left7 = _c526;
                if (!((_c526) == null)) {
                    (_c526)._parent9 = _a524;
                }
                /* _min_ax12 is min of ax1 */
                var _augval527 = (_a524).ax1;
                var _child528 = (_a524)._left7;
                if (!((_child528) == null)) {
                    var _val529 = (_child528)._min_ax12;
                    _augval527 = ((_augval527) < (_val529)) ? (_augval527) : (_val529);
                }
                var _child530 = (_a524)._right8;
                if (!((_child530) == null)) {
                    var _val531 = (_child530)._min_ax12;
                    _augval527 = ((_augval527) < (_val531)) ? (_augval527) : (_val531);
                }
                (_a524)._min_ax12 = _augval527;
                /* _min_ay13 is min of ay1 */
                var _augval532 = (_a524).ay1;
                var _child533 = (_a524)._left7;
                if (!((_child533) == null)) {
                    var _val534 = (_child533)._min_ay13;
                    _augval532 = ((_augval532) < (_val534)) ? (_augval532) : (_val534);
                }
                var _child535 = (_a524)._right8;
                if (!((_child535) == null)) {
                    var _val536 = (_child535)._min_ay13;
                    _augval532 = ((_augval532) < (_val536)) ? (_augval532) : (_val536);
                }
                (_a524)._min_ay13 = _augval532;
                /* _max_ay24 is max of ay2 */
                var _augval537 = (_a524).ay2;
                var _child538 = (_a524)._left7;
                if (!((_child538) == null)) {
                    var _val539 = (_child538)._max_ay24;
                    _augval537 = ((_augval537) < (_val539)) ? (_val539) : (_augval537);
                }
                var _child540 = (_a524)._right8;
                if (!((_child540) == null)) {
                    var _val541 = (_child540)._max_ay24;
                    _augval537 = ((_augval537) < (_val541)) ? (_val541) : (_augval537);
                }
                (_a524)._max_ay24 = _augval537;
                (_a524)._height10 = 1 + ((((((_a524)._left7) == null) ? (-1) : (((_a524)._left7)._height10)) > ((((_a524)._right8) == null) ? (-1) : (((_a524)._right8)._height10))) ? ((((_a524)._left7) == null) ? (-1) : (((_a524)._left7)._height10)) : ((((_a524)._right8) == null) ? (-1) : (((_a524)._right8)._height10)));
                /* _min_ax12 is min of ax1 */
                var _augval542 = (_b525).ax1;
                var _child543 = (_b525)._left7;
                if (!((_child543) == null)) {
                    var _val544 = (_child543)._min_ax12;
                    _augval542 = ((_augval542) < (_val544)) ? (_augval542) : (_val544);
                }
                var _child545 = (_b525)._right8;
                if (!((_child545) == null)) {
                    var _val546 = (_child545)._min_ax12;
                    _augval542 = ((_augval542) < (_val546)) ? (_augval542) : (_val546);
                }
                (_b525)._min_ax12 = _augval542;
                /* _min_ay13 is min of ay1 */
                var _augval547 = (_b525).ay1;
                var _child548 = (_b525)._left7;
                if (!((_child548) == null)) {
                    var _val549 = (_child548)._min_ay13;
                    _augval547 = ((_augval547) < (_val549)) ? (_augval547) : (_val549);
                }
                var _child550 = (_b525)._right8;
                if (!((_child550) == null)) {
                    var _val551 = (_child550)._min_ay13;
                    _augval547 = ((_augval547) < (_val551)) ? (_augval547) : (_val551);
                }
                (_b525)._min_ay13 = _augval547;
                /* _max_ay24 is max of ay2 */
                var _augval552 = (_b525).ay2;
                var _child553 = (_b525)._left7;
                if (!((_child553) == null)) {
                    var _val554 = (_child553)._max_ay24;
                    _augval552 = ((_augval552) < (_val554)) ? (_val554) : (_augval552);
                }
                var _child555 = (_b525)._right8;
                if (!((_child555) == null)) {
                    var _val556 = (_child555)._max_ay24;
                    _augval552 = ((_augval552) < (_val556)) ? (_val556) : (_augval552);
                }
                (_b525)._max_ay24 = _augval552;
                (_b525)._height10 = 1 + ((((((_b525)._left7) == null) ? (-1) : (((_b525)._left7)._height10)) > ((((_b525)._right8) == null) ? (-1) : (((_b525)._right8)._height10))) ? ((((_b525)._left7) == null) ? (-1) : (((_b525)._left7)._height10)) : ((((_b525)._right8) == null) ? (-1) : (((_b525)._right8)._height10)));
                if (!(((_b525)._parent9) == null)) {
                    /* _min_ax12 is min of ax1 */
                    var _augval557 = ((_b525)._parent9).ax1;
                    var _child558 = ((_b525)._parent9)._left7;
                    if (!((_child558) == null)) {
                        var _val559 = (_child558)._min_ax12;
                        _augval557 = ((_augval557) < (_val559)) ? (_augval557) : (_val559);
                    }
                    var _child560 = ((_b525)._parent9)._right8;
                    if (!((_child560) == null)) {
                        var _val561 = (_child560)._min_ax12;
                        _augval557 = ((_augval557) < (_val561)) ? (_augval557) : (_val561);
                    }
                    ((_b525)._parent9)._min_ax12 = _augval557;
                    /* _min_ay13 is min of ay1 */
                    var _augval562 = ((_b525)._parent9).ay1;
                    var _child563 = ((_b525)._parent9)._left7;
                    if (!((_child563) == null)) {
                        var _val564 = (_child563)._min_ay13;
                        _augval562 = ((_augval562) < (_val564)) ? (_augval562) : (_val564);
                    }
                    var _child565 = ((_b525)._parent9)._right8;
                    if (!((_child565) == null)) {
                        var _val566 = (_child565)._min_ay13;
                        _augval562 = ((_augval562) < (_val566)) ? (_augval562) : (_val566);
                    }
                    ((_b525)._parent9)._min_ay13 = _augval562;
                    /* _max_ay24 is max of ay2 */
                    var _augval567 = ((_b525)._parent9).ay2;
                    var _child568 = ((_b525)._parent9)._left7;
                    if (!((_child568) == null)) {
                        var _val569 = (_child568)._max_ay24;
                        _augval567 = ((_augval567) < (_val569)) ? (_val569) : (_augval567);
                    }
                    var _child570 = ((_b525)._parent9)._right8;
                    if (!((_child570) == null)) {
                        var _val571 = (_child570)._max_ay24;
                        _augval567 = ((_augval567) < (_val571)) ? (_val571) : (_augval567);
                    }
                    ((_b525)._parent9)._max_ay24 = _augval567;
                    ((_b525)._parent9)._height10 = 1 + (((((((_b525)._parent9)._left7) == null) ? (-1) : ((((_b525)._parent9)._left7)._height10)) > (((((_b525)._parent9)._right8) == null) ? (-1) : ((((_b525)._parent9)._right8)._height10))) ? (((((_b525)._parent9)._left7) == null) ? (-1) : ((((_b525)._parent9)._left7)._height10)) : (((((_b525)._parent9)._right8) == null) ? (-1) : ((((_b525)._parent9)._right8)._height10)));
                } else {
                    (this)._root1 = _b525;
                }
                _cursor474 = (_cursor474)._parent9;
            } else if ((_imbalance475) < (-1)) {
                if ((((((_cursor474)._right8)._left7) == null) ? (-1) : ((((_cursor474)._right8)._left7)._height10)) > (((((_cursor474)._right8)._right8) == null) ? (-1) : ((((_cursor474)._right8)._right8)._height10))) {
                    /* rotate ((_cursor474)._right8)._left7 */
                    var _a572 = (_cursor474)._right8;
                    var _b573 = (_a572)._left7;
                    var _c574 = (_b573)._right8;
                    /* replace _a572 with _b573 in (_a572)._parent9 */
                    if (!(((_a572)._parent9) == null)) {
                        if ((((_a572)._parent9)._left7) == (_a572)) {
                            ((_a572)._parent9)._left7 = _b573;
                        } else {
                            ((_a572)._parent9)._right8 = _b573;
                        }
                    }
                    if (!((_b573) == null)) {
                        (_b573)._parent9 = (_a572)._parent9;
                    }
                    /* replace _c574 with _a572 in _b573 */
                    (_b573)._right8 = _a572;
                    if (!((_a572) == null)) {
                        (_a572)._parent9 = _b573;
                    }
                    /* replace _b573 with _c574 in _a572 */
                    (_a572)._left7 = _c574;
                    if (!((_c574) == null)) {
                        (_c574)._parent9 = _a572;
                    }
                    /* _min_ax12 is min of ax1 */
                    var _augval575 = (_a572).ax1;
                    var _child576 = (_a572)._left7;
                    if (!((_child576) == null)) {
                        var _val577 = (_child576)._min_ax12;
                        _augval575 = ((_augval575) < (_val577)) ? (_augval575) : (_val577);
                    }
                    var _child578 = (_a572)._right8;
                    if (!((_child578) == null)) {
                        var _val579 = (_child578)._min_ax12;
                        _augval575 = ((_augval575) < (_val579)) ? (_augval575) : (_val579);
                    }
                    (_a572)._min_ax12 = _augval575;
                    /* _min_ay13 is min of ay1 */
                    var _augval580 = (_a572).ay1;
                    var _child581 = (_a572)._left7;
                    if (!((_child581) == null)) {
                        var _val582 = (_child581)._min_ay13;
                        _augval580 = ((_augval580) < (_val582)) ? (_augval580) : (_val582);
                    }
                    var _child583 = (_a572)._right8;
                    if (!((_child583) == null)) {
                        var _val584 = (_child583)._min_ay13;
                        _augval580 = ((_augval580) < (_val584)) ? (_augval580) : (_val584);
                    }
                    (_a572)._min_ay13 = _augval580;
                    /* _max_ay24 is max of ay2 */
                    var _augval585 = (_a572).ay2;
                    var _child586 = (_a572)._left7;
                    if (!((_child586) == null)) {
                        var _val587 = (_child586)._max_ay24;
                        _augval585 = ((_augval585) < (_val587)) ? (_val587) : (_augval585);
                    }
                    var _child588 = (_a572)._right8;
                    if (!((_child588) == null)) {
                        var _val589 = (_child588)._max_ay24;
                        _augval585 = ((_augval585) < (_val589)) ? (_val589) : (_augval585);
                    }
                    (_a572)._max_ay24 = _augval585;
                    (_a572)._height10 = 1 + ((((((_a572)._left7) == null) ? (-1) : (((_a572)._left7)._height10)) > ((((_a572)._right8) == null) ? (-1) : (((_a572)._right8)._height10))) ? ((((_a572)._left7) == null) ? (-1) : (((_a572)._left7)._height10)) : ((((_a572)._right8) == null) ? (-1) : (((_a572)._right8)._height10)));
                    /* _min_ax12 is min of ax1 */
                    var _augval590 = (_b573).ax1;
                    var _child591 = (_b573)._left7;
                    if (!((_child591) == null)) {
                        var _val592 = (_child591)._min_ax12;
                        _augval590 = ((_augval590) < (_val592)) ? (_augval590) : (_val592);
                    }
                    var _child593 = (_b573)._right8;
                    if (!((_child593) == null)) {
                        var _val594 = (_child593)._min_ax12;
                        _augval590 = ((_augval590) < (_val594)) ? (_augval590) : (_val594);
                    }
                    (_b573)._min_ax12 = _augval590;
                    /* _min_ay13 is min of ay1 */
                    var _augval595 = (_b573).ay1;
                    var _child596 = (_b573)._left7;
                    if (!((_child596) == null)) {
                        var _val597 = (_child596)._min_ay13;
                        _augval595 = ((_augval595) < (_val597)) ? (_augval595) : (_val597);
                    }
                    var _child598 = (_b573)._right8;
                    if (!((_child598) == null)) {
                        var _val599 = (_child598)._min_ay13;
                        _augval595 = ((_augval595) < (_val599)) ? (_augval595) : (_val599);
                    }
                    (_b573)._min_ay13 = _augval595;
                    /* _max_ay24 is max of ay2 */
                    var _augval600 = (_b573).ay2;
                    var _child601 = (_b573)._left7;
                    if (!((_child601) == null)) {
                        var _val602 = (_child601)._max_ay24;
                        _augval600 = ((_augval600) < (_val602)) ? (_val602) : (_augval600);
                    }
                    var _child603 = (_b573)._right8;
                    if (!((_child603) == null)) {
                        var _val604 = (_child603)._max_ay24;
                        _augval600 = ((_augval600) < (_val604)) ? (_val604) : (_augval600);
                    }
                    (_b573)._max_ay24 = _augval600;
                    (_b573)._height10 = 1 + ((((((_b573)._left7) == null) ? (-1) : (((_b573)._left7)._height10)) > ((((_b573)._right8) == null) ? (-1) : (((_b573)._right8)._height10))) ? ((((_b573)._left7) == null) ? (-1) : (((_b573)._left7)._height10)) : ((((_b573)._right8) == null) ? (-1) : (((_b573)._right8)._height10)));
                    if (!(((_b573)._parent9) == null)) {
                        /* _min_ax12 is min of ax1 */
                        var _augval605 = ((_b573)._parent9).ax1;
                        var _child606 = ((_b573)._parent9)._left7;
                        if (!((_child606) == null)) {
                            var _val607 = (_child606)._min_ax12;
                            _augval605 = ((_augval605) < (_val607)) ? (_augval605) : (_val607);
                        }
                        var _child608 = ((_b573)._parent9)._right8;
                        if (!((_child608) == null)) {
                            var _val609 = (_child608)._min_ax12;
                            _augval605 = ((_augval605) < (_val609)) ? (_augval605) : (_val609);
                        }
                        ((_b573)._parent9)._min_ax12 = _augval605;
                        /* _min_ay13 is min of ay1 */
                        var _augval610 = ((_b573)._parent9).ay1;
                        var _child611 = ((_b573)._parent9)._left7;
                        if (!((_child611) == null)) {
                            var _val612 = (_child611)._min_ay13;
                            _augval610 = ((_augval610) < (_val612)) ? (_augval610) : (_val612);
                        }
                        var _child613 = ((_b573)._parent9)._right8;
                        if (!((_child613) == null)) {
                            var _val614 = (_child613)._min_ay13;
                            _augval610 = ((_augval610) < (_val614)) ? (_augval610) : (_val614);
                        }
                        ((_b573)._parent9)._min_ay13 = _augval610;
                        /* _max_ay24 is max of ay2 */
                        var _augval615 = ((_b573)._parent9).ay2;
                        var _child616 = ((_b573)._parent9)._left7;
                        if (!((_child616) == null)) {
                            var _val617 = (_child616)._max_ay24;
                            _augval615 = ((_augval615) < (_val617)) ? (_val617) : (_augval615);
                        }
                        var _child618 = ((_b573)._parent9)._right8;
                        if (!((_child618) == null)) {
                            var _val619 = (_child618)._max_ay24;
                            _augval615 = ((_augval615) < (_val619)) ? (_val619) : (_augval615);
                        }
                        ((_b573)._parent9)._max_ay24 = _augval615;
                        ((_b573)._parent9)._height10 = 1 + (((((((_b573)._parent9)._left7) == null) ? (-1) : ((((_b573)._parent9)._left7)._height10)) > (((((_b573)._parent9)._right8) == null) ? (-1) : ((((_b573)._parent9)._right8)._height10))) ? (((((_b573)._parent9)._left7) == null) ? (-1) : ((((_b573)._parent9)._left7)._height10)) : (((((_b573)._parent9)._right8) == null) ? (-1) : ((((_b573)._parent9)._right8)._height10)));
                    } else {
                        (this)._root1 = _b573;
                    }
                }
                /* rotate (_cursor474)._right8 */
                var _a620 = _cursor474;
                var _b621 = (_a620)._right8;
                var _c622 = (_b621)._left7;
                /* replace _a620 with _b621 in (_a620)._parent9 */
                if (!(((_a620)._parent9) == null)) {
                    if ((((_a620)._parent9)._left7) == (_a620)) {
                        ((_a620)._parent9)._left7 = _b621;
                    } else {
                        ((_a620)._parent9)._right8 = _b621;
                    }
                }
                if (!((_b621) == null)) {
                    (_b621)._parent9 = (_a620)._parent9;
                }
                /* replace _c622 with _a620 in _b621 */
                (_b621)._left7 = _a620;
                if (!((_a620) == null)) {
                    (_a620)._parent9 = _b621;
                }
                /* replace _b621 with _c622 in _a620 */
                (_a620)._right8 = _c622;
                if (!((_c622) == null)) {
                    (_c622)._parent9 = _a620;
                }
                /* _min_ax12 is min of ax1 */
                var _augval623 = (_a620).ax1;
                var _child624 = (_a620)._left7;
                if (!((_child624) == null)) {
                    var _val625 = (_child624)._min_ax12;
                    _augval623 = ((_augval623) < (_val625)) ? (_augval623) : (_val625);
                }
                var _child626 = (_a620)._right8;
                if (!((_child626) == null)) {
                    var _val627 = (_child626)._min_ax12;
                    _augval623 = ((_augval623) < (_val627)) ? (_augval623) : (_val627);
                }
                (_a620)._min_ax12 = _augval623;
                /* _min_ay13 is min of ay1 */
                var _augval628 = (_a620).ay1;
                var _child629 = (_a620)._left7;
                if (!((_child629) == null)) {
                    var _val630 = (_child629)._min_ay13;
                    _augval628 = ((_augval628) < (_val630)) ? (_augval628) : (_val630);
                }
                var _child631 = (_a620)._right8;
                if (!((_child631) == null)) {
                    var _val632 = (_child631)._min_ay13;
                    _augval628 = ((_augval628) < (_val632)) ? (_augval628) : (_val632);
                }
                (_a620)._min_ay13 = _augval628;
                /* _max_ay24 is max of ay2 */
                var _augval633 = (_a620).ay2;
                var _child634 = (_a620)._left7;
                if (!((_child634) == null)) {
                    var _val635 = (_child634)._max_ay24;
                    _augval633 = ((_augval633) < (_val635)) ? (_val635) : (_augval633);
                }
                var _child636 = (_a620)._right8;
                if (!((_child636) == null)) {
                    var _val637 = (_child636)._max_ay24;
                    _augval633 = ((_augval633) < (_val637)) ? (_val637) : (_augval633);
                }
                (_a620)._max_ay24 = _augval633;
                (_a620)._height10 = 1 + ((((((_a620)._left7) == null) ? (-1) : (((_a620)._left7)._height10)) > ((((_a620)._right8) == null) ? (-1) : (((_a620)._right8)._height10))) ? ((((_a620)._left7) == null) ? (-1) : (((_a620)._left7)._height10)) : ((((_a620)._right8) == null) ? (-1) : (((_a620)._right8)._height10)));
                /* _min_ax12 is min of ax1 */
                var _augval638 = (_b621).ax1;
                var _child639 = (_b621)._left7;
                if (!((_child639) == null)) {
                    var _val640 = (_child639)._min_ax12;
                    _augval638 = ((_augval638) < (_val640)) ? (_augval638) : (_val640);
                }
                var _child641 = (_b621)._right8;
                if (!((_child641) == null)) {
                    var _val642 = (_child641)._min_ax12;
                    _augval638 = ((_augval638) < (_val642)) ? (_augval638) : (_val642);
                }
                (_b621)._min_ax12 = _augval638;
                /* _min_ay13 is min of ay1 */
                var _augval643 = (_b621).ay1;
                var _child644 = (_b621)._left7;
                if (!((_child644) == null)) {
                    var _val645 = (_child644)._min_ay13;
                    _augval643 = ((_augval643) < (_val645)) ? (_augval643) : (_val645);
                }
                var _child646 = (_b621)._right8;
                if (!((_child646) == null)) {
                    var _val647 = (_child646)._min_ay13;
                    _augval643 = ((_augval643) < (_val647)) ? (_augval643) : (_val647);
                }
                (_b621)._min_ay13 = _augval643;
                /* _max_ay24 is max of ay2 */
                var _augval648 = (_b621).ay2;
                var _child649 = (_b621)._left7;
                if (!((_child649) == null)) {
                    var _val650 = (_child649)._max_ay24;
                    _augval648 = ((_augval648) < (_val650)) ? (_val650) : (_augval648);
                }
                var _child651 = (_b621)._right8;
                if (!((_child651) == null)) {
                    var _val652 = (_child651)._max_ay24;
                    _augval648 = ((_augval648) < (_val652)) ? (_val652) : (_augval648);
                }
                (_b621)._max_ay24 = _augval648;
                (_b621)._height10 = 1 + ((((((_b621)._left7) == null) ? (-1) : (((_b621)._left7)._height10)) > ((((_b621)._right8) == null) ? (-1) : (((_b621)._right8)._height10))) ? ((((_b621)._left7) == null) ? (-1) : (((_b621)._left7)._height10)) : ((((_b621)._right8) == null) ? (-1) : (((_b621)._right8)._height10)));
                if (!(((_b621)._parent9) == null)) {
                    /* _min_ax12 is min of ax1 */
                    var _augval653 = ((_b621)._parent9).ax1;
                    var _child654 = ((_b621)._parent9)._left7;
                    if (!((_child654) == null)) {
                        var _val655 = (_child654)._min_ax12;
                        _augval653 = ((_augval653) < (_val655)) ? (_augval653) : (_val655);
                    }
                    var _child656 = ((_b621)._parent9)._right8;
                    if (!((_child656) == null)) {
                        var _val657 = (_child656)._min_ax12;
                        _augval653 = ((_augval653) < (_val657)) ? (_augval653) : (_val657);
                    }
                    ((_b621)._parent9)._min_ax12 = _augval653;
                    /* _min_ay13 is min of ay1 */
                    var _augval658 = ((_b621)._parent9).ay1;
                    var _child659 = ((_b621)._parent9)._left7;
                    if (!((_child659) == null)) {
                        var _val660 = (_child659)._min_ay13;
                        _augval658 = ((_augval658) < (_val660)) ? (_augval658) : (_val660);
                    }
                    var _child661 = ((_b621)._parent9)._right8;
                    if (!((_child661) == null)) {
                        var _val662 = (_child661)._min_ay13;
                        _augval658 = ((_augval658) < (_val662)) ? (_augval658) : (_val662);
                    }
                    ((_b621)._parent9)._min_ay13 = _augval658;
                    /* _max_ay24 is max of ay2 */
                    var _augval663 = ((_b621)._parent9).ay2;
                    var _child664 = ((_b621)._parent9)._left7;
                    if (!((_child664) == null)) {
                        var _val665 = (_child664)._max_ay24;
                        _augval663 = ((_augval663) < (_val665)) ? (_val665) : (_augval663);
                    }
                    var _child666 = ((_b621)._parent9)._right8;
                    if (!((_child666) == null)) {
                        var _val667 = (_child666)._max_ay24;
                        _augval663 = ((_augval663) < (_val667)) ? (_val667) : (_augval663);
                    }
                    ((_b621)._parent9)._max_ay24 = _augval663;
                    ((_b621)._parent9)._height10 = 1 + (((((((_b621)._parent9)._left7) == null) ? (-1) : ((((_b621)._parent9)._left7)._height10)) > (((((_b621)._parent9)._right8) == null) ? (-1) : ((((_b621)._parent9)._right8)._height10))) ? (((((_b621)._parent9)._left7) == null) ? (-1) : ((((_b621)._parent9)._left7)._height10)) : (((((_b621)._parent9)._right8) == null) ? (-1) : ((((_b621)._parent9)._right8)._height10)));
                } else {
                    (this)._root1 = _b621;
                }
                _cursor474 = (_cursor474)._parent9;
            }
        }
        (__x).ax2 = new_val;
    }
}
RectangleHolder.prototype.updateAy2 = function (__x, new_val) {
    if ((__x).ay2 != new_val) {
        /* _max_ay24 is max of ay2 */
        var _augval668 = new_val;
        var _child669 = (__x)._left7;
        if (!((_child669) == null)) {
            var _val670 = (_child669)._max_ay24;
            _augval668 = ((_augval668) < (_val670)) ? (_val670) : (_augval668);
        }
        var _child671 = (__x)._right8;
        if (!((_child671) == null)) {
            var _val672 = (_child671)._max_ay24;
            _augval668 = ((_augval668) < (_val672)) ? (_val672) : (_augval668);
        }
        (__x)._max_ay24 = _augval668;
        var _cursor673 = (__x)._parent9;
        var _changed674 = true;
        while ((_changed674) && (!((_cursor673) == (null)))) {
            var _old__max_ay24675 = (_cursor673)._max_ay24;
            var _old_height676 = (_cursor673)._height10;
            /* _max_ay24 is max of ay2 */
            var _augval677 = (_cursor673).ay2;
            var _child678 = (_cursor673)._left7;
            if (!((_child678) == null)) {
                var _val679 = (_child678)._max_ay24;
                _augval677 = ((_augval677) < (_val679)) ? (_val679) : (_augval677);
            }
            var _child680 = (_cursor673)._right8;
            if (!((_child680) == null)) {
                var _val681 = (_child680)._max_ay24;
                _augval677 = ((_augval677) < (_val681)) ? (_val681) : (_augval677);
            }
            (_cursor673)._max_ay24 = _augval677;
            (_cursor673)._height10 = 1 + ((((((_cursor673)._left7) == null) ? (-1) : (((_cursor673)._left7)._height10)) > ((((_cursor673)._right8) == null) ? (-1) : (((_cursor673)._right8)._height10))) ? ((((_cursor673)._left7) == null) ? (-1) : (((_cursor673)._left7)._height10)) : ((((_cursor673)._right8) == null) ? (-1) : (((_cursor673)._right8)._height10)));
            _changed674 = false;
            _changed674 = (_changed674) || (!((_old__max_ay24675) == ((_cursor673)._max_ay24)));
            _changed674 = (_changed674) || (!((_old_height676) == ((_cursor673)._height10)));
            _cursor673 = (_cursor673)._parent9;
        }
        (__x).ay2 = new_val;
    }
}
RectangleHolder.prototype.update = function (__x, ax1, ay1, ax2, ay2) {
    var _parent682 = (__x)._parent9;
    var _left683 = (__x)._left7;
    var _right684 = (__x)._right8;
    var _new_x685;
    if (((_left683) == null) && ((_right684) == null)) {
        _new_x685 = null;
        /* replace __x with _new_x685 in _parent682 */
        if (!((_parent682) == null)) {
            if (((_parent682)._left7) == (__x)) {
                (_parent682)._left7 = _new_x685;
            } else {
                (_parent682)._right8 = _new_x685;
            }
        }
        if (!((_new_x685) == null)) {
            (_new_x685)._parent9 = _parent682;
        }
    } else if ((!((_left683) == null)) && ((_right684) == null)) {
        _new_x685 = _left683;
        /* replace __x with _new_x685 in _parent682 */
        if (!((_parent682) == null)) {
            if (((_parent682)._left7) == (__x)) {
                (_parent682)._left7 = _new_x685;
            } else {
                (_parent682)._right8 = _new_x685;
            }
        }
        if (!((_new_x685) == null)) {
            (_new_x685)._parent9 = _parent682;
        }
    } else if (((_left683) == null) && (!((_right684) == null))) {
        _new_x685 = _right684;
        /* replace __x with _new_x685 in _parent682 */
        if (!((_parent682) == null)) {
            if (((_parent682)._left7) == (__x)) {
                (_parent682)._left7 = _new_x685;
            } else {
                (_parent682)._right8 = _new_x685;
            }
        }
        if (!((_new_x685) == null)) {
            (_new_x685)._parent9 = _parent682;
        }
    } else {
        var _root686 = (__x)._right8;
        var _x687 = _root686;
        var _descend688 = true;
        var _from_left689 = true;
        while (true) {
            if ((_x687) == null) {
                _x687 = null;
                break;
            }
            if (_descend688) {
                /* too small? */
                if (false) {
                    if ((!(((_x687)._right8) == null)) && (true)) {
                        if ((_x687) == (_root686)) {
                            _root686 = (_x687)._right8;
                        }
                        _x687 = (_x687)._right8;
                    } else if ((_x687) == (_root686)) {
                        _x687 = null;
                        break;
                    } else {
                        _descend688 = false;
                        _from_left689 = (!(((_x687)._parent9) == null)) && ((_x687) == (((_x687)._parent9)._left7));
                        _x687 = (_x687)._parent9;
                    }
                } else if ((!(((_x687)._left7) == null)) && (true)) {
                    _x687 = (_x687)._left7;
                    /* too large? */
                } else if (false) {
                    if ((_x687) == (_root686)) {
                        _x687 = null;
                        break;
                    } else {
                        _descend688 = false;
                        _from_left689 = (!(((_x687)._parent9) == null)) && ((_x687) == (((_x687)._parent9)._left7));
                        _x687 = (_x687)._parent9;
                    }
                    /* node ok? */
                } else if (true) {
                    break;
                } else if ((_x687) == (_root686)) {
                    _root686 = (_x687)._right8;
                    _x687 = (_x687)._right8;
                } else {
                    if ((!(((_x687)._right8) == null)) && (true)) {
                        if ((_x687) == (_root686)) {
                            _root686 = (_x687)._right8;
                        }
                        _x687 = (_x687)._right8;
                    } else {
                        _descend688 = false;
                        _from_left689 = (!(((_x687)._parent9) == null)) && ((_x687) == (((_x687)._parent9)._left7));
                        _x687 = (_x687)._parent9;
                    }
                }
            } else if (_from_left689) {
                if (false) {
                    _x687 = null;
                    break;
                } else if (true) {
                    break;
                } else if ((!(((_x687)._right8) == null)) && (true)) {
                    _descend688 = true;
                    if ((_x687) == (_root686)) {
                        _root686 = (_x687)._right8;
                    }
                    _x687 = (_x687)._right8;
                } else if ((_x687) == (_root686)) {
                    _x687 = null;
                    break;
                } else {
                    _descend688 = false;
                    _from_left689 = (!(((_x687)._parent9) == null)) && ((_x687) == (((_x687)._parent9)._left7));
                    _x687 = (_x687)._parent9;
                }
            } else {
                if ((_x687) == (_root686)) {
                    _x687 = null;
                    break;
                } else {
                    _descend688 = false;
                    _from_left689 = (!(((_x687)._parent9) == null)) && ((_x687) == (((_x687)._parent9)._left7));
                    _x687 = (_x687)._parent9;
                }
            }
        }
        _new_x685 = _x687;
        var _mp690 = (_x687)._parent9;
        var _mr691 = (_x687)._right8;
        /* replace _x687 with _mr691 in _mp690 */
        if (!((_mp690) == null)) {
            if (((_mp690)._left7) == (_x687)) {
                (_mp690)._left7 = _mr691;
            } else {
                (_mp690)._right8 = _mr691;
            }
        }
        if (!((_mr691) == null)) {
            (_mr691)._parent9 = _mp690;
        }
        /* replace __x with _x687 in _parent682 */
        if (!((_parent682) == null)) {
            if (((_parent682)._left7) == (__x)) {
                (_parent682)._left7 = _x687;
            } else {
                (_parent682)._right8 = _x687;
            }
        }
        if (!((_x687) == null)) {
            (_x687)._parent9 = _parent682;
        }
        /* replace null with _left683 in _x687 */
        (_x687)._left7 = _left683;
        if (!((_left683) == null)) {
            (_left683)._parent9 = _x687;
        }
        /* replace _mr691 with (__x)._right8 in _x687 */
        (_x687)._right8 = (__x)._right8;
        if (!(((__x)._right8) == null)) {
            ((__x)._right8)._parent9 = _x687;
        }
        /* _min_ax12 is min of ax1 */
        var _augval692 = (_x687).ax1;
        var _child693 = (_x687)._left7;
        if (!((_child693) == null)) {
            var _val694 = (_child693)._min_ax12;
            _augval692 = ((_augval692) < (_val694)) ? (_augval692) : (_val694);
        }
        var _child695 = (_x687)._right8;
        if (!((_child695) == null)) {
            var _val696 = (_child695)._min_ax12;
            _augval692 = ((_augval692) < (_val696)) ? (_augval692) : (_val696);
        }
        (_x687)._min_ax12 = _augval692;
        /* _min_ay13 is min of ay1 */
        var _augval697 = (_x687).ay1;
        var _child698 = (_x687)._left7;
        if (!((_child698) == null)) {
            var _val699 = (_child698)._min_ay13;
            _augval697 = ((_augval697) < (_val699)) ? (_augval697) : (_val699);
        }
        var _child700 = (_x687)._right8;
        if (!((_child700) == null)) {
            var _val701 = (_child700)._min_ay13;
            _augval697 = ((_augval697) < (_val701)) ? (_augval697) : (_val701);
        }
        (_x687)._min_ay13 = _augval697;
        /* _max_ay24 is max of ay2 */
        var _augval702 = (_x687).ay2;
        var _child703 = (_x687)._left7;
        if (!((_child703) == null)) {
            var _val704 = (_child703)._max_ay24;
            _augval702 = ((_augval702) < (_val704)) ? (_val704) : (_augval702);
        }
        var _child705 = (_x687)._right8;
        if (!((_child705) == null)) {
            var _val706 = (_child705)._max_ay24;
            _augval702 = ((_augval702) < (_val706)) ? (_val706) : (_augval702);
        }
        (_x687)._max_ay24 = _augval702;
        (_x687)._height10 = 1 + ((((((_x687)._left7) == null) ? (-1) : (((_x687)._left7)._height10)) > ((((_x687)._right8) == null) ? (-1) : (((_x687)._right8)._height10))) ? ((((_x687)._left7) == null) ? (-1) : (((_x687)._left7)._height10)) : ((((_x687)._right8) == null) ? (-1) : (((_x687)._right8)._height10)));
        var _cursor707 = _mp690;
        var _changed708 = true;
        while ((_changed708) && (!((_cursor707) == (_parent682)))) {
            var _old__min_ax12709 = (_cursor707)._min_ax12;
            var _old__min_ay13710 = (_cursor707)._min_ay13;
            var _old__max_ay24711 = (_cursor707)._max_ay24;
            var _old_height712 = (_cursor707)._height10;
            /* _min_ax12 is min of ax1 */
            var _augval713 = (_cursor707).ax1;
            var _child714 = (_cursor707)._left7;
            if (!((_child714) == null)) {
                var _val715 = (_child714)._min_ax12;
                _augval713 = ((_augval713) < (_val715)) ? (_augval713) : (_val715);
            }
            var _child716 = (_cursor707)._right8;
            if (!((_child716) == null)) {
                var _val717 = (_child716)._min_ax12;
                _augval713 = ((_augval713) < (_val717)) ? (_augval713) : (_val717);
            }
            (_cursor707)._min_ax12 = _augval713;
            /* _min_ay13 is min of ay1 */
            var _augval718 = (_cursor707).ay1;
            var _child719 = (_cursor707)._left7;
            if (!((_child719) == null)) {
                var _val720 = (_child719)._min_ay13;
                _augval718 = ((_augval718) < (_val720)) ? (_augval718) : (_val720);
            }
            var _child721 = (_cursor707)._right8;
            if (!((_child721) == null)) {
                var _val722 = (_child721)._min_ay13;
                _augval718 = ((_augval718) < (_val722)) ? (_augval718) : (_val722);
            }
            (_cursor707)._min_ay13 = _augval718;
            /* _max_ay24 is max of ay2 */
            var _augval723 = (_cursor707).ay2;
            var _child724 = (_cursor707)._left7;
            if (!((_child724) == null)) {
                var _val725 = (_child724)._max_ay24;
                _augval723 = ((_augval723) < (_val725)) ? (_val725) : (_augval723);
            }
            var _child726 = (_cursor707)._right8;
            if (!((_child726) == null)) {
                var _val727 = (_child726)._max_ay24;
                _augval723 = ((_augval723) < (_val727)) ? (_val727) : (_augval723);
            }
            (_cursor707)._max_ay24 = _augval723;
            (_cursor707)._height10 = 1 + ((((((_cursor707)._left7) == null) ? (-1) : (((_cursor707)._left7)._height10)) > ((((_cursor707)._right8) == null) ? (-1) : (((_cursor707)._right8)._height10))) ? ((((_cursor707)._left7) == null) ? (-1) : (((_cursor707)._left7)._height10)) : ((((_cursor707)._right8) == null) ? (-1) : (((_cursor707)._right8)._height10)));
            _changed708 = false;
            _changed708 = (_changed708) || (!((_old__min_ax12709) == ((_cursor707)._min_ax12)));
            _changed708 = (_changed708) || (!((_old__min_ay13710) == ((_cursor707)._min_ay13)));
            _changed708 = (_changed708) || (!((_old__max_ay24711) == ((_cursor707)._max_ay24)));
            _changed708 = (_changed708) || (!((_old_height712) == ((_cursor707)._height10)));
            _cursor707 = (_cursor707)._parent9;
        }
    }
    var _cursor728 = _parent682;
    var _changed729 = true;
    while ((_changed729) && (!((_cursor728) == (null)))) {
        var _old__min_ax12730 = (_cursor728)._min_ax12;
        var _old__min_ay13731 = (_cursor728)._min_ay13;
        var _old__max_ay24732 = (_cursor728)._max_ay24;
        var _old_height733 = (_cursor728)._height10;
        /* _min_ax12 is min of ax1 */
        var _augval734 = (_cursor728).ax1;
        var _child735 = (_cursor728)._left7;
        if (!((_child735) == null)) {
            var _val736 = (_child735)._min_ax12;
            _augval734 = ((_augval734) < (_val736)) ? (_augval734) : (_val736);
        }
        var _child737 = (_cursor728)._right8;
        if (!((_child737) == null)) {
            var _val738 = (_child737)._min_ax12;
            _augval734 = ((_augval734) < (_val738)) ? (_augval734) : (_val738);
        }
        (_cursor728)._min_ax12 = _augval734;
        /* _min_ay13 is min of ay1 */
        var _augval739 = (_cursor728).ay1;
        var _child740 = (_cursor728)._left7;
        if (!((_child740) == null)) {
            var _val741 = (_child740)._min_ay13;
            _augval739 = ((_augval739) < (_val741)) ? (_augval739) : (_val741);
        }
        var _child742 = (_cursor728)._right8;
        if (!((_child742) == null)) {
            var _val743 = (_child742)._min_ay13;
            _augval739 = ((_augval739) < (_val743)) ? (_augval739) : (_val743);
        }
        (_cursor728)._min_ay13 = _augval739;
        /* _max_ay24 is max of ay2 */
        var _augval744 = (_cursor728).ay2;
        var _child745 = (_cursor728)._left7;
        if (!((_child745) == null)) {
            var _val746 = (_child745)._max_ay24;
            _augval744 = ((_augval744) < (_val746)) ? (_val746) : (_augval744);
        }
        var _child747 = (_cursor728)._right8;
        if (!((_child747) == null)) {
            var _val748 = (_child747)._max_ay24;
            _augval744 = ((_augval744) < (_val748)) ? (_val748) : (_augval744);
        }
        (_cursor728)._max_ay24 = _augval744;
        (_cursor728)._height10 = 1 + ((((((_cursor728)._left7) == null) ? (-1) : (((_cursor728)._left7)._height10)) > ((((_cursor728)._right8) == null) ? (-1) : (((_cursor728)._right8)._height10))) ? ((((_cursor728)._left7) == null) ? (-1) : (((_cursor728)._left7)._height10)) : ((((_cursor728)._right8) == null) ? (-1) : (((_cursor728)._right8)._height10)));
        _changed729 = false;
        _changed729 = (_changed729) || (!((_old__min_ax12730) == ((_cursor728)._min_ax12)));
        _changed729 = (_changed729) || (!((_old__min_ay13731) == ((_cursor728)._min_ay13)));
        _changed729 = (_changed729) || (!((_old__max_ay24732) == ((_cursor728)._max_ay24)));
        _changed729 = (_changed729) || (!((_old_height733) == ((_cursor728)._height10)));
        _cursor728 = (_cursor728)._parent9;
    }
    if (((this)._root1) == (__x)) {
        (this)._root1 = _new_x685;
    }
    (__x)._left7 = null;
    (__x)._right8 = null;
    (__x)._min_ax12 = (__x).ax1;
    (__x)._min_ay13 = (__x).ay1;
    (__x)._max_ay24 = (__x).ay2;
    (__x)._height10 = 0;
    var _previous749 = null;
    var _current750 = (this)._root1;
    var _is_left751 = false;
    while (!((_current750) == null)) {
        _previous749 = _current750;
        if ((ax2) < ((_current750).ax2)) {
            _current750 = (_current750)._left7;
            _is_left751 = true;
        } else {
            _current750 = (_current750)._right8;
            _is_left751 = false;
        }
    }
    if ((_previous749) == null) {
        (this)._root1 = __x;
    } else {
        (__x)._parent9 = _previous749;
        if (_is_left751) {
            (_previous749)._left7 = __x;
        } else {
            (_previous749)._right8 = __x;
        }
    }
    var _cursor752 = (__x)._parent9;
    var _changed753 = true;
    while ((_changed753) && (!((_cursor752) == (null)))) {
        var _old__min_ax12754 = (_cursor752)._min_ax12;
        var _old__min_ay13755 = (_cursor752)._min_ay13;
        var _old__max_ay24756 = (_cursor752)._max_ay24;
        var _old_height757 = (_cursor752)._height10;
        /* _min_ax12 is min of ax1 */
        var _augval758 = (_cursor752).ax1;
        var _child759 = (_cursor752)._left7;
        if (!((_child759) == null)) {
            var _val760 = (_child759)._min_ax12;
            _augval758 = ((_augval758) < (_val760)) ? (_augval758) : (_val760);
        }
        var _child761 = (_cursor752)._right8;
        if (!((_child761) == null)) {
            var _val762 = (_child761)._min_ax12;
            _augval758 = ((_augval758) < (_val762)) ? (_augval758) : (_val762);
        }
        (_cursor752)._min_ax12 = _augval758;
        /* _min_ay13 is min of ay1 */
        var _augval763 = (_cursor752).ay1;
        var _child764 = (_cursor752)._left7;
        if (!((_child764) == null)) {
            var _val765 = (_child764)._min_ay13;
            _augval763 = ((_augval763) < (_val765)) ? (_augval763) : (_val765);
        }
        var _child766 = (_cursor752)._right8;
        if (!((_child766) == null)) {
            var _val767 = (_child766)._min_ay13;
            _augval763 = ((_augval763) < (_val767)) ? (_augval763) : (_val767);
        }
        (_cursor752)._min_ay13 = _augval763;
        /* _max_ay24 is max of ay2 */
        var _augval768 = (_cursor752).ay2;
        var _child769 = (_cursor752)._left7;
        if (!((_child769) == null)) {
            var _val770 = (_child769)._max_ay24;
            _augval768 = ((_augval768) < (_val770)) ? (_val770) : (_augval768);
        }
        var _child771 = (_cursor752)._right8;
        if (!((_child771) == null)) {
            var _val772 = (_child771)._max_ay24;
            _augval768 = ((_augval768) < (_val772)) ? (_val772) : (_augval768);
        }
        (_cursor752)._max_ay24 = _augval768;
        (_cursor752)._height10 = 1 + ((((((_cursor752)._left7) == null) ? (-1) : (((_cursor752)._left7)._height10)) > ((((_cursor752)._right8) == null) ? (-1) : (((_cursor752)._right8)._height10))) ? ((((_cursor752)._left7) == null) ? (-1) : (((_cursor752)._left7)._height10)) : ((((_cursor752)._right8) == null) ? (-1) : (((_cursor752)._right8)._height10)));
        _changed753 = false;
        _changed753 = (_changed753) || (!((_old__min_ax12754) == ((_cursor752)._min_ax12)));
        _changed753 = (_changed753) || (!((_old__min_ay13755) == ((_cursor752)._min_ay13)));
        _changed753 = (_changed753) || (!((_old__max_ay24756) == ((_cursor752)._max_ay24)));
        _changed753 = (_changed753) || (!((_old_height757) == ((_cursor752)._height10)));
        _cursor752 = (_cursor752)._parent9;
    }
    /* rebalance AVL tree */
    var _cursor773 = __x;
    var _imbalance774;
    while (!(((_cursor773)._parent9) == null)) {
        _cursor773 = (_cursor773)._parent9;
        (_cursor773)._height10 = 1 + ((((((_cursor773)._left7) == null) ? (-1) : (((_cursor773)._left7)._height10)) > ((((_cursor773)._right8) == null) ? (-1) : (((_cursor773)._right8)._height10))) ? ((((_cursor773)._left7) == null) ? (-1) : (((_cursor773)._left7)._height10)) : ((((_cursor773)._right8) == null) ? (-1) : (((_cursor773)._right8)._height10)));
        _imbalance774 = ((((_cursor773)._left7) == null) ? (-1) : (((_cursor773)._left7)._height10)) - ((((_cursor773)._right8) == null) ? (-1) : (((_cursor773)._right8)._height10));
        if ((_imbalance774) > (1)) {
            if ((((((_cursor773)._left7)._left7) == null) ? (-1) : ((((_cursor773)._left7)._left7)._height10)) < (((((_cursor773)._left7)._right8) == null) ? (-1) : ((((_cursor773)._left7)._right8)._height10))) {
                /* rotate ((_cursor773)._left7)._right8 */
                var _a775 = (_cursor773)._left7;
                var _b776 = (_a775)._right8;
                var _c777 = (_b776)._left7;
                /* replace _a775 with _b776 in (_a775)._parent9 */
                if (!(((_a775)._parent9) == null)) {
                    if ((((_a775)._parent9)._left7) == (_a775)) {
                        ((_a775)._parent9)._left7 = _b776;
                    } else {
                        ((_a775)._parent9)._right8 = _b776;
                    }
                }
                if (!((_b776) == null)) {
                    (_b776)._parent9 = (_a775)._parent9;
                }
                /* replace _c777 with _a775 in _b776 */
                (_b776)._left7 = _a775;
                if (!((_a775) == null)) {
                    (_a775)._parent9 = _b776;
                }
                /* replace _b776 with _c777 in _a775 */
                (_a775)._right8 = _c777;
                if (!((_c777) == null)) {
                    (_c777)._parent9 = _a775;
                }
                /* _min_ax12 is min of ax1 */
                var _augval778 = (_a775).ax1;
                var _child779 = (_a775)._left7;
                if (!((_child779) == null)) {
                    var _val780 = (_child779)._min_ax12;
                    _augval778 = ((_augval778) < (_val780)) ? (_augval778) : (_val780);
                }
                var _child781 = (_a775)._right8;
                if (!((_child781) == null)) {
                    var _val782 = (_child781)._min_ax12;
                    _augval778 = ((_augval778) < (_val782)) ? (_augval778) : (_val782);
                }
                (_a775)._min_ax12 = _augval778;
                /* _min_ay13 is min of ay1 */
                var _augval783 = (_a775).ay1;
                var _child784 = (_a775)._left7;
                if (!((_child784) == null)) {
                    var _val785 = (_child784)._min_ay13;
                    _augval783 = ((_augval783) < (_val785)) ? (_augval783) : (_val785);
                }
                var _child786 = (_a775)._right8;
                if (!((_child786) == null)) {
                    var _val787 = (_child786)._min_ay13;
                    _augval783 = ((_augval783) < (_val787)) ? (_augval783) : (_val787);
                }
                (_a775)._min_ay13 = _augval783;
                /* _max_ay24 is max of ay2 */
                var _augval788 = (_a775).ay2;
                var _child789 = (_a775)._left7;
                if (!((_child789) == null)) {
                    var _val790 = (_child789)._max_ay24;
                    _augval788 = ((_augval788) < (_val790)) ? (_val790) : (_augval788);
                }
                var _child791 = (_a775)._right8;
                if (!((_child791) == null)) {
                    var _val792 = (_child791)._max_ay24;
                    _augval788 = ((_augval788) < (_val792)) ? (_val792) : (_augval788);
                }
                (_a775)._max_ay24 = _augval788;
                (_a775)._height10 = 1 + ((((((_a775)._left7) == null) ? (-1) : (((_a775)._left7)._height10)) > ((((_a775)._right8) == null) ? (-1) : (((_a775)._right8)._height10))) ? ((((_a775)._left7) == null) ? (-1) : (((_a775)._left7)._height10)) : ((((_a775)._right8) == null) ? (-1) : (((_a775)._right8)._height10)));
                /* _min_ax12 is min of ax1 */
                var _augval793 = (_b776).ax1;
                var _child794 = (_b776)._left7;
                if (!((_child794) == null)) {
                    var _val795 = (_child794)._min_ax12;
                    _augval793 = ((_augval793) < (_val795)) ? (_augval793) : (_val795);
                }
                var _child796 = (_b776)._right8;
                if (!((_child796) == null)) {
                    var _val797 = (_child796)._min_ax12;
                    _augval793 = ((_augval793) < (_val797)) ? (_augval793) : (_val797);
                }
                (_b776)._min_ax12 = _augval793;
                /* _min_ay13 is min of ay1 */
                var _augval798 = (_b776).ay1;
                var _child799 = (_b776)._left7;
                if (!((_child799) == null)) {
                    var _val800 = (_child799)._min_ay13;
                    _augval798 = ((_augval798) < (_val800)) ? (_augval798) : (_val800);
                }
                var _child801 = (_b776)._right8;
                if (!((_child801) == null)) {
                    var _val802 = (_child801)._min_ay13;
                    _augval798 = ((_augval798) < (_val802)) ? (_augval798) : (_val802);
                }
                (_b776)._min_ay13 = _augval798;
                /* _max_ay24 is max of ay2 */
                var _augval803 = (_b776).ay2;
                var _child804 = (_b776)._left7;
                if (!((_child804) == null)) {
                    var _val805 = (_child804)._max_ay24;
                    _augval803 = ((_augval803) < (_val805)) ? (_val805) : (_augval803);
                }
                var _child806 = (_b776)._right8;
                if (!((_child806) == null)) {
                    var _val807 = (_child806)._max_ay24;
                    _augval803 = ((_augval803) < (_val807)) ? (_val807) : (_augval803);
                }
                (_b776)._max_ay24 = _augval803;
                (_b776)._height10 = 1 + ((((((_b776)._left7) == null) ? (-1) : (((_b776)._left7)._height10)) > ((((_b776)._right8) == null) ? (-1) : (((_b776)._right8)._height10))) ? ((((_b776)._left7) == null) ? (-1) : (((_b776)._left7)._height10)) : ((((_b776)._right8) == null) ? (-1) : (((_b776)._right8)._height10)));
                if (!(((_b776)._parent9) == null)) {
                    /* _min_ax12 is min of ax1 */
                    var _augval808 = ((_b776)._parent9).ax1;
                    var _child809 = ((_b776)._parent9)._left7;
                    if (!((_child809) == null)) {
                        var _val810 = (_child809)._min_ax12;
                        _augval808 = ((_augval808) < (_val810)) ? (_augval808) : (_val810);
                    }
                    var _child811 = ((_b776)._parent9)._right8;
                    if (!((_child811) == null)) {
                        var _val812 = (_child811)._min_ax12;
                        _augval808 = ((_augval808) < (_val812)) ? (_augval808) : (_val812);
                    }
                    ((_b776)._parent9)._min_ax12 = _augval808;
                    /* _min_ay13 is min of ay1 */
                    var _augval813 = ((_b776)._parent9).ay1;
                    var _child814 = ((_b776)._parent9)._left7;
                    if (!((_child814) == null)) {
                        var _val815 = (_child814)._min_ay13;
                        _augval813 = ((_augval813) < (_val815)) ? (_augval813) : (_val815);
                    }
                    var _child816 = ((_b776)._parent9)._right8;
                    if (!((_child816) == null)) {
                        var _val817 = (_child816)._min_ay13;
                        _augval813 = ((_augval813) < (_val817)) ? (_augval813) : (_val817);
                    }
                    ((_b776)._parent9)._min_ay13 = _augval813;
                    /* _max_ay24 is max of ay2 */
                    var _augval818 = ((_b776)._parent9).ay2;
                    var _child819 = ((_b776)._parent9)._left7;
                    if (!((_child819) == null)) {
                        var _val820 = (_child819)._max_ay24;
                        _augval818 = ((_augval818) < (_val820)) ? (_val820) : (_augval818);
                    }
                    var _child821 = ((_b776)._parent9)._right8;
                    if (!((_child821) == null)) {
                        var _val822 = (_child821)._max_ay24;
                        _augval818 = ((_augval818) < (_val822)) ? (_val822) : (_augval818);
                    }
                    ((_b776)._parent9)._max_ay24 = _augval818;
                    ((_b776)._parent9)._height10 = 1 + (((((((_b776)._parent9)._left7) == null) ? (-1) : ((((_b776)._parent9)._left7)._height10)) > (((((_b776)._parent9)._right8) == null) ? (-1) : ((((_b776)._parent9)._right8)._height10))) ? (((((_b776)._parent9)._left7) == null) ? (-1) : ((((_b776)._parent9)._left7)._height10)) : (((((_b776)._parent9)._right8) == null) ? (-1) : ((((_b776)._parent9)._right8)._height10)));
                } else {
                    (this)._root1 = _b776;
                }
            }
            /* rotate (_cursor773)._left7 */
            var _a823 = _cursor773;
            var _b824 = (_a823)._left7;
            var _c825 = (_b824)._right8;
            /* replace _a823 with _b824 in (_a823)._parent9 */
            if (!(((_a823)._parent9) == null)) {
                if ((((_a823)._parent9)._left7) == (_a823)) {
                    ((_a823)._parent9)._left7 = _b824;
                } else {
                    ((_a823)._parent9)._right8 = _b824;
                }
            }
            if (!((_b824) == null)) {
                (_b824)._parent9 = (_a823)._parent9;
            }
            /* replace _c825 with _a823 in _b824 */
            (_b824)._right8 = _a823;
            if (!((_a823) == null)) {
                (_a823)._parent9 = _b824;
            }
            /* replace _b824 with _c825 in _a823 */
            (_a823)._left7 = _c825;
            if (!((_c825) == null)) {
                (_c825)._parent9 = _a823;
            }
            /* _min_ax12 is min of ax1 */
            var _augval826 = (_a823).ax1;
            var _child827 = (_a823)._left7;
            if (!((_child827) == null)) {
                var _val828 = (_child827)._min_ax12;
                _augval826 = ((_augval826) < (_val828)) ? (_augval826) : (_val828);
            }
            var _child829 = (_a823)._right8;
            if (!((_child829) == null)) {
                var _val830 = (_child829)._min_ax12;
                _augval826 = ((_augval826) < (_val830)) ? (_augval826) : (_val830);
            }
            (_a823)._min_ax12 = _augval826;
            /* _min_ay13 is min of ay1 */
            var _augval831 = (_a823).ay1;
            var _child832 = (_a823)._left7;
            if (!((_child832) == null)) {
                var _val833 = (_child832)._min_ay13;
                _augval831 = ((_augval831) < (_val833)) ? (_augval831) : (_val833);
            }
            var _child834 = (_a823)._right8;
            if (!((_child834) == null)) {
                var _val835 = (_child834)._min_ay13;
                _augval831 = ((_augval831) < (_val835)) ? (_augval831) : (_val835);
            }
            (_a823)._min_ay13 = _augval831;
            /* _max_ay24 is max of ay2 */
            var _augval836 = (_a823).ay2;
            var _child837 = (_a823)._left7;
            if (!((_child837) == null)) {
                var _val838 = (_child837)._max_ay24;
                _augval836 = ((_augval836) < (_val838)) ? (_val838) : (_augval836);
            }
            var _child839 = (_a823)._right8;
            if (!((_child839) == null)) {
                var _val840 = (_child839)._max_ay24;
                _augval836 = ((_augval836) < (_val840)) ? (_val840) : (_augval836);
            }
            (_a823)._max_ay24 = _augval836;
            (_a823)._height10 = 1 + ((((((_a823)._left7) == null) ? (-1) : (((_a823)._left7)._height10)) > ((((_a823)._right8) == null) ? (-1) : (((_a823)._right8)._height10))) ? ((((_a823)._left7) == null) ? (-1) : (((_a823)._left7)._height10)) : ((((_a823)._right8) == null) ? (-1) : (((_a823)._right8)._height10)));
            /* _min_ax12 is min of ax1 */
            var _augval841 = (_b824).ax1;
            var _child842 = (_b824)._left7;
            if (!((_child842) == null)) {
                var _val843 = (_child842)._min_ax12;
                _augval841 = ((_augval841) < (_val843)) ? (_augval841) : (_val843);
            }
            var _child844 = (_b824)._right8;
            if (!((_child844) == null)) {
                var _val845 = (_child844)._min_ax12;
                _augval841 = ((_augval841) < (_val845)) ? (_augval841) : (_val845);
            }
            (_b824)._min_ax12 = _augval841;
            /* _min_ay13 is min of ay1 */
            var _augval846 = (_b824).ay1;
            var _child847 = (_b824)._left7;
            if (!((_child847) == null)) {
                var _val848 = (_child847)._min_ay13;
                _augval846 = ((_augval846) < (_val848)) ? (_augval846) : (_val848);
            }
            var _child849 = (_b824)._right8;
            if (!((_child849) == null)) {
                var _val850 = (_child849)._min_ay13;
                _augval846 = ((_augval846) < (_val850)) ? (_augval846) : (_val850);
            }
            (_b824)._min_ay13 = _augval846;
            /* _max_ay24 is max of ay2 */
            var _augval851 = (_b824).ay2;
            var _child852 = (_b824)._left7;
            if (!((_child852) == null)) {
                var _val853 = (_child852)._max_ay24;
                _augval851 = ((_augval851) < (_val853)) ? (_val853) : (_augval851);
            }
            var _child854 = (_b824)._right8;
            if (!((_child854) == null)) {
                var _val855 = (_child854)._max_ay24;
                _augval851 = ((_augval851) < (_val855)) ? (_val855) : (_augval851);
            }
            (_b824)._max_ay24 = _augval851;
            (_b824)._height10 = 1 + ((((((_b824)._left7) == null) ? (-1) : (((_b824)._left7)._height10)) > ((((_b824)._right8) == null) ? (-1) : (((_b824)._right8)._height10))) ? ((((_b824)._left7) == null) ? (-1) : (((_b824)._left7)._height10)) : ((((_b824)._right8) == null) ? (-1) : (((_b824)._right8)._height10)));
            if (!(((_b824)._parent9) == null)) {
                /* _min_ax12 is min of ax1 */
                var _augval856 = ((_b824)._parent9).ax1;
                var _child857 = ((_b824)._parent9)._left7;
                if (!((_child857) == null)) {
                    var _val858 = (_child857)._min_ax12;
                    _augval856 = ((_augval856) < (_val858)) ? (_augval856) : (_val858);
                }
                var _child859 = ((_b824)._parent9)._right8;
                if (!((_child859) == null)) {
                    var _val860 = (_child859)._min_ax12;
                    _augval856 = ((_augval856) < (_val860)) ? (_augval856) : (_val860);
                }
                ((_b824)._parent9)._min_ax12 = _augval856;
                /* _min_ay13 is min of ay1 */
                var _augval861 = ((_b824)._parent9).ay1;
                var _child862 = ((_b824)._parent9)._left7;
                if (!((_child862) == null)) {
                    var _val863 = (_child862)._min_ay13;
                    _augval861 = ((_augval861) < (_val863)) ? (_augval861) : (_val863);
                }
                var _child864 = ((_b824)._parent9)._right8;
                if (!((_child864) == null)) {
                    var _val865 = (_child864)._min_ay13;
                    _augval861 = ((_augval861) < (_val865)) ? (_augval861) : (_val865);
                }
                ((_b824)._parent9)._min_ay13 = _augval861;
                /* _max_ay24 is max of ay2 */
                var _augval866 = ((_b824)._parent9).ay2;
                var _child867 = ((_b824)._parent9)._left7;
                if (!((_child867) == null)) {
                    var _val868 = (_child867)._max_ay24;
                    _augval866 = ((_augval866) < (_val868)) ? (_val868) : (_augval866);
                }
                var _child869 = ((_b824)._parent9)._right8;
                if (!((_child869) == null)) {
                    var _val870 = (_child869)._max_ay24;
                    _augval866 = ((_augval866) < (_val870)) ? (_val870) : (_augval866);
                }
                ((_b824)._parent9)._max_ay24 = _augval866;
                ((_b824)._parent9)._height10 = 1 + (((((((_b824)._parent9)._left7) == null) ? (-1) : ((((_b824)._parent9)._left7)._height10)) > (((((_b824)._parent9)._right8) == null) ? (-1) : ((((_b824)._parent9)._right8)._height10))) ? (((((_b824)._parent9)._left7) == null) ? (-1) : ((((_b824)._parent9)._left7)._height10)) : (((((_b824)._parent9)._right8) == null) ? (-1) : ((((_b824)._parent9)._right8)._height10)));
            } else {
                (this)._root1 = _b824;
            }
            _cursor773 = (_cursor773)._parent9;
        } else if ((_imbalance774) < (-1)) {
            if ((((((_cursor773)._right8)._left7) == null) ? (-1) : ((((_cursor773)._right8)._left7)._height10)) > (((((_cursor773)._right8)._right8) == null) ? (-1) : ((((_cursor773)._right8)._right8)._height10))) {
                /* rotate ((_cursor773)._right8)._left7 */
                var _a871 = (_cursor773)._right8;
                var _b872 = (_a871)._left7;
                var _c873 = (_b872)._right8;
                /* replace _a871 with _b872 in (_a871)._parent9 */
                if (!(((_a871)._parent9) == null)) {
                    if ((((_a871)._parent9)._left7) == (_a871)) {
                        ((_a871)._parent9)._left7 = _b872;
                    } else {
                        ((_a871)._parent9)._right8 = _b872;
                    }
                }
                if (!((_b872) == null)) {
                    (_b872)._parent9 = (_a871)._parent9;
                }
                /* replace _c873 with _a871 in _b872 */
                (_b872)._right8 = _a871;
                if (!((_a871) == null)) {
                    (_a871)._parent9 = _b872;
                }
                /* replace _b872 with _c873 in _a871 */
                (_a871)._left7 = _c873;
                if (!((_c873) == null)) {
                    (_c873)._parent9 = _a871;
                }
                /* _min_ax12 is min of ax1 */
                var _augval874 = (_a871).ax1;
                var _child875 = (_a871)._left7;
                if (!((_child875) == null)) {
                    var _val876 = (_child875)._min_ax12;
                    _augval874 = ((_augval874) < (_val876)) ? (_augval874) : (_val876);
                }
                var _child877 = (_a871)._right8;
                if (!((_child877) == null)) {
                    var _val878 = (_child877)._min_ax12;
                    _augval874 = ((_augval874) < (_val878)) ? (_augval874) : (_val878);
                }
                (_a871)._min_ax12 = _augval874;
                /* _min_ay13 is min of ay1 */
                var _augval879 = (_a871).ay1;
                var _child880 = (_a871)._left7;
                if (!((_child880) == null)) {
                    var _val881 = (_child880)._min_ay13;
                    _augval879 = ((_augval879) < (_val881)) ? (_augval879) : (_val881);
                }
                var _child882 = (_a871)._right8;
                if (!((_child882) == null)) {
                    var _val883 = (_child882)._min_ay13;
                    _augval879 = ((_augval879) < (_val883)) ? (_augval879) : (_val883);
                }
                (_a871)._min_ay13 = _augval879;
                /* _max_ay24 is max of ay2 */
                var _augval884 = (_a871).ay2;
                var _child885 = (_a871)._left7;
                if (!((_child885) == null)) {
                    var _val886 = (_child885)._max_ay24;
                    _augval884 = ((_augval884) < (_val886)) ? (_val886) : (_augval884);
                }
                var _child887 = (_a871)._right8;
                if (!((_child887) == null)) {
                    var _val888 = (_child887)._max_ay24;
                    _augval884 = ((_augval884) < (_val888)) ? (_val888) : (_augval884);
                }
                (_a871)._max_ay24 = _augval884;
                (_a871)._height10 = 1 + ((((((_a871)._left7) == null) ? (-1) : (((_a871)._left7)._height10)) > ((((_a871)._right8) == null) ? (-1) : (((_a871)._right8)._height10))) ? ((((_a871)._left7) == null) ? (-1) : (((_a871)._left7)._height10)) : ((((_a871)._right8) == null) ? (-1) : (((_a871)._right8)._height10)));
                /* _min_ax12 is min of ax1 */
                var _augval889 = (_b872).ax1;
                var _child890 = (_b872)._left7;
                if (!((_child890) == null)) {
                    var _val891 = (_child890)._min_ax12;
                    _augval889 = ((_augval889) < (_val891)) ? (_augval889) : (_val891);
                }
                var _child892 = (_b872)._right8;
                if (!((_child892) == null)) {
                    var _val893 = (_child892)._min_ax12;
                    _augval889 = ((_augval889) < (_val893)) ? (_augval889) : (_val893);
                }
                (_b872)._min_ax12 = _augval889;
                /* _min_ay13 is min of ay1 */
                var _augval894 = (_b872).ay1;
                var _child895 = (_b872)._left7;
                if (!((_child895) == null)) {
                    var _val896 = (_child895)._min_ay13;
                    _augval894 = ((_augval894) < (_val896)) ? (_augval894) : (_val896);
                }
                var _child897 = (_b872)._right8;
                if (!((_child897) == null)) {
                    var _val898 = (_child897)._min_ay13;
                    _augval894 = ((_augval894) < (_val898)) ? (_augval894) : (_val898);
                }
                (_b872)._min_ay13 = _augval894;
                /* _max_ay24 is max of ay2 */
                var _augval899 = (_b872).ay2;
                var _child900 = (_b872)._left7;
                if (!((_child900) == null)) {
                    var _val901 = (_child900)._max_ay24;
                    _augval899 = ((_augval899) < (_val901)) ? (_val901) : (_augval899);
                }
                var _child902 = (_b872)._right8;
                if (!((_child902) == null)) {
                    var _val903 = (_child902)._max_ay24;
                    _augval899 = ((_augval899) < (_val903)) ? (_val903) : (_augval899);
                }
                (_b872)._max_ay24 = _augval899;
                (_b872)._height10 = 1 + ((((((_b872)._left7) == null) ? (-1) : (((_b872)._left7)._height10)) > ((((_b872)._right8) == null) ? (-1) : (((_b872)._right8)._height10))) ? ((((_b872)._left7) == null) ? (-1) : (((_b872)._left7)._height10)) : ((((_b872)._right8) == null) ? (-1) : (((_b872)._right8)._height10)));
                if (!(((_b872)._parent9) == null)) {
                    /* _min_ax12 is min of ax1 */
                    var _augval904 = ((_b872)._parent9).ax1;
                    var _child905 = ((_b872)._parent9)._left7;
                    if (!((_child905) == null)) {
                        var _val906 = (_child905)._min_ax12;
                        _augval904 = ((_augval904) < (_val906)) ? (_augval904) : (_val906);
                    }
                    var _child907 = ((_b872)._parent9)._right8;
                    if (!((_child907) == null)) {
                        var _val908 = (_child907)._min_ax12;
                        _augval904 = ((_augval904) < (_val908)) ? (_augval904) : (_val908);
                    }
                    ((_b872)._parent9)._min_ax12 = _augval904;
                    /* _min_ay13 is min of ay1 */
                    var _augval909 = ((_b872)._parent9).ay1;
                    var _child910 = ((_b872)._parent9)._left7;
                    if (!((_child910) == null)) {
                        var _val911 = (_child910)._min_ay13;
                        _augval909 = ((_augval909) < (_val911)) ? (_augval909) : (_val911);
                    }
                    var _child912 = ((_b872)._parent9)._right8;
                    if (!((_child912) == null)) {
                        var _val913 = (_child912)._min_ay13;
                        _augval909 = ((_augval909) < (_val913)) ? (_augval909) : (_val913);
                    }
                    ((_b872)._parent9)._min_ay13 = _augval909;
                    /* _max_ay24 is max of ay2 */
                    var _augval914 = ((_b872)._parent9).ay2;
                    var _child915 = ((_b872)._parent9)._left7;
                    if (!((_child915) == null)) {
                        var _val916 = (_child915)._max_ay24;
                        _augval914 = ((_augval914) < (_val916)) ? (_val916) : (_augval914);
                    }
                    var _child917 = ((_b872)._parent9)._right8;
                    if (!((_child917) == null)) {
                        var _val918 = (_child917)._max_ay24;
                        _augval914 = ((_augval914) < (_val918)) ? (_val918) : (_augval914);
                    }
                    ((_b872)._parent9)._max_ay24 = _augval914;
                    ((_b872)._parent9)._height10 = 1 + (((((((_b872)._parent9)._left7) == null) ? (-1) : ((((_b872)._parent9)._left7)._height10)) > (((((_b872)._parent9)._right8) == null) ? (-1) : ((((_b872)._parent9)._right8)._height10))) ? (((((_b872)._parent9)._left7) == null) ? (-1) : ((((_b872)._parent9)._left7)._height10)) : (((((_b872)._parent9)._right8) == null) ? (-1) : ((((_b872)._parent9)._right8)._height10)));
                } else {
                    (this)._root1 = _b872;
                }
            }
            /* rotate (_cursor773)._right8 */
            var _a919 = _cursor773;
            var _b920 = (_a919)._right8;
            var _c921 = (_b920)._left7;
            /* replace _a919 with _b920 in (_a919)._parent9 */
            if (!(((_a919)._parent9) == null)) {
                if ((((_a919)._parent9)._left7) == (_a919)) {
                    ((_a919)._parent9)._left7 = _b920;
                } else {
                    ((_a919)._parent9)._right8 = _b920;
                }
            }
            if (!((_b920) == null)) {
                (_b920)._parent9 = (_a919)._parent9;
            }
            /* replace _c921 with _a919 in _b920 */
            (_b920)._left7 = _a919;
            if (!((_a919) == null)) {
                (_a919)._parent9 = _b920;
            }
            /* replace _b920 with _c921 in _a919 */
            (_a919)._right8 = _c921;
            if (!((_c921) == null)) {
                (_c921)._parent9 = _a919;
            }
            /* _min_ax12 is min of ax1 */
            var _augval922 = (_a919).ax1;
            var _child923 = (_a919)._left7;
            if (!((_child923) == null)) {
                var _val924 = (_child923)._min_ax12;
                _augval922 = ((_augval922) < (_val924)) ? (_augval922) : (_val924);
            }
            var _child925 = (_a919)._right8;
            if (!((_child925) == null)) {
                var _val926 = (_child925)._min_ax12;
                _augval922 = ((_augval922) < (_val926)) ? (_augval922) : (_val926);
            }
            (_a919)._min_ax12 = _augval922;
            /* _min_ay13 is min of ay1 */
            var _augval927 = (_a919).ay1;
            var _child928 = (_a919)._left7;
            if (!((_child928) == null)) {
                var _val929 = (_child928)._min_ay13;
                _augval927 = ((_augval927) < (_val929)) ? (_augval927) : (_val929);
            }
            var _child930 = (_a919)._right8;
            if (!((_child930) == null)) {
                var _val931 = (_child930)._min_ay13;
                _augval927 = ((_augval927) < (_val931)) ? (_augval927) : (_val931);
            }
            (_a919)._min_ay13 = _augval927;
            /* _max_ay24 is max of ay2 */
            var _augval932 = (_a919).ay2;
            var _child933 = (_a919)._left7;
            if (!((_child933) == null)) {
                var _val934 = (_child933)._max_ay24;
                _augval932 = ((_augval932) < (_val934)) ? (_val934) : (_augval932);
            }
            var _child935 = (_a919)._right8;
            if (!((_child935) == null)) {
                var _val936 = (_child935)._max_ay24;
                _augval932 = ((_augval932) < (_val936)) ? (_val936) : (_augval932);
            }
            (_a919)._max_ay24 = _augval932;
            (_a919)._height10 = 1 + ((((((_a919)._left7) == null) ? (-1) : (((_a919)._left7)._height10)) > ((((_a919)._right8) == null) ? (-1) : (((_a919)._right8)._height10))) ? ((((_a919)._left7) == null) ? (-1) : (((_a919)._left7)._height10)) : ((((_a919)._right8) == null) ? (-1) : (((_a919)._right8)._height10)));
            /* _min_ax12 is min of ax1 */
            var _augval937 = (_b920).ax1;
            var _child938 = (_b920)._left7;
            if (!((_child938) == null)) {
                var _val939 = (_child938)._min_ax12;
                _augval937 = ((_augval937) < (_val939)) ? (_augval937) : (_val939);
            }
            var _child940 = (_b920)._right8;
            if (!((_child940) == null)) {
                var _val941 = (_child940)._min_ax12;
                _augval937 = ((_augval937) < (_val941)) ? (_augval937) : (_val941);
            }
            (_b920)._min_ax12 = _augval937;
            /* _min_ay13 is min of ay1 */
            var _augval942 = (_b920).ay1;
            var _child943 = (_b920)._left7;
            if (!((_child943) == null)) {
                var _val944 = (_child943)._min_ay13;
                _augval942 = ((_augval942) < (_val944)) ? (_augval942) : (_val944);
            }
            var _child945 = (_b920)._right8;
            if (!((_child945) == null)) {
                var _val946 = (_child945)._min_ay13;
                _augval942 = ((_augval942) < (_val946)) ? (_augval942) : (_val946);
            }
            (_b920)._min_ay13 = _augval942;
            /* _max_ay24 is max of ay2 */
            var _augval947 = (_b920).ay2;
            var _child948 = (_b920)._left7;
            if (!((_child948) == null)) {
                var _val949 = (_child948)._max_ay24;
                _augval947 = ((_augval947) < (_val949)) ? (_val949) : (_augval947);
            }
            var _child950 = (_b920)._right8;
            if (!((_child950) == null)) {
                var _val951 = (_child950)._max_ay24;
                _augval947 = ((_augval947) < (_val951)) ? (_val951) : (_augval947);
            }
            (_b920)._max_ay24 = _augval947;
            (_b920)._height10 = 1 + ((((((_b920)._left7) == null) ? (-1) : (((_b920)._left7)._height10)) > ((((_b920)._right8) == null) ? (-1) : (((_b920)._right8)._height10))) ? ((((_b920)._left7) == null) ? (-1) : (((_b920)._left7)._height10)) : ((((_b920)._right8) == null) ? (-1) : (((_b920)._right8)._height10)));
            if (!(((_b920)._parent9) == null)) {
                /* _min_ax12 is min of ax1 */
                var _augval952 = ((_b920)._parent9).ax1;
                var _child953 = ((_b920)._parent9)._left7;
                if (!((_child953) == null)) {
                    var _val954 = (_child953)._min_ax12;
                    _augval952 = ((_augval952) < (_val954)) ? (_augval952) : (_val954);
                }
                var _child955 = ((_b920)._parent9)._right8;
                if (!((_child955) == null)) {
                    var _val956 = (_child955)._min_ax12;
                    _augval952 = ((_augval952) < (_val956)) ? (_augval952) : (_val956);
                }
                ((_b920)._parent9)._min_ax12 = _augval952;
                /* _min_ay13 is min of ay1 */
                var _augval957 = ((_b920)._parent9).ay1;
                var _child958 = ((_b920)._parent9)._left7;
                if (!((_child958) == null)) {
                    var _val959 = (_child958)._min_ay13;
                    _augval957 = ((_augval957) < (_val959)) ? (_augval957) : (_val959);
                }
                var _child960 = ((_b920)._parent9)._right8;
                if (!((_child960) == null)) {
                    var _val961 = (_child960)._min_ay13;
                    _augval957 = ((_augval957) < (_val961)) ? (_augval957) : (_val961);
                }
                ((_b920)._parent9)._min_ay13 = _augval957;
                /* _max_ay24 is max of ay2 */
                var _augval962 = ((_b920)._parent9).ay2;
                var _child963 = ((_b920)._parent9)._left7;
                if (!((_child963) == null)) {
                    var _val964 = (_child963)._max_ay24;
                    _augval962 = ((_augval962) < (_val964)) ? (_val964) : (_augval962);
                }
                var _child965 = ((_b920)._parent9)._right8;
                if (!((_child965) == null)) {
                    var _val966 = (_child965)._max_ay24;
                    _augval962 = ((_augval962) < (_val966)) ? (_val966) : (_augval962);
                }
                ((_b920)._parent9)._max_ay24 = _augval962;
                ((_b920)._parent9)._height10 = 1 + (((((((_b920)._parent9)._left7) == null) ? (-1) : ((((_b920)._parent9)._left7)._height10)) > (((((_b920)._parent9)._right8) == null) ? (-1) : ((((_b920)._parent9)._right8)._height10))) ? (((((_b920)._parent9)._left7) == null) ? (-1) : ((((_b920)._parent9)._left7)._height10)) : (((((_b920)._parent9)._right8) == null) ? (-1) : ((((_b920)._parent9)._right8)._height10)));
            } else {
                (this)._root1 = _b920;
            }
            _cursor773 = (_cursor773)._parent9;
        }
    }
    (__x).ax1 = ax1;
    (__x).ay1 = ay1;
    (__x).ax2 = ax2;
    (__x).ay2 = ay2;
}
RectangleHolder.prototype.findMatchingRectangles = function (bx1, by1, bx2, by2, __callback) {
    var _root967 = (this)._root1;
    var _x968 = _root967;
    var _descend969 = true;
    var _from_left970 = true;
    while (true) {
        if ((_x968) == null) {
            _x968 = null;
            break;
        }
        if (_descend969) {
            /* too small? */
            if ((false) || (((_x968).ax2) <= (bx1))) {
                if ((!(((_x968)._right8) == null)) && ((((true) && ((((_x968)._right8)._min_ax12) < (bx2))) && ((((_x968)._right8)._min_ay13) < (by2))) && ((((_x968)._right8)._max_ay24) > (by1)))) {
                    if ((_x968) == (_root967)) {
                        _root967 = (_x968)._right8;
                    }
                    _x968 = (_x968)._right8;
                } else if ((_x968) == (_root967)) {
                    _x968 = null;
                    break;
                } else {
                    _descend969 = false;
                    _from_left970 = (!(((_x968)._parent9) == null)) && ((_x968) == (((_x968)._parent9)._left7));
                    _x968 = (_x968)._parent9;
                }
            } else if ((!(((_x968)._left7) == null)) && ((((true) && ((((_x968)._left7)._min_ax12) < (bx2))) && ((((_x968)._left7)._min_ay13) < (by2))) && ((((_x968)._left7)._max_ay24) > (by1)))) {
                _x968 = (_x968)._left7;
                /* too large? */
            } else if (false) {
                if ((_x968) == (_root967)) {
                    _x968 = null;
                    break;
                } else {
                    _descend969 = false;
                    _from_left970 = (!(((_x968)._parent9) == null)) && ((_x968) == (((_x968)._parent9)._left7));
                    _x968 = (_x968)._parent9;
                }
                /* node ok? */
            } else if ((((true) && (((_x968).ax1) < (bx2))) && (((_x968).ay1) < (by2))) && (((_x968).ay2) > (by1))) {
                break;
            } else if ((_x968) == (_root967)) {
                _root967 = (_x968)._right8;
                _x968 = (_x968)._right8;
            } else {
                if ((!(((_x968)._right8) == null)) && ((((true) && ((((_x968)._right8)._min_ax12) < (bx2))) && ((((_x968)._right8)._min_ay13) < (by2))) && ((((_x968)._right8)._max_ay24) > (by1)))) {
                    if ((_x968) == (_root967)) {
                        _root967 = (_x968)._right8;
                    }
                    _x968 = (_x968)._right8;
                } else {
                    _descend969 = false;
                    _from_left970 = (!(((_x968)._parent9) == null)) && ((_x968) == (((_x968)._parent9)._left7));
                    _x968 = (_x968)._parent9;
                }
            }
        } else if (_from_left970) {
            if (false) {
                _x968 = null;
                break;
            } else if ((((true) && (((_x968).ax1) < (bx2))) && (((_x968).ay1) < (by2))) && (((_x968).ay2) > (by1))) {
                break;
            } else if ((!(((_x968)._right8) == null)) && ((((true) && ((((_x968)._right8)._min_ax12) < (bx2))) && ((((_x968)._right8)._min_ay13) < (by2))) && ((((_x968)._right8)._max_ay24) > (by1)))) {
                _descend969 = true;
                if ((_x968) == (_root967)) {
                    _root967 = (_x968)._right8;
                }
                _x968 = (_x968)._right8;
            } else if ((_x968) == (_root967)) {
                _x968 = null;
                break;
            } else {
                _descend969 = false;
                _from_left970 = (!(((_x968)._parent9) == null)) && ((_x968) == (((_x968)._parent9)._left7));
                _x968 = (_x968)._parent9;
            }
        } else {
            if ((_x968) == (_root967)) {
                _x968 = null;
                break;
            } else {
                _descend969 = false;
                _from_left970 = (!(((_x968)._parent9) == null)) && ((_x968) == (((_x968)._parent9)._left7));
                _x968 = (_x968)._parent9;
            }
        }
    }
    var _prev_cursor5 = null;
    var _cursor6 = _x968;
    for (; ;) {
        if (!(!((_cursor6) == null))) break;
        var _name971 = _cursor6;
        /* ADVANCE */
        _prev_cursor5 = _cursor6;
        do {
            var _right_min972 = null;
            if ((!(((_cursor6)._right8) == null)) && ((((true) && ((((_cursor6)._right8)._min_ax12) < (bx2))) && ((((_cursor6)._right8)._min_ay13) < (by2))) && ((((_cursor6)._right8)._max_ay24) > (by1)))) {
                var _root973 = (_cursor6)._right8;
                var _x974 = _root973;
                var _descend975 = true;
                var _from_left976 = true;
                while (true) {
                    if ((_x974) == null) {
                        _x974 = null;
                        break;
                    }
                    if (_descend975) {
                        /* too small? */
                        if ((false) || (((_x974).ax2) <= (bx1))) {
                            if ((!(((_x974)._right8) == null)) && ((((true) && ((((_x974)._right8)._min_ax12) < (bx2))) && ((((_x974)._right8)._min_ay13) < (by2))) && ((((_x974)._right8)._max_ay24) > (by1)))) {
                                if ((_x974) == (_root973)) {
                                    _root973 = (_x974)._right8;
                                }
                                _x974 = (_x974)._right8;
                            } else if ((_x974) == (_root973)) {
                                _x974 = null;
                                break;
                            } else {
                                _descend975 = false;
                                _from_left976 = (!(((_x974)._parent9) == null)) && ((_x974) == (((_x974)._parent9)._left7));
                                _x974 = (_x974)._parent9;
                            }
                        } else if ((!(((_x974)._left7) == null)) && ((((true) && ((((_x974)._left7)._min_ax12) < (bx2))) && ((((_x974)._left7)._min_ay13) < (by2))) && ((((_x974)._left7)._max_ay24) > (by1)))) {
                            _x974 = (_x974)._left7;
                            /* too large? */
                        } else if (false) {
                            if ((_x974) == (_root973)) {
                                _x974 = null;
                                break;
                            } else {
                                _descend975 = false;
                                _from_left976 = (!(((_x974)._parent9) == null)) && ((_x974) == (((_x974)._parent9)._left7));
                                _x974 = (_x974)._parent9;
                            }
                            /* node ok? */
                        } else if ((((true) && (((_x974).ax1) < (bx2))) && (((_x974).ay1) < (by2))) && (((_x974).ay2) > (by1))) {
                            break;
                        } else if ((_x974) == (_root973)) {
                            _root973 = (_x974)._right8;
                            _x974 = (_x974)._right8;
                        } else {
                            if ((!(((_x974)._right8) == null)) && ((((true) && ((((_x974)._right8)._min_ax12) < (bx2))) && ((((_x974)._right8)._min_ay13) < (by2))) && ((((_x974)._right8)._max_ay24) > (by1)))) {
                                if ((_x974) == (_root973)) {
                                    _root973 = (_x974)._right8;
                                }
                                _x974 = (_x974)._right8;
                            } else {
                                _descend975 = false;
                                _from_left976 = (!(((_x974)._parent9) == null)) && ((_x974) == (((_x974)._parent9)._left7));
                                _x974 = (_x974)._parent9;
                            }
                        }
                    } else if (_from_left976) {
                        if (false) {
                            _x974 = null;
                            break;
                        } else if ((((true) && (((_x974).ax1) < (bx2))) && (((_x974).ay1) < (by2))) && (((_x974).ay2) > (by1))) {
                            break;
                        } else if ((!(((_x974)._right8) == null)) && ((((true) && ((((_x974)._right8)._min_ax12) < (bx2))) && ((((_x974)._right8)._min_ay13) < (by2))) && ((((_x974)._right8)._max_ay24) > (by1)))) {
                            _descend975 = true;
                            if ((_x974) == (_root973)) {
                                _root973 = (_x974)._right8;
                            }
                            _x974 = (_x974)._right8;
                        } else if ((_x974) == (_root973)) {
                            _x974 = null;
                            break;
                        } else {
                            _descend975 = false;
                            _from_left976 = (!(((_x974)._parent9) == null)) && ((_x974) == (((_x974)._parent9)._left7));
                            _x974 = (_x974)._parent9;
                        }
                    } else {
                        if ((_x974) == (_root973)) {
                            _x974 = null;
                            break;
                        } else {
                            _descend975 = false;
                            _from_left976 = (!(((_x974)._parent9) == null)) && ((_x974) == (((_x974)._parent9)._left7));
                            _x974 = (_x974)._parent9;
                        }
                    }
                }
                _right_min972 = _x974;
            }
            if (!((_right_min972) == null)) {
                _cursor6 = _right_min972;
                break;
            } else {
                while ((!(((_cursor6)._parent9) == null)) && ((_cursor6) == (((_cursor6)._parent9)._right8))) {
                    _cursor6 = (_cursor6)._parent9;
                }
                _cursor6 = (_cursor6)._parent9;
                if ((!((_cursor6) == null)) && (false)) {
                    _cursor6 = null;
                }
            }
        } while ((!((_cursor6) == null)) && (!((((true) && (((_cursor6).ax1) < (bx2))) && (((_cursor6).ay1) < (by2))) && (((_cursor6).ay2) > (by1)))));
        if (__callback(_name971)) {
            var _to_remove977 = _prev_cursor5;
            var _parent978 = (_to_remove977)._parent9;
            var _left979 = (_to_remove977)._left7;
            var _right980 = (_to_remove977)._right8;
            var _new_x981;
            if (((_left979) == null) && ((_right980) == null)) {
                _new_x981 = null;
                /* replace _to_remove977 with _new_x981 in _parent978 */
                if (!((_parent978) == null)) {
                    if (((_parent978)._left7) == (_to_remove977)) {
                        (_parent978)._left7 = _new_x981;
                    } else {
                        (_parent978)._right8 = _new_x981;
                    }
                }
                if (!((_new_x981) == null)) {
                    (_new_x981)._parent9 = _parent978;
                }
            } else if ((!((_left979) == null)) && ((_right980) == null)) {
                _new_x981 = _left979;
                /* replace _to_remove977 with _new_x981 in _parent978 */
                if (!((_parent978) == null)) {
                    if (((_parent978)._left7) == (_to_remove977)) {
                        (_parent978)._left7 = _new_x981;
                    } else {
                        (_parent978)._right8 = _new_x981;
                    }
                }
                if (!((_new_x981) == null)) {
                    (_new_x981)._parent9 = _parent978;
                }
            } else if (((_left979) == null) && (!((_right980) == null))) {
                _new_x981 = _right980;
                /* replace _to_remove977 with _new_x981 in _parent978 */
                if (!((_parent978) == null)) {
                    if (((_parent978)._left7) == (_to_remove977)) {
                        (_parent978)._left7 = _new_x981;
                    } else {
                        (_parent978)._right8 = _new_x981;
                    }
                }
                if (!((_new_x981) == null)) {
                    (_new_x981)._parent9 = _parent978;
                }
            } else {
                var _root982 = (_to_remove977)._right8;
                var _x983 = _root982;
                var _descend984 = true;
                var _from_left985 = true;
                while (true) {
                    if ((_x983) == null) {
                        _x983 = null;
                        break;
                    }
                    if (_descend984) {
                        /* too small? */
                        if (false) {
                            if ((!(((_x983)._right8) == null)) && (true)) {
                                if ((_x983) == (_root982)) {
                                    _root982 = (_x983)._right8;
                                }
                                _x983 = (_x983)._right8;
                            } else if ((_x983) == (_root982)) {
                                _x983 = null;
                                break;
                            } else {
                                _descend984 = false;
                                _from_left985 = (!(((_x983)._parent9) == null)) && ((_x983) == (((_x983)._parent9)._left7));
                                _x983 = (_x983)._parent9;
                            }
                        } else if ((!(((_x983)._left7) == null)) && (true)) {
                            _x983 = (_x983)._left7;
                            /* too large? */
                        } else if (false) {
                            if ((_x983) == (_root982)) {
                                _x983 = null;
                                break;
                            } else {
                                _descend984 = false;
                                _from_left985 = (!(((_x983)._parent9) == null)) && ((_x983) == (((_x983)._parent9)._left7));
                                _x983 = (_x983)._parent9;
                            }
                            /* node ok? */
                        } else if (true) {
                            break;
                        } else if ((_x983) == (_root982)) {
                            _root982 = (_x983)._right8;
                            _x983 = (_x983)._right8;
                        } else {
                            if ((!(((_x983)._right8) == null)) && (true)) {
                                if ((_x983) == (_root982)) {
                                    _root982 = (_x983)._right8;
                                }
                                _x983 = (_x983)._right8;
                            } else {
                                _descend984 = false;
                                _from_left985 = (!(((_x983)._parent9) == null)) && ((_x983) == (((_x983)._parent9)._left7));
                                _x983 = (_x983)._parent9;
                            }
                        }
                    } else if (_from_left985) {
                        if (false) {
                            _x983 = null;
                            break;
                        } else if (true) {
                            break;
                        } else if ((!(((_x983)._right8) == null)) && (true)) {
                            _descend984 = true;
                            if ((_x983) == (_root982)) {
                                _root982 = (_x983)._right8;
                            }
                            _x983 = (_x983)._right8;
                        } else if ((_x983) == (_root982)) {
                            _x983 = null;
                            break;
                        } else {
                            _descend984 = false;
                            _from_left985 = (!(((_x983)._parent9) == null)) && ((_x983) == (((_x983)._parent9)._left7));
                            _x983 = (_x983)._parent9;
                        }
                    } else {
                        if ((_x983) == (_root982)) {
                            _x983 = null;
                            break;
                        } else {
                            _descend984 = false;
                            _from_left985 = (!(((_x983)._parent9) == null)) && ((_x983) == (((_x983)._parent9)._left7));
                            _x983 = (_x983)._parent9;
                        }
                    }
                }
                _new_x981 = _x983;
                var _mp986 = (_x983)._parent9;
                var _mr987 = (_x983)._right8;
                /* replace _x983 with _mr987 in _mp986 */
                if (!((_mp986) == null)) {
                    if (((_mp986)._left7) == (_x983)) {
                        (_mp986)._left7 = _mr987;
                    } else {
                        (_mp986)._right8 = _mr987;
                    }
                }
                if (!((_mr987) == null)) {
                    (_mr987)._parent9 = _mp986;
                }
                /* replace _to_remove977 with _x983 in _parent978 */
                if (!((_parent978) == null)) {
                    if (((_parent978)._left7) == (_to_remove977)) {
                        (_parent978)._left7 = _x983;
                    } else {
                        (_parent978)._right8 = _x983;
                    }
                }
                if (!((_x983) == null)) {
                    (_x983)._parent9 = _parent978;
                }
                /* replace null with _left979 in _x983 */
                (_x983)._left7 = _left979;
                if (!((_left979) == null)) {
                    (_left979)._parent9 = _x983;
                }
                /* replace _mr987 with (_to_remove977)._right8 in _x983 */
                (_x983)._right8 = (_to_remove977)._right8;
                if (!(((_to_remove977)._right8) == null)) {
                    ((_to_remove977)._right8)._parent9 = _x983;
                }
                /* _min_ax12 is min of ax1 */
                var _augval988 = (_x983).ax1;
                var _child989 = (_x983)._left7;
                if (!((_child989) == null)) {
                    var _val990 = (_child989)._min_ax12;
                    _augval988 = ((_augval988) < (_val990)) ? (_augval988) : (_val990);
                }
                var _child991 = (_x983)._right8;
                if (!((_child991) == null)) {
                    var _val992 = (_child991)._min_ax12;
                    _augval988 = ((_augval988) < (_val992)) ? (_augval988) : (_val992);
                }
                (_x983)._min_ax12 = _augval988;
                /* _min_ay13 is min of ay1 */
                var _augval993 = (_x983).ay1;
                var _child994 = (_x983)._left7;
                if (!((_child994) == null)) {
                    var _val995 = (_child994)._min_ay13;
                    _augval993 = ((_augval993) < (_val995)) ? (_augval993) : (_val995);
                }
                var _child996 = (_x983)._right8;
                if (!((_child996) == null)) {
                    var _val997 = (_child996)._min_ay13;
                    _augval993 = ((_augval993) < (_val997)) ? (_augval993) : (_val997);
                }
                (_x983)._min_ay13 = _augval993;
                /* _max_ay24 is max of ay2 */
                var _augval998 = (_x983).ay2;
                var _child999 = (_x983)._left7;
                if (!((_child999) == null)) {
                    var _val1000 = (_child999)._max_ay24;
                    _augval998 = ((_augval998) < (_val1000)) ? (_val1000) : (_augval998);
                }
                var _child1001 = (_x983)._right8;
                if (!((_child1001) == null)) {
                    var _val1002 = (_child1001)._max_ay24;
                    _augval998 = ((_augval998) < (_val1002)) ? (_val1002) : (_augval998);
                }
                (_x983)._max_ay24 = _augval998;
                (_x983)._height10 = 1 + ((((((_x983)._left7) == null) ? (-1) : (((_x983)._left7)._height10)) > ((((_x983)._right8) == null) ? (-1) : (((_x983)._right8)._height10))) ? ((((_x983)._left7) == null) ? (-1) : (((_x983)._left7)._height10)) : ((((_x983)._right8) == null) ? (-1) : (((_x983)._right8)._height10)));
                var _cursor1003 = _mp986;
                var _changed1004 = true;
                while ((_changed1004) && (!((_cursor1003) == (_parent978)))) {
                    var _old__min_ax121005 = (_cursor1003)._min_ax12;
                    var _old__min_ay131006 = (_cursor1003)._min_ay13;
                    var _old__max_ay241007 = (_cursor1003)._max_ay24;
                    var _old_height1008 = (_cursor1003)._height10;
                    /* _min_ax12 is min of ax1 */
                    var _augval1009 = (_cursor1003).ax1;
                    var _child1010 = (_cursor1003)._left7;
                    if (!((_child1010) == null)) {
                        var _val1011 = (_child1010)._min_ax12;
                        _augval1009 = ((_augval1009) < (_val1011)) ? (_augval1009) : (_val1011);
                    }
                    var _child1012 = (_cursor1003)._right8;
                    if (!((_child1012) == null)) {
                        var _val1013 = (_child1012)._min_ax12;
                        _augval1009 = ((_augval1009) < (_val1013)) ? (_augval1009) : (_val1013);
                    }
                    (_cursor1003)._min_ax12 = _augval1009;
                    /* _min_ay13 is min of ay1 */
                    var _augval1014 = (_cursor1003).ay1;
                    var _child1015 = (_cursor1003)._left7;
                    if (!((_child1015) == null)) {
                        var _val1016 = (_child1015)._min_ay13;
                        _augval1014 = ((_augval1014) < (_val1016)) ? (_augval1014) : (_val1016);
                    }
                    var _child1017 = (_cursor1003)._right8;
                    if (!((_child1017) == null)) {
                        var _val1018 = (_child1017)._min_ay13;
                        _augval1014 = ((_augval1014) < (_val1018)) ? (_augval1014) : (_val1018);
                    }
                    (_cursor1003)._min_ay13 = _augval1014;
                    /* _max_ay24 is max of ay2 */
                    var _augval1019 = (_cursor1003).ay2;
                    var _child1020 = (_cursor1003)._left7;
                    if (!((_child1020) == null)) {
                        var _val1021 = (_child1020)._max_ay24;
                        _augval1019 = ((_augval1019) < (_val1021)) ? (_val1021) : (_augval1019);
                    }
                    var _child1022 = (_cursor1003)._right8;
                    if (!((_child1022) == null)) {
                        var _val1023 = (_child1022)._max_ay24;
                        _augval1019 = ((_augval1019) < (_val1023)) ? (_val1023) : (_augval1019);
                    }
                    (_cursor1003)._max_ay24 = _augval1019;
                    (_cursor1003)._height10 = 1 + ((((((_cursor1003)._left7) == null) ? (-1) : (((_cursor1003)._left7)._height10)) > ((((_cursor1003)._right8) == null) ? (-1) : (((_cursor1003)._right8)._height10))) ? ((((_cursor1003)._left7) == null) ? (-1) : (((_cursor1003)._left7)._height10)) : ((((_cursor1003)._right8) == null) ? (-1) : (((_cursor1003)._right8)._height10)));
                    _changed1004 = false;
                    _changed1004 = (_changed1004) || (!((_old__min_ax121005) == ((_cursor1003)._min_ax12)));
                    _changed1004 = (_changed1004) || (!((_old__min_ay131006) == ((_cursor1003)._min_ay13)));
                    _changed1004 = (_changed1004) || (!((_old__max_ay241007) == ((_cursor1003)._max_ay24)));
                    _changed1004 = (_changed1004) || (!((_old_height1008) == ((_cursor1003)._height10)));
                    _cursor1003 = (_cursor1003)._parent9;
                }
            }
            var _cursor1024 = _parent978;
            var _changed1025 = true;
            while ((_changed1025) && (!((_cursor1024) == (null)))) {
                var _old__min_ax121026 = (_cursor1024)._min_ax12;
                var _old__min_ay131027 = (_cursor1024)._min_ay13;
                var _old__max_ay241028 = (_cursor1024)._max_ay24;
                var _old_height1029 = (_cursor1024)._height10;
                /* _min_ax12 is min of ax1 */
                var _augval1030 = (_cursor1024).ax1;
                var _child1031 = (_cursor1024)._left7;
                if (!((_child1031) == null)) {
                    var _val1032 = (_child1031)._min_ax12;
                    _augval1030 = ((_augval1030) < (_val1032)) ? (_augval1030) : (_val1032);
                }
                var _child1033 = (_cursor1024)._right8;
                if (!((_child1033) == null)) {
                    var _val1034 = (_child1033)._min_ax12;
                    _augval1030 = ((_augval1030) < (_val1034)) ? (_augval1030) : (_val1034);
                }
                (_cursor1024)._min_ax12 = _augval1030;
                /* _min_ay13 is min of ay1 */
                var _augval1035 = (_cursor1024).ay1;
                var _child1036 = (_cursor1024)._left7;
                if (!((_child1036) == null)) {
                    var _val1037 = (_child1036)._min_ay13;
                    _augval1035 = ((_augval1035) < (_val1037)) ? (_augval1035) : (_val1037);
                }
                var _child1038 = (_cursor1024)._right8;
                if (!((_child1038) == null)) {
                    var _val1039 = (_child1038)._min_ay13;
                    _augval1035 = ((_augval1035) < (_val1039)) ? (_augval1035) : (_val1039);
                }
                (_cursor1024)._min_ay13 = _augval1035;
                /* _max_ay24 is max of ay2 */
                var _augval1040 = (_cursor1024).ay2;
                var _child1041 = (_cursor1024)._left7;
                if (!((_child1041) == null)) {
                    var _val1042 = (_child1041)._max_ay24;
                    _augval1040 = ((_augval1040) < (_val1042)) ? (_val1042) : (_augval1040);
                }
                var _child1043 = (_cursor1024)._right8;
                if (!((_child1043) == null)) {
                    var _val1044 = (_child1043)._max_ay24;
                    _augval1040 = ((_augval1040) < (_val1044)) ? (_val1044) : (_augval1040);
                }
                (_cursor1024)._max_ay24 = _augval1040;
                (_cursor1024)._height10 = 1 + ((((((_cursor1024)._left7) == null) ? (-1) : (((_cursor1024)._left7)._height10)) > ((((_cursor1024)._right8) == null) ? (-1) : (((_cursor1024)._right8)._height10))) ? ((((_cursor1024)._left7) == null) ? (-1) : (((_cursor1024)._left7)._height10)) : ((((_cursor1024)._right8) == null) ? (-1) : (((_cursor1024)._right8)._height10)));
                _changed1025 = false;
                _changed1025 = (_changed1025) || (!((_old__min_ax121026) == ((_cursor1024)._min_ax12)));
                _changed1025 = (_changed1025) || (!((_old__min_ay131027) == ((_cursor1024)._min_ay13)));
                _changed1025 = (_changed1025) || (!((_old__max_ay241028) == ((_cursor1024)._max_ay24)));
                _changed1025 = (_changed1025) || (!((_old_height1029) == ((_cursor1024)._height10)));
                _cursor1024 = (_cursor1024)._parent9;
            }
            if (((this)._root1) == (_to_remove977)) {
                (this)._root1 = _new_x981;
            }
            _prev_cursor5 = null;
        }
    };
}
; 
 
 buildViz = function (d3) {
    return function (widthInPixels = 800,
                     heightInPixels = 600,
                     max_snippets = null,
                     color = null,
                     sortByDist = true,
                     useFullDoc = false,
                     greyZeroScores = false,
                     asianMode = false,
                     nonTextFeaturesMode = false,
                     showCharacteristic = true,
                     wordVecMaxPValue = false,
                     saveSvgButton = false,
                     reverseSortScoresForNotCategory = false,
                     minPVal = 0.05,
                     pValueColors = false,
                     xLabelText = null,
                     yLabelText = null) {
        var divName = 'd3-div-1';

        // Set the dimensions of the canvas / graph
        var margin = {top: 30, right: 20, bottom: 30, left: 50},
            width = widthInPixels - margin.left - margin.right,
            height = heightInPixels - margin.top - margin.bottom;

        // Set the ranges
        var x = d3.scaleLinear().range([0, width]);
        var y = d3.scaleLinear().range([height, 0]);

        console.log('X Label');
        console.log(xLabelText);
        console.log('Y Label');
        console.log(yLabelText);
        console.log(yLabelText == null);
        console.log(yLabelText != null);
        console.log(yLabelText === undefined);
        console.log(yLabelText !== undefined);

        function axisLabelerFactory(axis) {
            if ((axis == "x" && xLabelText == null)
                || (axis == "y" && yLabelText == null))
                return function (d, i) {
                    return ["Infrequent", "Average", "Frequent"][i];
                };

            return function (d, i) {
                return ["Low", "Medium", "High"][i];
            }
        }

        var xAxis = d3.axisBottom(x).ticks(3).tickFormat(axisLabelerFactory('x'));
        var yAxis = d3.axisLeft(y).ticks(3).tickFormat(axisLabelerFactory('y'));

        // var label = d3.select("body").append("div")
        var label = d3.select('#' + divName).append("div")
            .attr("class", "label");

        var interpolateLightGreys = d3.interpolate(d3.rgb(230, 230, 230), d3.rgb(130, 130, 130));
        // setup fill color
        //var color = d3.interpolateRdYlBu;
        if (color == null) {
            color = d3.interpolateRdYlBu;
        }
        ;

        // Adds the svg canvas
        // var svg = d3.select("body")
        svg = d3.select('#' + divName)
            .append("svg")
            .attr("width", width + margin.left + margin.right + 200)
            .attr("height", height + margin.top + margin.bottom)
            .append("g")
            .attr("transform",
                "translate(" + margin.left + "," + margin.top + ")");

        var lastCircleSelected = null;

        function deselectLastCircle() {
            if (lastCircleSelected) {
                lastCircleSelected.style["stroke"] = null;
                lastCircleSelected = null;
            }
        }

        function getSentenceBoundaries(text) {
            // !!! need to use spacy's sentence splitter
            if (asianMode) {
                var sentenceRe = /\n/gmi;
            } else {
                var sentenceRe = /\(?[^\.\?\!\n\b]+[\n\.!\?]\)?/g;
            }
            var offsets = [];
            var match;
            while ((match = sentenceRe.exec(text)) != null) {
                offsets.push(match.index);
            }
            offsets.push(text.length);
            return offsets;
        }

        function getMatchingSnippet(text, boundaries, start, end) {
            var sentenceStart = null;
            var sentenceEnd = null;
            for (var i in boundaries) {
                var position = boundaries[i];
                if (position <= start && (sentenceStart == null || position > sentenceStart)) {
                    sentenceStart = position;
                }
                if (position >= end) {
                    sentenceEnd = position;
                    break;
                }
            }
            var snippet = (text.slice(sentenceStart, start) + "<b>" + text.slice(start, end)
            + "</b>" + text.slice(end, sentenceEnd)).trim();
            if (sentenceStart == null) {
                sentenceStart = 0;
            }
            return {'snippet': snippet, 'sentenceStart': sentenceStart};
        }

        function gatherTermContexts(d) {
            var category_name = fullData['info']['category_name'];
            var not_category_name = fullData['info']['not_category_name'];
            var matches = [[], []];
            if (fullData.docs === undefined) return matches;
            if (!nonTextFeaturesMode) {
                return searchInText(d);
            } else {
                return searchInExtraFeatures(d);
            }
        }

        function searchInExtraFeatures(d) {
            var matches = [[], []];
            var term = d.term;
            for (var i in fullData.docs.extra) {

                if (term in fullData.docs.extra[i]) {
                    var strength = fullData.docs.extra[i][term] /
                        Object.values(fullData.docs.extra[i]).reduce(
                            function (a, b) {
                                return a + b
                            });
                    var text = fullData.docs.texts[i];
                    if (!useFullDoc)
                        text = text.slice(0, 300);
                    var curMatch = {'id': i, 'snippets': [text], 'strength': strength};

                    curMatch['meta'] = fullData.docs.meta[i];
                    matches[fullData.docs.labels[i]].push(curMatch);
                }
            }
            for (var i in [0, 1]) {
                matches[i] = matches[i].sort(function (a, b) {
                    return a.strength < b.strength ? 1 : -1
                })
            }
            return {'contexts': matches, 'info': d};
        }

        // from https://medium.com/reactnative/emojis-in-javascript-f693d0eb79fb
        var emojiRE = /(?:[\u2700-\u27bf]|(?:\ud83c[\udde6-\uddff]){2}|[\ud800-\udbff][\udc00-\udfff])[\ufe0e\ufe0f]?(?:[\u0300-\u036f\ufe20-\ufe23\u20d0-\u20f0]|\ud83c[\udffb-\udfff])?(?:\u200d(?:[^\ud800-\udfff]|(?:\ud83c[\udde6-\uddff]){2}|[\ud800-\udbff][\udc00-\udfff])[\ufe0e\ufe0f]?(?:[\u0300-\u036f\ufe20-\ufe23\u20d0-\u20f0]|\ud83c[\udffb-\udfff])?)*/g;

        function isEmoji(str) {
            console.log(str);
            if (str.match(emojiRE)) return true;
            return false;
        }

        function searchInText(d) {
            function stripNonWordChars(term) {
                //d.term.replace(" ", "[^\\w]+")
            }

            function buildMatcher(term) {

                var boundary = '\\b';
                var wordSep = "[^\\w]+";
                if (asianMode) {
                    boundary = '( |$|^)';
                    wordSep = ' ';
                }
                if (isEmoji(term)) {
                    boundary = '';
                    wordSep = '';
                }
                var regexp = new RegExp(boundary + '('
                    + term.replace('$', '\\$').replace(' ', wordSep, 'gim')
                    + ')' + boundary, 'gim');
                try {
                    regexp.exec('X');
                } catch (err) {
                    console.log("Can't search " + term);
                    console.log(err);
                    return null;
                }
                return regexp;
            }

            var matches = [[], []];
            var pattern = buildMatcher(d.term);
            if (pattern !== null) {
                for (var i in fullData.docs.texts) {
                    if (fullData.docs.labels[i] > 1) continue;
                    var text = fullData.docs.texts[i];
                    //var pattern = new RegExp("\\b(" + stripNonWordChars(d.term) + ")\\b", "gim");
                    var match;
                    var sentenceOffsets = null;
                    var lastSentenceStart = null;
                    var matchFound = false;
                    var curMatch = {'id': i, 'snippets': []};
                    if (fullData.docs.meta) {
                        curMatch['meta'] = fullData.docs.meta[i];
                    }
                    while ((match = pattern.exec(text)) != null) {
                        if (sentenceOffsets == null) {
                            sentenceOffsets = getSentenceBoundaries(text);
                        }
                        var foundSnippet = getMatchingSnippet(text, sentenceOffsets,
                            match.index, pattern.lastIndex);
                        if (foundSnippet.sentenceStart == lastSentenceStart) continue; // ensure we don't duplicate sentences
                        lastSentenceStart = foundSnippet.sentenceStart;
                        curMatch.snippets.push(foundSnippet.snippet);
                        matchFound = true;
                    }
                    if (matchFound) {
                        if (useFullDoc) {
                            curMatch.snippets = [
                                text
                                    .replace(/\n$/g, '\n\n')
                                    .replace(
                                        //new RegExp("\\b(" + d.term.replace(" ", "[^\\w]+") + ")\\b",
                                        //    'gim'),
                                        pattern,
                                        '<b>$&</b>')
                            ];
                        }
                        matches[fullData.docs.labels[i]].push(curMatch);

                    }
                }
            }
            return {'contexts': matches, 'info': d};
        }

        function displayTermContexts(termInfo, jump=true) {
            var contexts = termInfo.contexts;
            var info = termInfo.info;
            if (contexts[0].length == 0 && contexts[1].length == 0) {
                return null;
            }
            //var categoryNames = [fullData.info.category_name,
            //    fullData.info.not_category_name];
            var catInternalName = fullData.info.category_internal_name;
            fullData.docs.categories
                .map(
                    function (catName, catIndex) {
                        if (max_snippets != null) {
                            var contextsToDisplay = contexts[catIndex].slice(0, max_snippets);
                        }
                        var divId = catName == catInternalName ? '#cat' : '#notcat';
                        var temp = d3.select(divId)
                            .selectAll("div").remove();
                        contexts[catIndex].forEach(function (context) {
                            var meta = context.meta ? context.meta : '&nbsp;';
                            d3.select(divId)
                                .append("div")
                                .attr('class', 'snippet_meta')
                                .html(meta);
                            context.snippets.forEach(function (snippet) {
                                d3.select(divId)
                                    .append("div")
                                    .attr('class', 'snippet')
                                    .html(snippet);
                            })

                        });
                    });
            d3.select('#termstats')
                .selectAll("div")
                .remove();
            d3.select('#termstats')
                .append('div')
                .attr("class", "snippet_header")
                .html('Term: <b>' + info.term + '</b>');
            var message = '';
            var cat_name = fullData.info.category_name;
            var ncat_name = fullData.info.not_category_name;

            function getFrequencyDescription(name, count25k, count) {
                var desc = name + ' frequency: <div class=text_subhead>' + count25k
                    + ' per 25,000 terms</div>';
                if (count == 0) {
                    desc += '<u>Not found in any ' + name + ' documents.</u>';
                } else {
                    desc += '<u>Some of the ' + count + ' mentions:</u>';
                }
                return desc;
            }

            d3.select('#cathead')
                .style('fill', color(1))
                .html(getFrequencyDescription(cat_name, info.cat25k, info.cat));
            d3.select('#notcathead')
                .style('fill', color(0))
                .html(getFrequencyDescription(ncat_name, info.ncat25k, info.ncat));
            console.log(info);
            if (jump) {
                if (window.location.hash == '#snippets') {
                    window.location.hash = '#snippetsalt';
                } else {
                    window.location.hash = '#snippets';
                }
            }
        }

        function showTooltip(d, pageX, pageY) {
            deselectLastCircle();
            var message = d.term + "<br/>" + d.cat25k + ":" + d.ncat25k + " per 25k words";
            if (!sortByDist) {
                message += '<br/>score: ' + d.os.toFixed(5);
            }
            /*
             if (d.p) {
             message += ';  (p:' + d.p.toFixed(5) +')';
             }*/

            tooltip.transition()
                .duration(0)
                .style("opacity", 1)
                .style("z-index", 10000000);
            tooltip.html(message)
                .style("left", (pageX) + "px")
                .style("top", (pageY - 28) + "px");

            tooltip.on('click', function () {
                tooltip.transition()
                    .style('opacity', 0)
            });
        }

        handleSearch = function (event) {
            deselectLastCircle();
            var searchTerm = document
                .getElementById("searchTerm")
                .value
                .toLowerCase()
                .replace("'", " '")
                .trim();
            showToolTipForTerm(searchTerm);
            var termInfo = termDict[searchTerm];
            if (termInfo != null) {
                displayTermContexts(gatherTermContexts(termInfo), false);
            }
            return false;
        };

        function showToolTipForTerm(searchTerm) {
            var searchTermInfo = termDict[searchTerm];
            if (searchTermInfo === undefined) {
                d3.select("#alertMessage")
                    .text(searchTerm + " didn't make it into the visualization.");
            } else {
                d3.select("#alertMessage").text("");
                var circle = mysvg._groups[0][searchTermInfo.i];
                var mySVGMatrix = circle.getScreenCTM()
                    .translate(circle.cx.baseVal.value, circle.cy.baseVal.value);
                var pageX = mySVGMatrix.e;
                var pageY = mySVGMatrix.f;
                circle.style["stroke"] = "black";
                showTooltip(searchTermInfo, pageX, pageY);
                lastCircleSelected = circle;

            }
        };

        function makeWordInteractive(domObj, term) {
            return domObj
                .on("mouseover", function (d) {
                    showToolTipForTerm(term);
                    d3.select(this).style("stroke", "black");
                })
                .on("mouseout", function (d) {
                    tooltip.transition()
                        .duration(0)
                        .style("opacity", 0);
                    d3.select(this).style("stroke", null);
                })
                .on("click", function (d) {
                    displayTermContexts(gatherTermContexts(termDict[term]));
                });
        }

        function processData(fullData) {
            var modelInfo = fullData['info'];
            /*
             categoryTermList.data(modelInfo['category_terms'])
             .enter()
             .append("li")
             .text(function(d) {return d;});
             */
            data = fullData['data'];
            termDict = Object();
            data.forEach(function (x, i) {
                termDict[x.term] = x;
                termDict[x.term].i = i;
            });

            console.log(data);
            // Scale the range of the data.  Add some space on either end.
            x.domain([-0.1, d3.max(data, function (d) {
                return d.x;
            }) + 0.1]);
            y.domain([-0.1, d3.max(data, function (d) {
                return d.y;
            }) + 0.1]);

            /*
             data.sort(function (a, b) {
             return Math.abs(b.os) - Math.abs(a.os)
             });
             */


            //var rangeTree = null; // keep boxes of all points and labels here
            var rectHolder = new RectangleHolder();
            // Add the scatterplot
            mysvg = svg
                .selectAll("dot")
                .data(data)
                .enter()
                .append("circle")
                .attr("r", function (d) {
                    if (pValueColors && d.p) {
                        return (d.p >= 1 - minPVal || d.p <= minPVal) ? 2 : 1.75;
                    }
                    return 2;
                })
                .attr("cx", function (d) {
                    return x(d.x);
                })
                .attr("cy", function (d) {
                    return y(d.y);
                })
                .style("fill", function (d) {
                    //.attr("fill", function (d) {
                    if (greyZeroScores && d.os == 0) {
                        return d3.rgb(230, 230, 230);
                    } else if (pValueColors && d.p) {
                        if (d.p >= 1 - minPVal) {
                            return d3.interpolateYlGnBu(d.s);
                        } else if (d.p <= minPVal) {
                            return d3.interpolateYlOrBr(d.s);
                        } else {
                            return interpolateLightGreys(d.s);
                        }
                    } else {
                        return color(d.s);
                    }
                })
                .on("mouseover", function (d) {
                    showTooltip(d, d3.event.pageX, d3.event.pageY);
                    d3.select(this).style("stroke", "black");
                })
                .on("click", function (d) {
                    displayTermContexts(gatherTermContexts(d));
                })
                .on("mouseout", function (d) {
                    tooltip.transition()
                        .duration(0)
                        .style("opacity", 0);
                    d3.select(this).style("stroke", null);
                });

            coords = Object();

            function censorPoints(datum) {
                var term = datum.term;
                var curLabel = svg.append("text")
                    .attr("x", x(datum.x))
                    .attr("y", y(datum.y) + 3)
                    .attr("text-anchor", "middle")
                    .text("x");
                var bbox = curLabel.node().getBBox();
                var borderToRemove = .5;
                var x1 = bbox.x + borderToRemove,
                    y1 = bbox.y + borderToRemove,
                    x2 = bbox.x + bbox.width - borderToRemove,
                    y2 = bbox.y + bbox.height - borderToRemove;
                //rangeTree = insertRangeTree(rangeTree, x1, y1, x2, y2, '~~' + term);
                rectHolder.add(new Rectangle(x1, y1, x2, y2));

                curLabel.remove();
            }

            function labelPointsIfPossible(i) {
                var term = data[i].term;

                var configs = [
                    {'anchor': 'end', 'xoff': -5, 'yoff': -3},
                    {'anchor': 'end', 'xoff': -5, 'yoff': 10},
                    {'anchor': 'start', 'xoff': 3, 'yoff': 10},
                    {'anchor': 'start', 'xoff': 3, 'yoff': -3},
                    {'anchor': 'start', 'xoff': 5, 'yoff': 10},
                    {'anchor': 'start', 'xoff': 5, 'yoff': -3},
                    {'anchor': 'start', 'xoff': 10, 'yoff': 15},
                    {'anchor': 'start', 'xoff': -10, 'yoff': -15},
                    {'anchor': 'start', 'xoff': 10, 'yoff': -15},
                    {'anchor': 'start', 'xoff': -10, 'yoff': 15},
                ];
                var matchedElement = null;
                for (var configI in configs) {
                    var config = configs[configI];
                    var curLabel = makeWordInteractive(
                        svg.append("text")
                            .attr("x", x(data[i].x) + config['xoff'])
                            .attr("y", y(data[i].y) + config['yoff'])
                            .attr('class', 'label')
                            .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                            .attr('font-size', '10px')
                            .attr("text-anchor", config['anchor'])
                            .text(term),
                        term
                    );
                    var bbox = curLabel.node().getBBox();
                    var borderToRemove = .5;
                    var x1 = bbox.x + borderToRemove,
                        y1 = bbox.y + borderToRemove,
                        x2 = bbox.x + bbox.width - borderToRemove,
                        y2 = bbox.y + bbox.height - borderToRemove;
                    //matchedElement = searchRangeTree(rangeTree, x1, y1, x2, y2);
                    var matchedElement = false;
                    rectHolder.findMatchingRectangles(x1, y1, x2, y2, function (elem) {
                        matchedElement = true;
                        return false;
                    });
                    if (matchedElement) {
                        curLabel.remove();
                    } else {
                        break;
                    }
                }

                if (!matchedElement || term == 'auto') {
                    coords[term] = [x1, y1, x2, y2];
                    //rangeTree = insertRangeTree(rangeTree, x1, y1, x2, y2, term);
                    rectHolder.add(new Rectangle(x1, y1, x2, y2));
                    return true;

                } else {
                    //curLabel.remove();
                    return false;
                }

            }

            var radius = 2;

            function euclideanDistanceSort(a, b) {
                var aCatDist = a.x * a.x + (1 - a.y) * (1 - a.y);
                var aNotCatDist = a.y * a.y + (1 - a.x) * (1 - a.x);
                var bCatDist = b.x * b.x + (1 - b.y) * (1 - b.y);
                var bNotCatDist = b.y * b.y + (1 - b.x) * (1 - b.x);
                return (Math.min(aCatDist, aNotCatDist) > Math.min(bCatDist, bNotCatDist)) * 2 - 1;
            }

            function euclideanDistanceSortForCategory(a, b) {
                var aCatDist = a.x * a.x + (1 - a.y) * (1 - a.y);
                var bCatDist = b.x * b.x + (1 - b.y) * (1 - b.y);
                return (aCatDist > bCatDist) * 2 - 1;
            }

            function euclideanDistanceSortForNotCategory(a, b) {
                var aNotCatDist = a.y * a.y + (1 - a.x) * (1 - a.x);
                var bNotCatDist = b.y * b.y + (1 - b.x) * (1 - b.x);
                return (aNotCatDist > bNotCatDist) * 2 - 1;
            }

            function scoreSort(a, b) {
                return a.s - b.s;
            }

            function scoreSortReverse(a, b) {
                return b.s - a.s;
            }

            function backgroundScoreSort(a, b) {
                return b.bg - a.bg;
            }

            function arePointsPredictiveOfDifferentCategories(a, b) {
                var aCatDist = a.x * a.x + (1 - a.y) * (1 - a.y);
                var bCatDist = b.x * b.x + (1 - b.y) * (1 - b.y);
                var aNotCatDist = a.y * a.y + (1 - a.x) * (1 - a.x);
                var bNotCatDist = b.y * b.y + (1 - b.x) * (1 - b.x);
                var aGood = aCatDist < aNotCatDist;
                var bGood = bCatDist < bNotCatDist;
                return {aGood: aGood, bGood: bGood};
            }

            function scoreSortForCategory(a, b) {
                var __ret = arePointsPredictiveOfDifferentCategories(a, b);
                var aGood = __ret.aGood;
                var bGood = __ret.bGood;
                if (aGood && !bGood) return -1;
                if (!aGood && bGood) return 1;
                return b.s - a.s;
            }

            function scoreSortForNotCategory(a, b) {
                var __ret = arePointsPredictiveOfDifferentCategories(a, b);
                var aGood = __ret.aGood;
                var bGood = __ret.bGood;
                if (aGood && !bGood) return 1;
                if (!aGood && bGood) return -1;
                if (reverseSortScoresForNotCategory)
                    return a.s - b.s;
                else
                    return b.s - a.s;
            }

            data = data.sort(sortByDist ? euclideanDistanceSort : scoreSort);
            console.log("Sorted Data:");
            console.log(data);
            data.forEach(censorPoints);

            var myXAxis = svg.append("g")
                .attr("class", "x axis")
                .attr("transform", "translate(0," + height + ")")
                .call(xAxis);

            function registerFigureBBox(curLabel) {
                var bbox = curLabel.node().getBBox();
                var borderToRemove = 1.5;
                var x1 = bbox.x + borderToRemove,
                    y1 = bbox.y + borderToRemove,
                    x2 = bbox.x + bbox.width - borderToRemove,
                    y2 = bbox.y + bbox.height - borderToRemove;
                rectHolder.add(new Rectangle(x1, y1, x2, y2));
                //return insertRangeTree(rangeTree, x1, y1, x2, y2, '~~_other_');
            }

            //rangeTree = registerFigureBBox(myXAxis);
            var xLabel = svg.append("text")
                .attr("class", "x label")
                .attr("text-anchor", "end")
                .attr("x", width)
                .attr("y", height - 6)
                .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                .attr('font-size', '10px')
                .text(getLabelText('x'));

            //console.log('xLabel');
            //console.log(xLabel);

            //rangeTree = registerFigureBBox(xLabel);
            // Add the Y Axis
            var myYAxis = svg.append("g")
                .attr("class", "y axis")
                .call(yAxis)
                .selectAll("text")
                .style("text-anchor", "end")
                .attr("dx", "30px")
                .attr("dy", "-13px")
                .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                .attr('font-size', '10px')
                .attr("transform", "rotate(-90)");
            registerFigureBBox(myYAxis);

            function getLabelText(axis) {
                if (axis == 'y') {
                    if (yLabelText == null)
                        return modelInfo['category_name'] + " Frequency";
                    else
                        return yLabelText;
                } else {
                    if (xLabelText == null)
                        return modelInfo['not_category_name'] + " Frequency";
                    else
                        return xLabelText;
                }
            }

            var yLabel = svg.append("text")
                .attr("class", "y label")
                .attr("text-anchor", "end")
                .attr("y", 6)
                .attr("dy", ".75em")
                .attr("transform", "rotate(-90)")
                .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                .attr('font-size', '10px')
                .text(getLabelText('y'));
            registerFigureBBox(yLabel);

            var catHeader = svg.append("text")
                .attr("text-anchor", "start")
                .attr("x", width)
                .attr("dy", "6px")
                .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                .attr('font-size', '12px')
                .attr('font-weight', 'bolder')
                .attr('font-decoration', 'underline')
                .text("Top " + fullData['info']['category_name']);
            registerFigureBBox(catHeader);
            console.log(catHeader);

            function showWordList(word, termDataList) {
                var maxWidth = word.node().getBBox().width;
                for (var i in termDataList) {
                    var curTerm = termDataList[i].term;
                    word = (function (word, curTerm) {
                        return makeWordInteractive(
                            svg.append("text")
                                .attr("text-anchor", "start")
                                .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                                .attr('font-size', '12px')
                                .attr("x", word.node().getBBox().x)
                                .attr("y", word.node().getBBox().y
                                    + 2 * word.node().getBBox().height)
                                .text(curTerm)
                            ,
                            curTerm);
                    })(word, curTerm);
                    if (word.node().getBBox().width > maxWidth)
                        maxWidth = word.node().getBBox().width;
                    registerFigureBBox(word);
                }
                return {
                    'word': word,
                    'maxWidth': maxWidth
                };
            }

            function pickEuclideanDistanceSortAlgo(category) {
                if (category == true) return euclideanDistanceSortForCategory;
                return euclideanDistanceSortForNotCategory;
            }

            function pickScoreSortAlgo(category) {
                console.log("PICK SCORE ALGO")
                console.log(category)
                if (category == true) {
                    return scoreSortForCategory;
                } else {
                    return scoreSortForNotCategory;
                }
            }

            function pickTermSortingAlgorithm(category) {
                if (sortByDist) return pickEuclideanDistanceSortAlgo(category);
                return pickScoreSortAlgo(category);
            }

            function showAssociatedWordList(header, isAssociatedToCategory, length=14) {
                var sortedData = null;
                var sortingAlgo = pickTermSortingAlgorithm(isAssociatedToCategory);
                sortedData = data.sort(sortingAlgo);
                if (wordVecMaxPValue) {
                    function signifTest(x) {
                        if (isAssociatedToCategory)
                            return x.p >= 1 - minPVal;
                        return x.p <= minPVal;
                    }

                    sortedData = sortedData.filter(signifTest)
                }
                return showWordList(header, sortedData.slice(0, length));

            }

            var wordListData = showAssociatedWordList(catHeader, true);
            var word = wordListData.word;
            var maxWidth = wordListData.maxWidth;

            catHeader = svg.append("text")
                .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                .attr('font-size', '12px')
                .attr('font-weight', 'bolder')
                .attr('font-decoration', 'underline')
                .attr("text-anchor", "start")
                .attr("x", width)
                .attr("y", word.node().getBBox().y + 4 * word.node().getBBox().height)
                .text("Top " + fullData['info']['not_category_name']);


            wordListData = showAssociatedWordList(catHeader, false);
            word = wordListData.word;
            if (wordListData.maxWidth > maxWidth) {
                maxWidth = wordListData.maxWidth;
            }


            if (!nonTextFeaturesMode && !asianMode && showCharacteristic) {
                var title = 'Characteristic';
                if (wordVecMaxPValue) {
                    title = 'Most similar';
                }
                word = svg.append("text")
                    .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                    .attr("text-anchor", "start")
                    .attr('font-size', '12px')
                    .attr('font-weight', 'bolder')
                    .attr('font-decoration', 'underline')
                    .attr("x", catHeader.node().getBBox().x + maxWidth + 10)
                    .attr("dy", "6px")
                    .text(title);
                var sortMethod = backgroundScoreSort;
                if (wordVecMaxPValue) {
                    sortMethod = scoreSortReverse;
                }
                var wordListData = showWordList(word, data.sort(sortMethod).slice(0, 30));
                ;

                word = wordListData.word;
                maxWidth = wordListData.maxWidth;
                console.log(maxWidth);
                console.log(word.node().getBBox().x + maxWidth);

                svg.attr('width', word.node().getBBox().x + 3 * maxWidth + 10);
            }

            var numPointsLabeled = 0;
            for (var i = 0; i < data.length; i++) {
                if (labelPointsIfPossible(i)) numPointsLabeled++;
            }
            console.log('numPointsLabeled');
            console.log(numPointsLabeled);


            function populateCorpusStats() {
                var wordCounts = {};
                var docCounts = {}
                fullData.docs.labels.forEach(function (x, i) {
                    var cnt = (
                        fullData.docs.texts[i]
                            .trim()
                            .replace(/['";:,.?¿\-!¡]+/g, '')
                            .match(/\S+/g) || []
                    ).length;
                    wordCounts[x] = wordCounts[x] ? wordCounts[x] + cnt : cnt
                });
                fullData.docs.labels.forEach(function (x) {
                    docCounts[x] = docCounts[x] ? docCounts[x] + 1 : 1
                });
                var messages = [];
                fullData.docs.categories.forEach(function (x, i) {
                    var name = fullData.info.not_category_name;
                    if (x == fullData.info.category_internal_name) {
                        name = fullData.info.category_name;
                    }

                    messages.push('<b>' + name + '</b> document count: '
                        + Number(docCounts[i]).toLocaleString('en')
                        + '; word count: '
                        + Number(wordCounts[i]).toLocaleString('en'));
                });

                d3.select('#corpus-stats')
                    .style('width', width + margin.left + margin.right + 200)
                    .append('div')
                    .html(messages.join('<br />'));
            };

            if (fullData.docs) {
                populateCorpusStats();
            }

            if (saveSvgButton) {
                // from https://stackoverflow.com/questions/23218174/how-do-i-save-export-an-svg-file-after-creating-an-svg-with-d3-js-ie-safari-an
                var svgElement = document.getElementById("d3-div-1");

                var serializer = new XMLSerializer();
                var source = serializer.serializeToString(svgElement);

                if (!source.match(/^<svg[^>]+xmlns="http\:\/\/www\.w3\.org\/2000\/svg"/)) {
                    source = source.replace(/^<svg/, '<svg xmlns="https://www.w3.org/2000/svg"');
                }
                if (!source.match(/^<svg[^>]+"http\:\/\/www\.w3\.org\/1999\/xlink"/)) {
                    source = source.replace(/^<svg/, '<svg xmlns:xlink="https://www.w3.org/1999/xlink"');
                }

                source = '<?xml version="1.0" standalone="no"?>\r\n' + source;

                var url = "data:image/svg+xml;charset=utf-8," + encodeURIComponent(source);

                var downloadLink = document.createElement("a");
                downloadLink.href = url;
                downloadLink.download = fullData['info']['category_name'] + ".svg";
                downloadLink.innerText = 'Download SVG';
                document.body.appendChild(downloadLink);

            }

        };

        fullData = getDataAndInfo();
        processData(fullData);

        // The tool tip is down here in order to make sure it has the highest z-index
        var tooltip = d3.select('#' + divName)
            .append("div")
            .attr("class", sortByDist ? "tooltip" : "tooltipscore")
            .style("opacity", 0);
    };
}(d3);

function getDataAndInfo() { return{"docs": {"categories": ["Not Experienced", "Experienced"], "meta": ["Dave DeBarr (seattle, Intermediate)", "Pavlo Andriychenko (london, Experienced)", "Eduard Goma (barcelona, Novice)", "Alexander Hendorf (berlin, Novice)", "Skipper Seabold (dc, Novice)", "Francois Dion (carolinas, Novice)", "Sebastian Raschka (chicago, Novice)", "Ville Tuulos (sfo, Intermediate)", "Alexander Sibiryakov (berlin, Intermediate)", "Peadar Coyle (london, Intermediate)", "Joe McCarthy (seattle, Intermediate)", "Lev Konstantinovskiy (london, Intermediate)", "Jaime Fernandez del Rio (barcelona, Intermediate)", "David Higgins (berlin, Intermediate)", "Ryan Zotti (dc, Novice)", "Rob Chew (carolinas, Intermediate)", "Tom Augspurger (chicago, Novice)", "James Powell (sfo, Novice)", "Maciej Gryka (berlin, Intermediate)", "Bryan Van de Ven (london, Intermediate)", "Valentina Staneva (seattle, Novice)", "Jonathan Fernandes (london, Novice)", "Manu Carricano (barcelona, Experienced)", "Stephen Simmons (berlin, Intermediate)", "Stephen Simmons (dc, Intermediate)", "Dan Blanchard (carolinas, Intermediate)", "Jacob Schreiber (chicago, Intermediate)", "Krishna Sankar (sfo, Intermediate)", "Katharine Jarmul (berlin, Novice)", "Conrad Ho (london, Novice)", "James Powell (seattle, Novice)", "Oliver Zeigermann (london, Intermediate)", "Jorge Cimentada (barcelona, Novice)", "Gerrit Gruben (berlin, Intermediate)", "Aron Ahmadia (dc, Intermediate)", "Zeydy Ortiz (carolinas, Novice)", "Kevin Van Gundy (chicago, Intermediate)", "Divya Sardana (sfo, Intermediate)", "Daniel Kirsch (berlin, Intermediate)", "Stephen Simmons (london, Novice)", "Quentin Caudron (seattle, Novice)", "Edward Bullen (london, Novice)", "Guillem Borrell Nogueras (barcelona, Intermediate)", "Alexandru Agachi (berlin, Novice)", "Max Tsvetovat (dc, Experienced)", "Daniel Chen (carolinas, Novice)", "Hunter Owens (chicago, Intermediate)", "Ramesh Sampath (sfo, Novice)", "Angelos Kapsimanis (berlin, Intermediate)", "Daniel K Slater (london, Intermediate)", "Jim Crist (seattle, Novice)", "Julia Wagemann (london, Intermediate)", "Jordi Torrents (barcelona, Novice)", "James Powell (berlin, Intermediate)", "Nadia Udler (dc, Intermediate)", "Matt McCormick (carolinas, Intermediate)", "Nicole Carlson (chicago, Intermediate)", "Stephen F. Elston (sfo, Novice)", "Andreas Lattner (berlin, Intermediate)", "Malcolm Sherrington (london, Novice)", "Maxwell W Libbrecht (seattle, Intermediate)", "Andrew Stretton (london, Intermediate)", "Francesc Alted (barcelona, Intermediate)", "Jo-fai Chow (berlin, Novice)", "Patrick Harrison (dc, Intermediate)", "Ben Bolte (carolinas, Intermediate)", "Huda Nassar (chicago, Novice)", "James Powell (sfo, Novice)", "Ryan Henderson (berlin, Intermediate)", "  (london, Novice)", "Steve Dower (seattle, Novice)", "Oliver Laslett (london, Intermediate)", "Marius Miron (barcelona, Intermediate)", "Adrin Jalali (berlin, Experienced)", "Hunter Owens (dc, Novice)", "Peter Parente (carolinas, Intermediate)", "Safia Abdalla (chicago, Novice)", "Bryan Van de Ven (sfo, Intermediate)", "Nathan Epstein (berlin, Novice)", "Conrad Ho (london, Novice)", "Stephen Elston (seattle, Novice)", "Nick Radcliffe (london, Intermediate)", "Margriet Groenendijk (barcelona, Novice)", "Gustavo A. Patino (dc, Novice)", "Ashton Drew (carolinas, Intermediate)", "Jeffrey Yau (sfo, Intermediate)", "Martina Pugliese (berlin, Intermediate)", "Geoffrey French (london, Experienced)", "Denny Lee (seattle, Novice)", "Dr. Egor Kraev (london, Intermediate)", "Maciej Siwek (barcelona, Intermediate)", "Trent McConaghy (berlin, Intermediate)", "Kevin Markham (dc, Intermediate)", "Sarah Bird (carolinas, Novice)", "Mike Mull (chicago, Intermediate)", "Tony Kelman (sfo, Novice)", "Anton Dubrau (berlin, Intermediate)", "Mark Needham (london, Intermediate)", "Lingqiang Kong (seattle, Novice)", "Valerio Maggio (london, Intermediate)", "Guillem Borrell Nogueras (barcelona, Experienced)", "Ross Kippenbrock (berlin, Intermediate)", "Thomas Caswell (dc, Intermediate)", "John Pearson (carolinas, Intermediate)", "Bill Lattner (chicago, Experienced)", "Tracy Teal (sfo, Novice)", "Katharina Rasch (berlin, Intermediate)", "Min Ragan-Kelley (london, Intermediate)", "Alex Thomas (seattle, Intermediate)", "Elena Nemtseva (london, Intermediate)", "Claudia Guirao (barcelona, Novice)", "Miroslav Batchkarov (berlin, Novice)", "Steven Lott (dc, Intermediate)", "Chunhui Higgins (carolinas, Intermediate)", "David Giard (chicago, Intermediate)", "Jamie Whitacre (sfo, Intermediate)", "Andrej Warkentin (berlin, Novice)", "Raghotham Sripadraj (london, Intermediate)", "Katrina Riehl (seattle, Novice)", "James Powell (london, Intermediate)", "Guillem Duran Ballester (barcelona, Intermediate)", "Radovan Kavicky (berlin, Intermediate)", "Dhavide Aruliah (dc, Novice)", "Kyle Snavely (carolinas, Intermediate)", "James Powell (chicago, Intermediate)", "Randall Shane, PhD (sfo, Intermediate)", "Lev Konstantinovskiy  (berlin, Novice)", "Ulrich Zink (london, Intermediate)", "Felix Cheung (seattle, Intermediate)", "Will Moy (london, Novice)", "Constant Bridon (barcelona, Experienced)", "Max Humber (berlin, Novice)", "Ashrith Barthur (dc, Novice)", "Carol Willing (carolinas, Novice)", "Brian Lange (chicago, Novice)", "Josh Yudaken (sfo, Intermediate)", "Nils Magnus (berlin, Intermediate)", "Jake Coltman (london, Intermediate)", "Jason Kessler (seattle, Intermediate)", "Thomas Huijskens (london, Intermediate)", "Jordi Torrents (barcelona, Novice)", "Tal Perry (berlin, Intermediate)", "Chase Coleman (dc, Novice)", "Michael A. Alcorn (carolinas, Intermediate)", "Tom Augspurger (chicago, Intermediate)", "Stuart Geiger (sfo, Novice)", "Edouard Fouch\u00c3\u00a9 (berlin, Novice)", "Andreas Freise (london, Novice)", "Andreas Schreiber (seattle, Novice)", "Marco Bonzanini (london, Intermediate)", "Yufeng Guo (barcelona, Intermediate)", "Stefan Otte (berlin, Intermediate)", "Kelly Jin (dc, Novice)", "Marius van Niekerk (carolinas, Intermediate)", "Rayid Ghani (chicago, Novice)", "Dani Ushizima (sfo, Novice)", "Abhishek Thakur (berlin, Intermediate)", "Marco Bonzanini (london, Novice)", "George Richardson (seattle, Novice)", "Soledad Galli (london, Intermediate)", "Jordi Deu-Pons (barcelona, Intermediate)", "Raphael Pierzina (berlin, Novice)", "Robert Cohn (dc, Novice)", "Sarah Bird (carolinas, Novice)", "Joel Grus (chicago, Intermediate)", "Ross Taylor (sfo, Intermediate)", "Jessica Palmer (berlin, Novice)", "Kyran Dale (london, Novice)", "Olivia Gunton (seattle, Intermediate)", "Jens Nie (london, Novice)", "Jaime Fernandez del Rio (barcelona, Novice)", "Matti Lyra (berlin, Intermediate)", "Jason Grout (dc, Intermediate)", "Erin Mullaney (carolinas, Novice)", "Rob Story (chicago, Intermediate)", "James Powell (sfo, Intermediate)", "Thomas Reineking (berlin, Intermediate)", "Frank Kelly (london, Novice)", "Claire Kelley (seattle, Intermediate)", "Peadar Coyle (london, Experienced)", "Pascal van Kooten (barcelona, Experienced)", "Florian Wilhelm (berlin, Intermediate)", "Femi Anthony (dc, Intermediate)", "Philip Semanchuk (carolinas, Intermediate)", "Pavan Ramkumar (chicago, Experienced)", "Matt Davis (sfo, Novice)", "Mike M\u00c3\u00bcller (berlin, Novice)", "Annabelle Rolland (london, Novice)", "Ryan J. O'Neil (seattle, Intermediate)", "Chris Fregly (london, Intermediate)", "Egor Bulychev (barcelona, Intermediate)", "Karolina Alexiou (berlin, Intermediate)", "Pragyansmita Nayak (dc, Intermediate)", "Iain Carmichael (carolinas, Intermediate)", "J. Henry Hinnefeld (chicago, Intermediate)", "Chris Gorgolewski (sfo, Intermediate)", "Trent McConaghy (berlin, Intermediate)", "Sylvain Corlay (london, Intermediate)", "Raj Singh (seattle, Novice)", "Nathan Epstein (london, Intermediate)", "Carles Illa (barcelona, Novice)", "Nick Radcliffe (berlin, Intermediate)", "Marck Vaisman (dc, Intermediate)", "Jaafar Ben-Abdallah (carolinas, Intermediate)", "Kevin Goetsch (chicago, Intermediate)", "Chia-Chi Chang (sfo, Intermediate)", "Maciej Jaskowski (berlin, Novice)", "Or Weizman (london, Novice)", "Julie Michelman (seattle, Intermediate)", "Dr Caroline Clark (london, Novice)", "Nicholas A. Del Grosso (barcelona, Novice)", "Carlotta Schatten (berlin, Intermediate)", "Laura Lorenz (dc, Intermediate)", "Bret Davidson (carolinas, Novice)", "Alison Stanton (chicago, Intermediate)", "Rohan Koodli (sfo, Novice)", "Moritz Neeb (berlin, Intermediate)", "Ian  Ozsvald (london, Intermediate)", "Michal Monselise (seattle, Intermediate)", "Aileen Nielsen (london, Novice)", "Yang Liu (barcelona, Intermediate)", "Sirin Odrowski (berlin, Novice)", "Jiaqi Liu (dc, Intermediate)", "Benjamin Bengfort (carolinas, Intermediate)", "Matthew Turk (chicago, Novice)", "Sadayuki Furuhashi (sfo, Intermediate)", "Kashif Rasul (berlin, Intermediate)", "Moreno Bonaventura (london, Intermediate)", "Gary Dunow (seattle, Novice)", "Dr. Emlyn Clay (london, Intermediate)", "Camilo Cardona (barcelona, Novice)", "Alexander Weiss (berlin, Intermediate)", "Ariel M\u00e2\u0080\u0099ndange-Pfupfu (dc, Intermediate)", "Tony Ojeda (carolinas, Intermediate)", "Mike Heilman (chicago, Experienced)", "Florian Hartl (sfo, Experienced)", "David Higgins (berlin, Novice)", "Ronert Obst (london, Experienced)", "Keen Browne (seattle, Novice)", "Rebecca Bilbro (london, Intermediate)", "\u00d3scar Gallardo Rom\u00e1n (barcelona, Novice)", "Dr. Kristian Rother (berlin, Intermediate)", "Vishal Patel (dc, Intermediate)", "Andrew Knight (carolinas, Novice)", "Leon Sasson (chicago, Intermediate)", "Julie  Lavoie (sfo, Novice)", "Danny Bickson (berlin, Novice)", "Linda Uruchurtu (london, Intermediate)", "Pranav Bahl (seattle, Intermediate)", "Delger Enkhbayar (london, Intermediate)", "Massimo Domenico Sammito (barcelona, Novice)", "Rafael Schultze-Kraft (berlin, Novice)", "Alex Casalboni (dc, Intermediate)", "Kevin Prybol (carolinas, Novice)", "Piero Ferrante (chicago, Intermediate)", "Ajinkya More (sfo, Intermediate)", "[email\u00a0protected] (berlin, Intermediate)", "Steve Holden (london, Novice)", "Sumit Kumar (seattle, Intermediate)", "Nils Hammerla (london, Intermediate)", "Guillem Borrell Nogueras (barcelona, Intermediate)", "Abhishek Thakur (berlin, Intermediate)", "Nicole Donnelly (dc, Novice)", "Alice Broadhead (carolinas, Intermediate)", "Jorge Luis Rivero P\u00c3\u00a9rez (chicago, Experienced)", "Anusua Trivedi (sfo, Experienced)", "Guertel Idai (berlin, Novice)", "Tetiana Ivanova (london, Novice)", "Stephanie Kim (seattle, Intermediate)", "Ian Ozsvald (london, Intermediate)", "Miguel Zavala-Ake (barcelona, Intermediate)", "Andreas Dewes (berlin, Intermediate)", "Will Voorhees (dc, Intermediate)", "Ginny Ghezzo (carolinas, Intermediate)", "Nicholas Kridler (chicago, Novice)", "Jose Quesada (berlin, Intermediate)", "Vincent D. Warmerdam (london, Intermediate)", "Kyle Shaffer (seattle, Intermediate)", "Mike Innes (london, Intermediate)", "Claudia Mill\u00e1n (barcelona, Novice)", "Vincent D. Warmerdam (berlin, Intermediate)", "Jackie Kazil (dc, Intermediate)", "Francois Dion (carolinas, Intermediate)", "David Beazley (chicago, Novice)", "Megan Price (sfo, Novice)", "Jakob van Santen (berlin, Novice)", "Pete Owlett (london, Intermediate)", "Jeff Fischer (seattle, Intermediate)", "Tiffany Harte (london, Intermediate)", "Joaquin Pais (barcelona, Novice)", "David Soares Batista (berlin, Intermediate)", "Steven Lott (dc, Intermediate)", "Jean-Christophe Fillion-Robin (carolinas, Intermediate)", "Tudor Radoaca (chicago, Novice)", "Ian Stokes-Rees (sfo, Intermediate)", "Nora Neumann (berlin, Novice)", "Katie Barr (london, Intermediate)", "Sujit Pal (seattle, Intermediate)", "Alex Glaser (london, Novice)", "Diego Hueltes (barcelona, Intermediate)", "Fran\u00e7oise Provencher (berlin, Novice)", "Piero Ferrante (dc, Intermediate)", "Timothy Hopper (carolinas, Novice)", "Elizabeth Wickes (chicago, Intermediate)", "Gregory Kamradt (sfo, Intermediate)", "Fang Xu (berlin, Intermediate)", "  (london, Novice)", "Lou Harwood (seattle, Intermediate)", "Vincent D. Warmerdam (london, Novice)", "Emily Gorcenski (berlin, Experienced)", "Leland McInnes (dc, Novice)", "Dr. Anthony Scopatz (carolinas, Intermediate)", "Mehrdad Yazdani (chicago, Novice)", "Greg Dingle (sfo, Intermediate)", "Ian Ozsvald (berlin, Intermediate)", "Delia Rusu (london, Intermediate)", "PyData Seattle Diversity Panel (seattle, Novice)", "Thomas Alisi (london, Intermediate)", "Tom Bocklisch (berlin, Intermediate)", "Sebastien Genty (dc, Novice)", "Leah Silen (carolinas, Novice)", "Michael David Watson (chicago, Intermediate)", "Samantha Zeitlin (sfo, Intermediate)", "Robert Meyer (berlin, Intermediate)", "Ali Zaidi (london, Intermediate)", "Sourabh Bajaj (seattle, Intermediate)", "Nick Sorros (london, Intermediate)", "Amit Steinberg (berlin, Intermediate)", "Daniel Chen (dc, Novice)", "Dr. Lea Shanley (carolinas, Intermediate)", "Mat Kallada (chicago, Intermediate)", "Veronique Blanchard (sfo, Intermediate)", "Jie Bao (berlin, Intermediate)", "Dirk Gorissen (london, Intermediate)", "Jim Crist (seattle, Novice)", "Emma Deraze (london, Novice)", "H\u00e9ctor Andrade Loarca (berlin, Intermediate)", "Alex DeBrie (dc, Intermediate)", "Shariq Iqbal (carolinas, Intermediate)", "Eric Bunch (chicago, Novice)", "Mark Mikofski  (sfo, Intermediate)", "Shoaib Burq (berlin, Novice)", "Denis Akhiyarov (seattle, Intermediate)", "Guillaume Allain (london, Intermediate)", "Thomas Kober (berlin, Intermediate)", "Mandi Traud (dc, Novice)", "Colin Copeland (carolinas, Novice)", "Liz Sander (chicago, Intermediate)", "Charles Doutriaux (sfo, Intermediate)", "Matthew Honnibal (berlin, Experienced)", "Ben Chamberlain (london, Intermediate)", "Saranga Komanduri (seattle, Novice)", "Uwe L. Korn (london, Intermediate)", "Aisha Bello (berlin, Novice)", "Michal Malohlava (dc, Intermediate)", "Rebecca Bilbro  (carolinas, Intermediate)", "Cathy Deng (chicago, Intermediate)", "Steve Dower (sfo, Novice)", "Anne Matthies (berlin, Experienced)", "John Gill (london, Novice)", "Dr. Rutu Mulkar-Mehta (seattle, Intermediate)", "Frank Kelly (london, Novice)", "Robert Meyer (berlin, Intermediate)", "Anuj Gupta (dc, Intermediate)", "Rebecca Conley (carolinas, Novice)", "Taposh Roy (sfo, Novice)", "Felix Biessmann (berlin, Novice)", "Neal Lathia (london, Novice)", "Justin Kiggins (seattle, Novice)", "Laila Alabidi (london, Novice)", "Roelof Pieters (berlin, Intermediate)", "Darshan Pandit (dc, Intermediate)", "Josh Day (carolinas, Intermediate)", "David Mertz, Ph.D. (sfo, Novice)", "Valentine Gogichashvili (berlin, Intermediate)", "Keira Zhou (seattle, Novice)", "Andrew Rowan (london, Experienced)", "Ulrike Thalheim (berlin, Novice)", "Austin Rochford (dc, Intermediate)", "James Powell (carolinas, Intermediate)", "Sanghamitra Deb (sfo, Intermediate)", "Ruby Childs (london, Novice)", "Nathaniel Cook (seattle, Intermediate)", "Thomas Nuttall (london, Intermediate)", "Lev Konstantinovskiy (berlin, Intermediate)", "Greg Lamp (dc, Novice)", "Deepak Roy Chittajallu (carolinas, Intermediate)", "Jonathan Whitmore (sfo, Intermediate)", "Rui Miguel Forte (london, Intermediate)", "Moderated by James Powell (seattle, Intermediate)", "Zack Akil (london, Novice)", "Hendrik Heuer (berlin, Intermediate)", "SriSatish Ambati (dc, Novice)", "Josh Howes (carolinas, Intermediate)", "Daniel Moisset (berlin, Novice)", "James Powell (london, Experienced)", "Nicholas Kridler (seattle, Novice)", "Nuno Castro (london, Novice)", "Oliver Eberle (berlin, Intermediate)", "Gustavo A. Patino (dc, Intermediate)", "Ulric Wong (carolinas, Intermediate)", "Sumeet Sandhu (sfo, Intermediate)", "Lukasz Czarnecki (berlin, Intermediate)", "Travis Oliphant (london, Novice)", "Kelsey Jordahl (seattle, Novice)", "Lev Konstantinovskiy (london, Intermediate)", "Jonathan Ronen (berlin, Intermediate)", "Stephanie Kim (dc, Intermediate)", "Rob Agle (carolinas, Novice)", "James Powell (berlin, Experienced)", "Tariq Rashid (london, Novice)", "Josh Weissbock (seattle, Novice)", "Andrew Patterson (london, Intermediate)", "Vaibhav Singh (berlin, Intermediate)", "Brendan Herger (dc, Intermediate)", "Marshall Wang (carolinas, Intermediate)", "Christopher Roach (sfo, Novice)", "Christian Rebernik (berlin, Intermediate)", "Christian Hennig (london, Intermediate)", "Naoto Usuyama (seattle, Intermediate)", "Jeff Abrahamson (london, Intermediate)", "Daniele Rapati (berlin, Intermediate)", "Tim Marcinowski (dc, Novice)", "Carol Willing (carolinas, Intermediate)", "Nitin Borwankar (sfo, Intermediate)", "Frank Kaufer (berlin, Intermediate)", "Irwin Zaid (london, Intermediate)", "Maria Patterson (seattle, Intermediate)", "Alexandr Notchenko (london, Experienced)", "Matthew Rocklin (dc, Intermediate)", "Lanhui Wang (carolinas, Novice)", "Michelle Tran (berlin, Novice)", "Luc Rocher (london, Novice)", "Chris Fregly (seattle, Intermediate)", "Stephen Whitworth (london, Intermediate)", "Jon Bodner (dc, Novice)", "Hope E. Paasch (carolinas, Novice)", "Dat Tran (sfo, Intermediate)", "Eddie Bell (london, Experienced)", "Jean-Rene Gauthier (seattle, Intermediate)", "Dr. Shahzia Holtom (london, Experienced)", "Tony Ojeda (dc, Novice)", "Candida Haynes  (sfo, Novice)", "Thomas Wiecki (london, Intermediate)", "Pramit Choudhary (seattle, Intermediate)", "Soraya Hausl (london, Intermediate)", "Renee M. P. Teate (dc, Novice)", "Tanmoy Mukherjee (london, Intermediate)", "Dhruv Madeka (seattle, Intermediate)", "Deborah Leem (london, Novice)", "Holden Karau (dc, Intermediate)", "Philippe Bracke (london, Novice)", "Michael Sarahan (seattle, Intermediate)", "Ed Snelson (london, Intermediate)", "Marie Whittaker (dc, Novice)", "Calvin Giles (london, Experienced)", "en zyme (seattle, Novice)", "Raoul-Gabriel Urma (london, Intermediate)", "Jonathan Sedar (london, Experienced)", "Stephen Hoover (seattle, Intermediate)", "Yasen Kiprov (london, Novice)", "James Powell (dc, Novice)", "Sandra Greiss (london, Intermediate)", "Jeffrey Heer (seattle, Novice)", "James Allen (london, Novice)", "Christopher D. White (dc, Intermediate)", "  (london, Novice)", "Brian Granger, Chris Colbert & Ian Rose (seattle, Intermediate)", "Katie Barr (london, Intermediate)", "David Eads (dc, Intermediate)", "Rich Lewis (london, Experienced)", "Eloisa Tran (seattle, Intermediate)", "Tariq Rashid (london, Novice)", "Andy  Terrel (dc, Intermediate)", "Simon Byrne (london, Novice)", "Bryan Van de ven (seattle, Intermediate)", "Bobby Filar (dc, Novice)", "Delia Rusu (london, Novice)", "Rob Story (seattle, Intermediate)", "Konstantinos Xirogiannopoulos (dc, Intermediate)", "Marko Vasiljevski (london, Intermediate)", "Katie Porterfield (seattle, Novice)", "Fabrizio Milo (dc, Experienced)", "Aileen Nielsen (london, Intermediate)", "Randall J. LeVeque (seattle, Novice)", "Graham Markall (london, Experienced)", "Matt Braymer-Hayes (seattle, Intermediate)", "Jennifer A. Stark (dc, Novice)", "Lev Konstantinovskiy  (london, Intermediate)", "Tom Radcliffe (seattle, Experienced)", "Gene Kogan (london, Novice)", "Ramon Navarro Bosch (barcelona, Intermediate)", "Irina Vidal Migallon (berlin, Novice)", "Elizabeth Lindsey (dc, Novice)", "Luke Starnes (carolinas, Intermediate)", "Benjamin Hodel (chicago, Experienced)", "Dr. Frank Gerhardt (berlin, Novice)", "Tim Vivian-Griffiths (london, Intermediate)", "Kathryn Harris (london, Novice)", "Alexey Grigorev (berlin, Intermediate)", "Star Ying (dc, Novice)", "Thomas A Caswell (carolinas, Novice)", "Brian E. Granger (sfo, Novice)", "Juha Suomalainen (berlin, Intermediate)", "Ricardo Pio Monti (london, Experienced)", "Eszter Windhager-Pokol (london, Intermediate)", "Nikolai Nowaczyk (london, Intermediate)", "Natalia Angarita-Jaimes (london, Intermediate)", "Thomas Kluyver (london, Experienced)", "Maciej Gryka (london, Intermediate)"], "labels": [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0], "texts": ["Using CNTK's Python Interface for Deep Learning\n \nTopics to be covered include ...\n\nCognitive Toolkit (CNTK) installation\nWhat is \"machine learning\"? [gradient descent example]\nWhat is \"learning representations\"?\nWhy do Graphics Processing Units (GPUs) help?\nHow do we prevent overfitting?\nCNTK Packages and Modules\nDeep Learning Examples [including Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) examples]\n\n \nWe will review tutorial examples of using CNTK's python interface for image classification, speech recognition, and natural language processing.", "Make your research interactive with Jupyter Dashboards\n \nJupyter is great for tinkering and research. But what if you are not the only consumer of that notebook? What if other consumers want to slice and visualise your data differently? I will show how to turn fresh ideas or existing Jupyter notebooks into beautiful web apps without the need to worry about the web server, HTTP and content deployment.\n \nI will show the tools and processes for building interactive web apps using the Jupyter stack. Simple, easy, beautiful.", "Introduction to data analysis with Pandas\n \nWhat will we do in the workshop\n\nReading CSV files. Explore the different possibilities we have (with/without header, parsing dates, different separators, ...)\nUse of DatetimeIndex: resampling, slicing\nUse of multiindex\nPlotting data\n\n \nIntroductory workshop to show the first steps to be taken when analyzing data with Pandas. In this workshop we will load a dataset into a pandas dataframe, transform it and clean it to perform our initial data analysis. We will use matplotlib with ggplot styling to visualize the data.", "Introduction to Data-Analysis with Pandas\n \nPandas is the Swiss-Multipurpose Knife for Data Analysis in Python. With Pandas dealing with data-analysis is easy and simple but there are some things you need to get your head around first as Data-Frames and Data-Series.\nThe tutorial provides a compact introduction to Pandas for beginners:\n\nreading and writing data across multiple formats (CSV, Excel, JSON, SQL, HTML,\u2026)\ndata visualisation\nstatistical data analysis and aggregation.\nwork with built-in data visualisation\ninner-mechanics of Pandas: Data-Frames, Data-Series & Numpy.\nworking and making the most of indexes.\nhow to mangle, reshape and pivot\n\nThe tutorial will be provided as Jupiter notebooks.\n \nPandas is the Swiss-Multipurpose Knife for Data Analysis in Python. With Pandas dealing with data-analysis is easy and simple but there are some things you need to get your head around first as Data-Frames and Data-Series.\nThe tutorial provides a compact introduction to Pandas for beginners for I/O, data visualisation, statistical data analysis and aggregation within Jupiter notebooks.", "Using Dask for Parallel Computing in Python\n \nThe tutorial will introduce users to the core concepts of dask including\n\nthe dask scheduler\ndask graphs\nthe dask caching layer\n\nThe tutorial will also introduce the core data structures including\n\ndask arrays\ndask bags\n\n \nDask is a relatively new library for parallel computing in Python. It builds around familiar data structures to users of the PyData stack and enables them to scale up their work on one or many machines. This tutorial will introduce users to the core concepts of dask by working through some example problems. The tutorial will be distributed via Jupyter Notebooks.", "Datascience on the web\n \nJupyter is a great notebook environment for Python based data science and exploratory data analysis. You can share the notebooks via a github repository,  as html or even on the web using something like JupyterHub. How can we turn the work we have done in the notebook into a real web application?\nIn this tutorial, you will learn to structure your notebook for web deployment, how to create a skeleton Flask application, add a model and add a visualization. While you have some experience with Jupyter and Python, you do not have any previous web application experience.\nBring your laptop and you will be able to do all of these hands-on things:\n\n\nget to the virtual environment\n\n\nreview the Jupyter notebook\n\n\nrefactor for reuse\n\n\ncreate a basic Flask application\n\n\nbring in the model\n\n\nadd the visualization\n\n\nprofit!\n\n\n \nLearn to deploy your research as a web application. You have been using Jupyter and Python to do some interesting research, build models, visualize results. In this tutorial, you\u00e2\u0080\u0099ll learn how to easily go from a notebook to a Flask web application which you can share.", "Learning scikit-learn -- An Introduction to Machine Learning in Python\n \nThis tutorial will teach you the basics of scikit-learn. We will learn how to leverage powerful algorithms from the two main domains of machine learning: supervised and unsupervised learning. In this talk, I will give you a brief overview of the basic concepts of classification and regression analysis, how to build powerful predictive models from labeled data. Furthermore, we will go over the basics of clustering analysis to discover hidden structures in unlabeled data. Although it's not a requirement for attending this tutorial, I highly recommend you to check out the accompanying GitHub repository at https://github.com/rasbt/pydata-chicago2016-ml-tutorial 1-2 days before the tutorial. During the session, we will not only talk about scikit-learn, but we will also go over some live code examples and code simple machine-learning algorithms from scratch to get the knack of scikit-learn's API.   \nIf you have any questions about the tutorial, please don't hesitate to contact me. You can either open an \"issue\" on GitHub or reach me via email at mail_at_sebastianraschka.com. I am looking forward to meeting you soon! \n \nThis tutorial provides you with a comprehensive introduction to machine learning in Python using the popular scikit-learn library. We will learn how to tackle common problems in predictive modeling and clustering analysis that can be used in real-world problems, in business and in research applications. And we will implement certain algorithms as scratch as well, to internalize the inner workings ", "TrailDB tutorial: Store and process billions of events efficiently\n \nWhat is TrailDB?\nTrailDB is designed to be a core building block for systems that need to store and process a large number of discrete events, organized by a primary key. It is complementary to existing relational and time-series databases and key-value stores.\nWhat makes TrailDB different is immutability: Immutable data enables deeper compression, scalability, and architectural decisions, which would not be feasible with existing databases. This is especially true for cloud environments with object stores like Amazon S3 that are a perfect match for compressed, immutable files.\nDeveloper productivity is another main motivation of TrailDB. Individual files are easy to manipulate using standard filesystem tools. The easily portable C library has only a few easily available dependencies, making it easily deployable. The API is clean and minimal by design. Language bindings are provided for Python, Go, Haskell, R, and D.\nTrailDB is a perfect match for use cases that involve detecting patterns over time, such as web/mobile analytics, anomaly detection, and various machine learning models. Since 2014, AdRoll has used TrailDB to store and query over 20 trillion events that power a number of products at AdRoll. TrailDB was open-sourced in May 2016.\nTutorial\nThis is a hands-on tutorial teaching all you have to know about TrailDB and how to use it in Python. You can bring your own data set consisting of events with a timestamp, a primary key and additional fields, or you can use one of the public data sets provided.\nOutline\nTrailDB 101\n\nInstall TrailDB on your laptop or a server\nBuild your first TrailDB\nQuerying TrailDBs\n\nTrailDB 201\n\nOptimizing performance\nDesigning an architecture around TrailDB \nTrailDB internals and contributing back\n\n \nTrailDB is an efficient library for storing and querying series of events. It shines at compressing a large number of discrete events in a small space - often small enough to allow processing on a single server. TrailDB is implemented in C and it comes with Python bindings.\nThis is a hands-on tutorial that gets you started with TrailDB. Bring your own data or play with a public data set.", "Frontera: open source, large scale web crawling framework\n \nIn this talk I'm going to share our experience crawling the Spanish web. We aimed at crawling about ~600K websites in .es zone, to collect statistics about hosts and their sizes. I'll describe crawler architecture, storage, problems we faced during the crawl and solutions found.\nOur solution is accessible in open source, as Frontera framework. It provides pluggable document and queue storage: RDBMS or Key-Value based, crawling strategy management, communication bus to choose: Kafka or ZeroMQ, using Scrapy as a fetcher, or plugging your own fetching component.\nFrontera allows to build a scalable, distributed web crawler to crawl the Web at high rates and large volumes. Frontera is online by design, allowing to modify the crawler components without stopping the whole process. Also Frontera can be used to build a focused crawlers to crawl and revisit a finite set of websites.\nTalk is organized in fascinating form: problem description, solution proposed, and issues appeared during the development and running the crawl.\n \nWe've tried to crawl the Spanish (.es zone) internet,  containing about ~600K websites to collect stats about hosts and their sizes. I'll describe the crawler architecture, storage, problems we faced with during the crawl and solutions found. Finally we released our solution as Frontera framework, allowing to build an online, scalable web crawlers using Python.", "Lies damned lies and statistics in Python\n \nStatistical inference is a fundamental tool in science and engineering, but it is often poorly understood. This tutorial will take you from first principles through how to do good statistical analysis in Python. \nBoth frequentist and Bayesian versions will be included, and attendees will leave the talk with a good understanding of statistical inference, how to use Scikit-learn, Statsmodels and PyMC3.\nThis will be an entire case study comparing three different libraries for solving the same regression problem. \nAttendees should bring a Laptop with Anaconda installed and the latest version of Scikitlearn, PyMC3 and Scikit-Learn. \nNotebooks will be provided.\n \nJoin Peadar as he talks you through how to do good statistical analysis with Statsmodels, scikit-learn and PyMC3\nWe'll compare and debug models (logistic regression and more advanced versions) in three different statistics packages in Python. \nPeadar will show you how Python can help you fake statistics without having to open up your graduate textbook :)", "D\u2019oh! Unevenly spaced time series analysis of The Simpsons in Pandas\n \nIndeed data scientists occasionally analyze time series data in which the events of interest are unevenly spaced. For example, when we want to understand how a change to a user interface for Indeed Hire recruiters affects the time it takes them to review candidates, we might look at changes in time intervals between individual candidate dispositions in our logs. When we want to understand the ratio of new business to repeat business - or explore different definitions of repeat business - we analyze the intervals in the creation dates of new requisitions from the same client.\nThe Pandas data analysis library offers powerful tools for conducting time series analysis. When working on unevenly spaced time series, we have found the shift() and transform() DataFrame methods particularly helpful. Many of the examples of using these methods that we found on the web were used only on small, artificial datasets. Determining how best to apply them to real datasets was not always as straightforward as we would have hoped.\nRather than use internal proprietary data to illustrate examples of how these methods can be used effectively to analyze unevenly spaced time series data, we will instead use data from a publicly available dataset of episodes of The Simpsons at data.world. In doing so, we will also provide an introduction on how to use the data.world API.\nThe purpose of this tutorial is to\n\nProvide a brief, focused primer on some basic aspects of Pandas\nProvide an overview of data.world datasets and accessing them via the API\nShow how advanced Pandas tools can be used for analyzing unevenly spaced time series data\n\nParticipants will be best prepared for this tutorial if they\n\nUnderstand Python basics\nHave Python 2 or Python 3 installed on their computers\nInstall  the latest versions of Pandas and Jupyter Notebook (recommended: use Anaconda)\nInstall the data.world Python API (pip install git+git://github.com/datadotworld/data.world-py.git)\nCreate a data.world account and an API key via the data.world Advanced Settings page\n\nUpdate: jupyter notebooks associated with the tutorial have been uploaded to a GitHub repository.\n \nThis tutorial will explore the use of tools in the Pandas data analysis library for analyzing unevenly spaced time series data. The tutorial will start off with a brief primer on Pandas and the data.world API, and demonstrate how to use Pandas tools for analyzing data from The Simpsons episodes from data.world.", "Topic Modelling (and more) with NLP framework Gensim\n \nTopic Modelling is a great way to analyse completely unstructured textual data - and with the python NLP framework Gensim, it's very, very easy to do this. The purpose of this tutorial is to guide one through the whole process of topic modelling - right from pre-processing your raw textual data, creating your topic models, evaluating the topic models, to visualising them. Advanced topic modelling techniques will also be covered in this tutorial, such as Dynamic Topic Modelling, Topic Coherence, Document Word Coloring, and LSI/HDP.\nThe python packages used during the tutorial will be spaCy (for pre-processing), gensim (for topic modelling), and pyLDAvis (for visualisation). The interface for the tutorial will be an Jupyter notebook. \nThe takeaway from the tutorial would be the participants ability to get their hands dirty with analysing their own textual data, through the entire lifecycle of cleaning raw data to visualising topics.\n \nTopic Modelling is an information retrieval technique to identify topics in a large corpus of text documents. This tutorial will guide you through the process of analysing your textual data through topic modelling - from finding and cleaning your data, pre-processing using spaCy, applying topic modelling algorithms using gensim - before moving on to more advanced textual analysis techniques.", "Taking NumPy in Stride\n \nThe workshop material is on github!\nhttps://github.com/jaimefrio/pydatabcn2017/\nCheck the README in the above link for more details.\n \nThis workshop is aimed at users already familiar with NumPy. We will dissect\nthe NumPy memory model with the help of a very powerful abstraction: strides.\nParticipants will learn how to create different views out of the same data,\nincluding multidimensional ones, get a new angle on how and why broadcasting\nworks, and explore related techniques to write faster, more efficient code.", "Introduction to Julia for Scientific Computing and Data Science\n \nJulia is a new and exciting language, sponsored in part by NumFocus, and developed at MIT. With a focus on fast numerical computing, it has a syntactical complexity similar to that of Python or Matlab but a performance orders of magnitude faster. This means you can quickly code up your ideas in a scripting language, but you don't need to switch langauges when you later need to squeeze out every last ounce of performance on production data.\nShould you stick with Python, or is it time to move to Julia?\nWe will try to answer this question by presenting an introduction to the language, followed by a sampling of packages from the most important projects relevant to data and numerical science. Python has become a powerhouse in these fields largely due to its available libraries. Julia is fast making up this ground, with an integrated package manager and the ability to call Python libraries, we will show where the gap has already been bridged and where there is still reason to hold back. We will demonstrate that the language syntax is already as easy as the simplest specialist alternatives (eg. Matlab/Mathematica/S-Plus), but this is a fully fledged programming language, and the libraries are no longer necessarily a reason to hold back.\nThe Tutorial format\nWe will begin by introducing the basic syntax, showing the paths of least resistance for users moving from Python, Matlab and other major languages. We will quickly move-on to explain the details of how Julia is different from other scripted programming languages. Typically, these differences, such as the system of multiple dispatch, have been specifically chosen as they allow for faster runtimes from scripted code without impacting on read/writeability. Finally, we will work through some specific examples of Visualisation (Plots.jl), Data Wrangling (ie DataFrame-type options), Numerical Optimisation (JuliaOpt/JuMP.jl) and High-Performance Computing (JuliaGPU/OpenCL.jl, also profiling, benchmarking and debugging if time allows) as an introduction to some of the packages currently available in the Julia package ecosystem.\n\nNote: this is an interactive tutorial please follow the Installation Instructions, which will be posted at https://github.com/daveh19/pydataberlin2017 shortly before the meeting, if you wish to participate!\n\n \nDeveloped at MIT, with a focus on fast numerical computing, Julia has a syntactical complexity similar to that of Python or Matlab but a performance orders of magnitude faster. We will present an introduction to the language, followed by a sampling of some of our favourite packages. The focus is on which aspects of Julia are currently ready for use by numerical computing and data scientists.", "How to Build Your Own Self-Driving (Toy) Car\n \nHere are some of the things you'll learn about:\n\nGetting started with the Raspberry Pi\nTenserFlow and machine learning\nOptimally tuning neural networks\nModel training on AWS GPUs\nOpenCV and Haar Cascades for object detection in images\n\n \nI\u00e2\u0080\u0099ve spent the past 6 months building a self-driving toy car using a Raspberry Pi, OpenCV, and TensorFlow. If you\u00e2\u0080\u0099ve ever thought about building your own self-driving toy car, this presentation will help you avoid common pitfalls and shed light on important tradeoffs that you\u00e2\u0080\u0099ll have to weigh along the way. I\u00e2\u0080\u0099ll cover things like how to parse images, how to effectively tune machine learning neural", "Connected:  A Social Network Analysis Tutorial with NetworkX\n \nMethods will be illustrated using a dataset of the romantic relationships between characters on \"Grey's Anatomy\", an American medical drama on the ABC television network.  Analysis and intuition will be emphasized over theory and mathematical rigor.  An IPython/Jupyter notebook format will be used as we code through the examples together.\nOUTLINE\n\nWhat is Social Network Analysis?  A short history, motivating examples, and terminology\nNodes\nEdges\nAdjacency\nAttributes and Weights\n\n\nCreating Graphs\nCreate graph object\nAdd nodes and edges\nAdd attributes\nUndirected vs. Directed graphs\n\n\nVisualizing Graphs\nDraw graph with Matplotlib\nDraw graph with Graphviz\nOther visualization options -- Python and beyond (Gephi, Cytoscape, D3, etc.)\n\n\nCentrality - \"Who's the Boss?\"\nDefinition of Centrality\nExamples (Medici family, trade example, etc.)\nCompare and contrast popular centrality measures on dataset\nDegree\nCloseness\nBetweenness\nEigenvector\n\n\nCommunity - \"Where do I belong?\"\nDefinition of Community and Community Detection\nExamples\nPerform community detection on dataset\nk-cliques or Girvan-Newman\n\n\n\n\nLink Prediction - \"People you May Know\"\nDefinition of Link Prediction\nExamples\nPerform link prediction on dataset\nJaccard coefficient\nPreferential Attachment\n\n\n\n\nExporting graphs\n\nBACKGROUND AND PRE-REQS\nIf you've obtained python through Anaconda, you should be set. In essence, we need Python 3 and the conda package manager.\n\nAnaconda\nPython 3\nConda package manager\n\n\nPackages listed below\n\nThis is an intermediate level tutorial, so we expect  prior knowledge of:\n\nPython types & data structures\nInstalling python packages through pip or conda\nData Science Packages:\npandas\nnumpy\nmatplotlib\n\n\n\n \nSocial Network Analysis (SNA), the study of the relational structure between actors, is used throughout the social and natural sciences to discover insight from connected entities.  In this tutorial, you will learn how to use the NetworkX library to analyze network data in Python,  emphasizing intuition over theory.  ", "Pandas: .head() to .tail()\n \nThe tutorial will focus on solving common problems in data analysis by writing clean, readable, efficient code.\nPandas will be the primary tool, though integrations with other libraries like scikit-learn, statsmodels, and matplotlib will be demonstrated.\nThe emphasis will be on gradually learning methods for massaging data into the correct form through real applications, rather than an exhaustive walk-through of pandas' API.\nThis tutorial is aimed at beginner and intermediate PyData users.\nAttendees will hopefully have some experience with NumPy.\nThe basics of NumPy and its relationship to pandas will briefly be covered.\nThe core of the tutorial covers\n\nSelecting and indexing\nReshaping and tidy data\nGrouped operations and summarization\nMerging and joining\n\nAfter covering the those operations outlined above we'll next (time-permitting) look at some of the more specialized areas of pandas including Categoricals, time-series analysis, hierarchical indexes, chunked / out of core processing, and data pipelines.\n \nAn introduction to using pandas for data analysis.\nMaterials are available on Github.\nPlease clone the repository and follow the setup before arriving.", "What's new in Python 3\n \nComing soon.\n \nComing soon.", "Removing Soft Shadows with Hard Data\n \nManipulated images lose believability if the user's edits fail to account for shadows. We propose a method that makes removal and editing of soft shadows easy. Soft shadows are ubiquitous, but remain notoriously difficult to extract and manipulate. We posit that soft shadows can be segmented, and therefore edited, by learning a mapping function for image patches that generates shadow mattes. We validate this premise by removing soft shadows from photographs with only a small amount of user input.\nGiven only broad user brush strokes that indicate the region to be processed, our new supervised regression algorithm automatically unshadows an image, removing the umbra and penumbra. The resulting lit image is frequently perceived as a believable shadow-free version of the scene. We tested the approach on a large set of soft shadow images, and performed a user study that compared our method to the state of the art and to real lit scenes. Our results are more difficult to identify as being altered, and are perceived as preferable compared to prior work.\n \nFind out how we used recent advances in photorealistic rendering and machine learning to teach a Random Forest how soft shadows look like. See how this model can then be used to to remove and modify shadows in RGB photographs with minimal user input.", "Bokeh for Data Applications and Visualization\n \n\n0:00 - 0:05 Introduction and Setup\n0:05 - 0:20 Review and Practice with Bokeh Basics\n0:20 - 0:40 Widgets, Interactions and Streaming in the Notebook\n0:40 - 1:05 Handling Larger Data Sets with Bokeh and Datashader\n1:05 - 1:30 Building Bokeh Applications\n\n \nBokeh is an Interactive visualization library that targets modern web browsers for presentation. It provides elegant, concise construction of basic exploratory and advanced custom graphics, but can also be used to develop sophisticated  dashboards and data applications.", "From Novice to Data Ninja\n \nData are all around us and come in all sizes and shapes. As a beginner Python learner you often dream to become a master of all data types and handle them with ease: but soon you get intimidated by piles of libraries and unfamiliar jargon surrounding specific data formats and processing tools. The good news is that once you overcome the initial hurdles, you realize that you can analyze these diverse datasets using similar methodology.   \nThe goal of this tutorial is to provide participants with hands-on experience working with diverse types of data: text, sound, video, graphs, GIS. Each data module will include an example with a small dataset to introduce the properties of the data type, and one with a larger dataset to equip the learners with tools to handle the vast data in the wild. Throughout the tutorial we will illustrate how same data mining techniques can be used on different types of data.\nhttps://github.com/valentina-s/Novice2DataNinja\n \nThis tutorial aims to introduce beginner Python learners to the diverse types of data a\u00a0data scientist may face at work: time series, images, videos, text, graphs, geospatial data. In two hours participants will become comfortable handling each type of data and extracting interesting information from it.  Participants will also learn tips and tricks how to handle large datasets of each kind.", "From zero to Kung Fu Panda(s)\n \nThese are the topics we will cover to go from zero to hero in 1.5 hours.\nSection 1 \u2013 \u201cZero\u201d (30 minutes)\nIn the first 30 minutes, I will introduce the attendees  to a dataset that we will be using for the tutorial session. I want attendees to get their hands dirty from the start and get a feel for Pandas using commands that will help them get an overview of their data set with some basic plotting.\nSection 2 \u2013 \u201cPanda(s) in training\u201d (30 minutes)\nNow that the audience have interacted with Pandas, I want them to give them a more detailed view of Panda's building blocks - Dataframes and Series. I will introduce the concept of broadcasting across Series using Boolean Indexing and basic data manipulation. Additionally we will look at both identifying and effectively dealing with missing data. This segment will be peppered with exercises for participants to practice and confirm their learning.\nSection 3 \u2013 \u201cKung Fu Panda(s)\u201d (30 minutes)\nIn this  last section, we will look at more complex plotting options with Seaborn. I will introduce groupby, indexing, stacking, unstacking and friends. We will end this session by getting a better understanding of manipulating strings and regular expressions in Pandas.\n \nPandas is Python\u2019s data science work horse and is used routinely by Data scientists across the globe. Using a hands-on based approach, attendees will go from beginner level to covering intermediate-level content  in 90 minutes.", "The Perfect Wave hackathon\n \n29 millions surfers worldwide are looking for the same scarce resource: good waves. But what makes a good wave? Swell, coast & beach topography, (low) density of local population, reputation? The quality of a wave is highly subjective and depends both of measurable elements and surfers' perceptions. The aim of the \"Perfet Wave Hackathon\" is therefore to improve current swell models using traditional data inputs (tides, weather, water level, circulation, waves, etc.) with local contextual data, open data, and any relevant input that could improve model accuracy and the quality of a related analytical product.\nThe workshop is organized in a 4h sessions, with 2 main steps: \n\nStep 1: the Take Off\n\nGetting started with the objectives & constraints, the API and take off of the modelling activity.\n\nStep 2: the Tube\n\nTime for finalizing your tricks, fine tune the presentation and get your score from the Jury of Senior Data Scientist.\nHow are you going to be evaluated?\nCommitment and degree of difficulty\nInnovative and progressive maneuvers\nCombination of major maneuvers\nVariety of maneuvers\nSpeed, power and flow\n\nGood luck and enjoy!\n \nWaves are great metaphors for models: what makes a good model? What makes a good wave? In this 4h workshop you will have access to core weather and ocean data through a dedicated API to create advanced forecasting models and new exciting surfing related analytical products.", "Pandas from the Inside / \"Big Pandas\"\n \nPandas is great way to quickly get started with data analysis in Python: intuitive DataFrames from R; fast numpy arrays under the hood; groupby just like SQL. But this familiarity is deceptive and both new and experienced pandas users often get stuck on things they feel should be simple.\nIn the first part of this tutorial, we look inside pandas to see how DataFrames actually work when building, indexing and grouping tables. We will learn which pandas operations are fast and why, and how to avoid common performance pitfalls. By the end of the tutorial, you will develop a strong and reliable intuition about using pandas effectively.\nIn the second part, we switch gear to bigger problems where our data sets can't fit in local memory. First we see how pandas behaves as we start to hit memory limits. Then we look at Dask, whose distributed/deferred DataFrames are a near drop-in replacement for pandas. Then we come back to pure pandas and look for ways to manage bigger datasets with clever data storage,.\nDuring this tutorial, you are welcome to follow along on your laptop with the sample data sets and example code in a Jupyter notebook. These will be made available on GitHub here just before the tutorial. The code targets Python 3 and the latest pandas/dask release:\n \nPandas is great for data analysis in Python. It promises intuitive DataFrames from R; speed like numpy; groupby like SQL. But there are plenty of pitfalls. This tutorial looks inside pandas to see how DataFrames actually work when building, indexing and grouping tables. You will learn how to write fast, efficient code, and how to scale up to bigger problems with libraries like Dask.", "Pandas from the Inside\n \nPandas is great way to quickly get started with data analysis in Python: intuitive DataFrames from R; fast numpy arrays under the hood; groupby just like SQL. But this familiarity is deceptive and both new and experienced pandas users often get stuck on things they feel should be simple. \nIn this tutorial, we look inside pandas to see how DataFrames actually work when building, indexing and grouping tables. We will learn which pandas operations are fast and why, and how to avoid common performance pitfalls. By the end of the tutorial, you will develop a strong and reliable intuition about using pandas effectively.  \nDuring this tutorial, you are welcome to follow along on your laptop with the sample data sets and example code in a Jupyter notebook. These will be made available on GitHub a few days before the tutorial. The code targets Python 3 and the latest pandas release (currently 0.18.1).\n \nPandas is great for data analysis in Python: intuitive DataFrames from R; fast numpy arrays under the hood; groupby like in SQL. But this familiarity is deceptive: pandas users often get stuck on things they feel should be simple. This talk look inside pandas to see how DataFrames actually work when building, indexing and grouping tables. You will learn how to write fast, efficient pandas code.", "Simplifying large scale parallel processing with Storm and streamparse\n \nThis talk will cover the basics of Apache Storm and streamparse and why they're useful, discuss some of the efforts we're taking to unify the core components of the two competing Python Storm libraries (pyleus and streamparse), and show some of the command-line utilities that streamparse provides to simplify managing Storm topologies.\n \nStreamparse is a popular Python library for writing bolts/spouts (i.e., workers/producers) for use with Apache Storm. If you've ever been bitten by the GIL when trying to process data at scale, you will enjoy seeing how the Storm/streamparse combination can be used to sidestep the issue entirely.\nThis talk will cover the basics of Apache Storm and streamparse and why they're useful, discuss some of the efforts we're taking to unify the core components of the two competing Python Storm libraries (pyleus and streamparse), and show some of the command-line utilities that streamparse provides to simplify managing Storm topologies.", "pomegranate: fast and flexible probabilistic models in python\n \npomegranate is a python module for probabilistic modelling with a speedy cython implementation and a focus on ease of use. It includes a wide variety of probability distributions, which can be used by themselves or assembled into general mixture models, hidden Markov models, Markov chains, naive Bayes, Bayesian networks, or factor graphs all with a consistent scikit-learn-like API. It also natively supports multithreaded (not multiprocessing!) model fitting and predictions across all platforms, a rarity among Python packages. In this talk I will showcase its speed and ease of use with examples of probabilistic modelling, kernel densities, hidden Markov models, Bayesian Networks, and general Mixture models. I will finish by showcasing its flexibility with examples of hidden Markov models of general Mixture model emissions, and using the Naive Bayes estimator to compare hidden Markov models to each other.  \n \npomegranate is a python module for probabilistic modelling focusing on both ease of use and speed, beating out competitors, including scikit-learn, in benchmarks. In this talk I will describe how to use pomegranate to simply create sophisticated hidden Markov models, Bayesian Networks, General Mixture Models (and more!) and benchmark their implementations to other python packages. ", "Pandas, Data Wrangling & Data Science\n \n\nData Wrangling & Data Science Pipeline\nPandas \u00e2\u0080\u0093 APIs & Namespaces\nPandas \u00e2\u0080\u0093 Basic Maneuvers\nHands-on : Titanic Dataset\n\n\nPandas \u00e2\u0080\u0093 Data Wrangling \u00e2\u0080\u0093 Transformations, Aggregations & Join\nHands-on : NW Dataset, State Of The Union Speeches & Debates, Recsys-2015 Data\n\n\nQ & A\n\n \nLet us explore Pandas from a Data Science perspective, mainly data exploration & feature extraction.  In the process we will also ponder Data Science pragmas. We start with Pandas fundamentals and then move on to analyzing datasets. If you want to follow along, have a working iPython, download the notebooks at https://github.com/xsankar/cautious-octo-waffle and the data. Run PreFlightCheck.ipynb.", "Data Wrangling with Python\n \nOverview\nIn this tutorial, we'll be taking an exploratory look at how to perform data wrangling using Python. It's assumed the audience has a working understanding of Python and some basic data analysis exposure. \nParticipants will leave the class with a better understanding of useful Python libraries for data analysis, as well as some hands-on scripts built throughout the tutorial. With these initial first steps, they should feel comfortable integrating data analysis into their current Python projects.\nPlease download the github repository before the tutorial.\nOutline\nIntroduction to Data Wrangling with Python\n\nHow to ask questions\nWhere to find data\nWhy Python?\n\nInitial setup\n\nIPython Notebook\nGetting the data\n\nImporting data\n\nWorking with easy formats\nWorking with hard formats\nAPIs\n\nExploring data\n\nUsing pandas\nAsking simple questions\nJoining datasets\n\nAnalysing data\n\nWhat is the data saying\nStandardization and normalization\nMaking conlusion\n\nReporting your findings\n\nWho is your audience\nCharting data\nInteractive charts and graphs\n\nNext steps\n\nWhere to go from here\nQ&A\n\"Homework\"\n\n \nWant to learn how to clean, investigate, explore and analyze your data using Python? This workshop will take you from using Python as a developer into the basics of using Python as a data wrangler. We will cover an introduction to several data science libraries including Pandas, as well as some basic charting and reporting libraries.", "Beginner Bootcamp\n \nInspired by Harry Percival's tutorials (of Obey the Testing Goat fame) at Pycon and EuroPython, this session will mainly consist of self-directed learning sessions, where we work through a series of tutorials, assisted by coaches. This will be interspersed with a few brief 10-minute talks.\nThe talks will include:\n- an intro to programming in python\n- scraping & useful public api's to access data\n- using pandas to clean, analyze and manipulate data\n- dumping data into a datastore and retrieving it\n- different tools to visualize data, both from within a Jupiter notebook and possibly as a website\nRemember to bring a laptop if you want to participate! Friendly coaches with experience in each of these areas will be sitting at the table with you in each small group, making themselves available to answer your questions, provide hints if you get stuck, and pointers to where to find more information and resources.  \n \nWhether you\u00e2\u0080\u0099re totally new to programming or you already know another language, this hands-on session will give you a crash-course in data analytics in Python, including Jupyter/ipython notebooks, pandas, various data sources, matplotlib, and other data visualization tools. Depending on how far we get, we may even look into how to share your data and results in a web based format.", "So you want to be a Python expert?\n \n.\n \n.", "Introduction to Convolutional Neural Networks using TensorFlow and Keras\n \nWe will discuss how Convolutional Neural Networks work and how you can apply them to the task of image classification. This will be illustrated by an example using real pictures of German speed limit signs which we will process with Tensorflow and Keras as a frontend. I will provide you with a Notebook so you can follow along without installation or try out the examples later at home.\n \nIntroduction to image classification using Tensorflow. I will provide a Notebook for you to try things out.", "An introduction to the tidyverse\n \nIf you\u2019ve used R you\u2019ve probably stumbled into the name ggplot2, dplyr or tidyverse. These names are R packages created by RStudio's Chief Scientist Hadley Wickham. The tidyverse is a series of packages that work extremely well with each other and have been created with the aim of easing the data analysis process. As stated by its main author, the philosophy of the tidyverse is to allow the analyst to concentrate on the substantive questions rather than on technicalities of data analysis.\nIn this workshop we will discuss the philosophy behind the tidyverse. We will discuss why these packages work well together and how they complement each other. We will learn how to input data from softwares such as Excel, Stata and SPSS and we will spend most of the workshop  using the dplyr and ggplot2 packages in a real-world example. Expect to brush up on your stats skills as we analyze trends in police killings in the United States. We will explore this data as we learn about Exploratory Data Analysis (EDA) and the tidyverse workflow. \nHope to see you there!\nContents:\n- Introduction to the tidyverse\n- Data visualization with ggplot2\n- A brief introduction to the pipe\n- Data transformation with dplyr\n- Data import with haven and readr\n- Exploratory Data Analysis (EDA)\n \nThe tidyverse is a series of packages that work extremely well with each other and have been created with the aim of easing the data analysis process. In this workshop we will discuss why these packages work well together and how they complement each other. We will spend most of the workshop getting our hands dirty by using the dplyr and ggplot2 packages in a real-world data analysis.", "Leveling up your Jupyter notebook skills\n \n\nOverview of the Jupyter project + setup to get everyone on board.\nHandling the UI, know the shortcuts\nDifferent type of cells\nExporting notebooks for presentations\nHandling different kernels\nSet styles for visualizations for professional quality\nMod the style of the web interface yourself via CSS \nProfiling code in notebooks, use Cython\nDebugging in notebooks\n\n \nMost of us regularly work with Jupyter notebooks, but fail to see obvious productivity gains involving its usage. Did you know that the web interface works like a modal editor such as VIM? Do you know that you can actually profile AND debug code in notebooks? How about setting formulas or use pre-made style settings for visualizations? Let us go through the tricks of the trade together!", "Parallel Python - Analyzing Large Data Sets\n \nThese materials are adapted and enhanced from original materials developed by Rocklin, Ragan-Kelley, and Zaitlen for SciPy 2016. \nFor the first half, we will cover basic ideas and common patterns encountered when analyzing large data sets in parallel. We start by diving into a sequence of examples that require increasingly complex tools. From the most basic parallel API: map, we will cover some general asynchronous programming with Futures, and high level APIs for large data sets, such as Spark RDDs and Dask collections, and streaming patterns. For the second half, we focus on traits of particular parallel frameworks, including strategies for picking the right tool for your job. We will finish with some common challenges in parallel analysis, such as debugging parallel code when it goes wrong, as well as deployment and setup strategies.\nPart one: We dive into common problems with a variety of tools\n\nParallel Map\nAsynchronous Futures\nHigh Level Datasets\nStreaming\n\nPart two: We analyze common traits of parallel computing systems.\n\nProcesses and Threads. The GIL, inter-worker communication, and contention\nLatency and overhead. Batching, profiling.\nCommunication mechanisms. Sockets, MPI, Disk, IPC.\nStuff that gets in the way. Serialization, Native v. JVM, Setup, Resource Managers, Sample Configurations\nDebugging async and parallel code / Historical perspective\n\nWe intend to cover the following tools: concurrent.futures, multiprocessing/threading, joblib, IPython parallel, Dask, Spark\n \nStudents will walk away with a high-level understanding of both parallel problems and how to reason about parallel computing frameworks. They will also walk away with hands-on experience using a variety of frameworks easily accessible from Python.", "Scalable Data Science with Spark and R\n \nIn this tutorial, we will focus on SparkR.  The outline of the tutorial is as follows:\n- Introduction to cluster computing with Spark\n- Getting started with SparkR\n- Deep dive into SparkR DataFrame API\n- Additional resources\nIn preparation for this tutorial please install.packages(\"SparkR\") in your system.\n \nProcessing large datasets in R have been limited by the amount of memory in the local system.  To overcome the native R limitation, several cluster computing alternatives have recently emerged including Apache Spark.  In this session, we will discuss the architecture of Spark and introduce the SparkR library.  We will work through examples of the API and discuss additional resources to learn more.", "Building a Recommendation Engine with Neo4j and Python\n \nIn this session we will show how to build a meetup.com recommendation engine using Neo4j and Python.\nOur solution will be a hybrid which makes uses of both content based and collaborative filtering to come up with multi layered recommendations that take different datasets into account e.g. we'll combine data from the meetup.com and twitter APIs.\nWe'll evolve the solution from scratch and look at the decisions we make along the way in terms of modelling and coming up with factors that might lead to better recommendations for the end user.\n \nIn this session we will show how to build a meetup.com recommendation engine using Neo4j and Python. Our solution will be a hybrid approach which makes uses of both content based and collaborative filtering using Neo4j to glue all the data together, Cypher to query the dataset and Python to do analysis and pre/post processing of data.", "Building Recommender Systems using Python\n \nThe tutorial will start with an emphasis on learning the concepts behind recommender systems. Then, we will build three variants of recommender systems in Python. More specifically, we will cover:\n\nPopularity based recommender system\nClassification model based recommender system\nNearest neighbor based collaborative filtering\nModel based collaborative filtering (Matrix Factorization)\n\nWe will also learn how to evaluate and compare different models of recommender systems using precision and recall curves. \nA song dataset will be used to recommend songs using the built popularity based and nearest neighbor based recommender systems. Finally, we will use a small toy dataset to illustrate the intuition behind the matrix factorization based recommender system.\n \nThis tutorial is about learning to build a recommender system in Python. The audience will learn the intuition behind different types of recommender systems and specifically implement three of them in python. They will get to learn how to evaluate recommender systems using precision and recall curves on a song dataset.", "Functional Programming in Python\n \nI will talk about\n\nhigher order functions\npartial function application and currying\nfunction composition\nfunctional collection transformations (and why they are relevant for PySpark)\n(fake) lazy evaluation\n\nand how it all relates to Python (and what's missing).\n \nThere should be one-- and preferably only one --obvious way to do it. And that is functional - at least in my opinion.  I'm working with Python for a living since two years after coming from a background in Ruby, Haskell, Clojure, and some more languages. Since then I have tried to marry ideomatic Python to the functional style I learned to love. It's time to share my experience.", "Pandas from the inside\n \nWe briefly set the scene with a map of the evolving Pandas landscape for large scale analytics. This is all very exciting and brings many new options. However for many users, Pandas' sweet spot remains much smaller scale. For this talk, we focus in on the DataFrame and how it actually works.\nWe will dig deeper and deeper into the components of a DataFrame, including Series and Categorical, DataFrame, Index, MultiIndex and GroupBy objects. For common operations like selecting rows, grouping, subtotals and joins, we will see what internal structures are created and how they fit together. From this, get a better understanding of the various API layers, and how to use them effectively.\nWe conclude the talk with a jump back to large scale data analysis and a quick look at how a distributed storage solution like Dask solves one of these problems. \n \n[THIS SESSION WILL BEGIN AT 11:00] Pandas is great way to quickly get started with data analysis in Python: intuitive DataFrames from R; fast numpy arrays under the hood; groupby like SQL. But this familiarity is deceptive and new Pandas users often get stuck on things they feel should be simple. This tutorial/talk takes a look inside Pandas to see how DataFrames actually work when indexing, grouping and joining tables.  ", "Introduction to data analytics with pandas\n \nIntroduction\nIn this hands-on workshop, we'll walk through the exploratory analysis of real-world data. Datasets are often messy, full of holes and inconsistencies, and a data scientist or analyst may spend a large fraction of their time cleaning and preparing data.\nFortunately, pandas makes a lot of this fairly trivial. It allows the user to import data from all sorts of different sources, and then manipulate the powerful DataFrame object. Analytics with pandas are human-friendly.\nWorkshop\nPulling in the data\nStarting with some data in CSV form, we'll look at the general properties of our dataset. What columns do we have; what kind of values are contained in them ? We'll identify problematic fields, and join two datasets to make one complete dataframe.\nCleaning\nWe've identified problems with our data, and now it's time to correct them. We'll fill in missing values, drop irrelevant rows, and fix incorrect datatypes. \nTransforming the data\nNext, we'll standardise some numerical fields where we're looking for deviations rather than absolute values, and derive some new columns based on the data we have.\nVisualisation\nThroughout, we'll be generating visualisations, to guide us in where to go next. \nPrerequisites\nYou'll need to be fairly comfortable working with Python. We won't be doing anything overly complicated, but having a grasp of Python syntax is expected.\nLaptop\nIf you want to follow along, please have a working Python setup, with pandas and matplotlib installed. Aim for a recent version of pandas. If you're unsure what to install, I recommend getting Python 3 through Anaconda : https://www.continuum.io/downloads - this distribution comes with everything you need and is very friendly.\n \nData analytics in Python benefits from the beautiful API offered by the pandas library. With it, manipulating and analysing data is fast and seamless. In this workshop, we'll take a hands-on approach to performing an exploratory analysis in pandas. We'll begin by importing some real data. Then, we'll clean it, transform it, and analyse it, finishing with some visualisations.", "Building a ChatBot with Python, NLTK and scikit\n \nWorking code samples and a  basic ChatBot framework (written in Python) will be provided and explained so that a simple Q&A bot that learns from previous experience and responds to questions with appropriate answers can be created. In this talk we will cover:\n\nBuild a basic ChatBot Framework using core Python and a SQL database.\nDemonstrate and experiment with a Learning-by-Example bot using ranking functions in Python and SQL to get some basic chat functionality working.\nIntroduce the Python NLTK to extract features from the chat sentences and words stored in the chatbot database.\nWork through a feature engineering example using NLTK and Sci-Kit and Numpy to show how we can classify sentences using Supervised Learning and estimate the accuracy of our classification model.\nApply the sentence classification ML model to our chatbot engine to target responses more accurately.\n\nPrerequisites \nAttendees will need:\n+ Anaconda for Python 3.5 or 3.6\n+ NLTK (Python Natural Language Toolkit  - pip install nltk)\n+ The Stanford Java CoreNLP Parser (https://stanfordnlp.github.io/CoreNLP/  or   wget https://nlp.stanford.edu/software/stanford-corenlp-full-2016-10-31.zip and un-zip)\n+ Java rel 8   \nTheoretically all of this could be installed on the day but it would just help to save time by preparing in advance.\nMost of what I am demonstrating will probably work against Python 2.7, but it hasn\u2019t been tested with 2.7.\n \nIntroducing the basics of Natural Language Processing using Python NLTK and Machine Learning packages to classify language in order to create a simple Q&A bot.", "High Performance Python for Data Analysis.\n \nThis tutorial covers some of the performance issue found while introducing python to a fairly large development team with roots in Java. All the examples are based on real-world performance problems that were found during the last year. Some of them are fairly trivial, but they are useful to explain why Python is slow, at least compared to other programming languages.\nIt's the same old story. One engineer designs a functionality for a client with test data, but once the real-world data arrives, the implementation is too slow. The code has to be refactored, but in the most isolated way possible. In addition, the client may have some constraints like not being able to compile extensions. The engineer calls someone more experienced in Python and the research begins. This workshop will be useful if you want to know some shortcuts.\nEven Pandas, that has a fairly good performance, can be seriously improved with some profiling, advanced Numpy tricks, Numexpr, the numba JIT compiler, or Cython extensions. In many of the examples the speedup is of two orders of magnitude.\nThe syllabus will be the following:\n\nWhy (stock) Python is slow. Deal with it.\nWhy Numpy is fast, if you use it properly. Numexpr may help sometimes.\nWhy Cython and Numba are fast, and how to keep them fast.\nWhy dates are hard and slow, but there are ways to deal with them.\nWhy O(2) computations in Pandas are slow, but if you are careful with allocations, they're not a great deal.\n\n \nWriting fast and memory efficient code in Python requires some experience, and some best practices are counter-intuitive if you come from static languages like C++ and Java. This live coding session covers the reasons why Python may be slow and how to write efficient and fast Python applications, with special emphasis in profiling, efficient use of Numpy, and extensions with Numba and Cython.", "Introductory tutorial on data exploration and statistical models\n \nI would be happy to conduct an introductory level tutorial on exploring a dataset with the pandas/StatsModels/scikit-learn framework:\n\nDescriptive statistics. Here we will describe each variable depending on its type, as well as the dataset overall.\nVisualization for categorical and quantitative variables. We will learn effective visualization techniques for each type of variable in the dataset.\nStatistical modeling for quantitative and categorical, explanatory and response variables: chi-square tests of independence, linear regression and logistic regression. We will learn to test hypotheses, and to interpret our models, their strengths, and their limitations.\nI will then expand to the application of machine learning techniques, including decision trees, random forests, lasso regression, and clustering. Here we will explore the advantages and disadvantages of each of these techniques, as well as apply them to the dataset.\n\nThis would be a very applied, introductory tutorial, to the statistical exploration of a dataset and the building of statistical models from it. I would be happy to send you the ipython notebook for this tutorial as well.\n \nThis tutorial will focus on analyzing a dataset and building statistical models from it. We will describe and visualize the data. We will then build and analyze statistical models, including linear and logistic regression, as well as chi-square tests of independence. We will then apply 4 machine learning techniques to the dataset: decision trees, random forests, lasso regression, and clustering.", "Beyond Sentiment -- Emotion Mining with Python and machine learning\n \nPsychology recognizes 6 basic emotions -- joy, sadness, anger, disgust, fear and surprise. Sentiment analysis recognizes only two, joy and sadness -- and is notoriously inaccurate. In this talk, I will introduce students to using machine learning for emotion analysis of texts. We will train an \"Inside Out\" multidimensional classifier, and learn how to make a nuanced predictions of emotional content of movie reviews, touching on such stalwarts as \"bittersweet melodrama\", \"disgusting comedy\", and the potent mix of anger, fear and surprise in action movies. \nWe'll add a couple more dimensions to the classifier, judging emotional distance (immediacy), certainty, and other variables that constitute the latest findings in cognitive science of emotions. \nWe will use Python with SCLearn module, and TensorFlow to build the classifiers. \n \nLearn how to extract emotional content from textual data - and how to build a sentiment analysis tool that does not suck.\nTypical sentiment analysis tries to map the entire rich and varied world of human emotions into \"good\" vs \"bad\". In this tutorial, we use the characters of \"Inside Out\" and machine learning to build a nuanced model of human emotions -- and put it in production!", "Introduction to Pandas\n \nThe purpose of the tutorial is to expose users to Pandas for data analysis\nand move users into a more reproducible analysis workflow compared to spreadsheet programs.\nThe tutorial will begin with loading in tabular data and various ways to view columns and rows of data.\nThe initial part of the tutorial will show users how to load in data and quickly create descriptive statistical plots.\nNext, we will cover more of pandas internal data structures.\nThis will cover some fundamental knowledge about Python as a programming language, mainly object methods.\nFinally, before showing users basic data cleaning examples, we will cover data visualizations using matplotlib, seaborn, and pandas itself.\nThe next section of the tutorial will show learners how to assemble and merge multiple datasets,\nand how to work with missing values.\nLastly, before we get to fitting models, I will go over how to recode variables for analysis.\nThe main purpose is to show users how to use Pandas, and not how to fit machine learning models.\nHowever, I show a simple model at the end so users see how data cleaning and analysis all fit together in a workflow.\n \nThis tutorial aims to expose future data analysis with the Python Pandas library.\nThe tutorial is meant for absolute beginners to Pandas and Python as a programming language.\nWe will begin with loading tabular data and various ways to calculate summary statistics and visualize data.\nNext, we will learn various ways to join multiple datasets, and how we can work with missing values etc.", "Luigi & Data Pipelines\n \nData pipelines are hard. Too often we resort to retrofitting janky scripts, relying on keeping a readme up to data, etc. \nFirst, this proposal lays out the variety of tools that are available to build data pipelines. This talk will discuss why you should be using Luigi and how to use it in a variety of common use cases. \nNext, we will build a basic exploratory analysis using Chicago open data and Luigi to demonstrate the power of this concept and how it works with Jupyter. \nFinally, we'll retrofit a larger, more complex project to use Luigi to show how you can use it in bigger organizations. \n \nYou need a data pipeline. This talk will discuss the lifecycle of projects using Jupyter notebooks & Luigi as a data pipeline management tool for a variety of projects, from greenfield to retrofitting complex systems. It will included a hands on demo. ", "Build Data Apps by Deploying ML Models as API Services\n \nDeploy your ML Models as a Service\nIn this talk, we will learn one way to take our Machine Learning models and make them available as a Prediction Service.  We will work through the following steps.\n\nCreate a Simple Machine learning Model using Scikit-Learn / Pandas\nPickle the model\nUsing Tornado Web App, Make this model available as an API Service\nBuild an Web App that uses this deployed Model\nAdd Authentication to our Prediction API\nOptionally, add Redis to Cache Prediction Results\nDeploy the model in the Cloud (AWS)\n\nPlease have Anaconda or Miniconda installed on your local machine.  I will mostly be using Python 3.5, but Python 2.7 should be fine as well.\n \nAs data scientists, we love building models using IPython Notebooks / Scikit-Learn / Pandas eco-system.  But integrating these models with an web app can be a challenge.  In this tutorial, we will take our machine learn ing models and make them available as APIs for use by Web and Mobile Apps.  We will also build a simple webapp that uses our prediction service.", "(CANCELLED) The Simple Leads To The Spectacular\n \nIntroduction\nSometimes building a new system can be an even more daunting task than it sounds if you have to comply to the regulations of a highly regulated industry like Telecommunications, which also comes with huge volumes of data in high velocity.\nWe needed to move beyond the Proof-Of-Concept where the company's analytics suite was before and move to a architecture which could be scaled, be highly configurable to comply with all the legal and privacy requirements in every country, allow the data scientists to perform their analysis in the data, enable the development of novel data products and do it with a team that lacked experience in big data solutions.\nWhy Python end-to-end?\nPython proved to be the solution that through its simplicity allowed to build a system that matched the requirements, ease its deployment and be the 'linqua franca' for the company's engineers.\nWe believe that Python can help many companies to build successful products and many engineers and data scientists to enjoy doing it and we want to share our experience on doing it.\n \nSometimes the business and technical requirements make the design of a new system nearly impossible or excessively complex.  Python's elegance and simplicity helped to build a data pipeline that processes Terabytes per day while becoming the 'lingua franca'  for every technical person in the company.", "Building a Pong playing AI in just 1 hour(plus 4 days training...)\n \nWe will start by setting up a Python an agent in Pong that moves completely randomly.\nI will then talk over what Q-learning is and how it works and a bit about Convolutional nets. \nWe will then build the actual agent using Tensorflow.\nFull code will be given, including links for downloading the resources required, so hopefully the audience can build this on their laptops. I would hope to give enough information that users will be able to make there own agents independently once finished.\n \nWe will build an AI that can master the game of Pong in just 1 hour.  In the course of this we will talk through some of the tools involved. Q-learning , Deep learning and Convolutional  nets and how they fit into Pong. Most of the heavy lifting will be done using Google's recently released Tensorflow libraries.", "Parallelizing Scientific Python with Dask\n \nDask is a flexible tool for parallelizing Python code on a single machine or across a cluster. \nWe can think of dask at a high and a low level\n\n\nHigh level collections: Dask provides high-level Array, Bag, and DataFrame collections that mimic and build upon NumPy arrays, Python lists, and Pandas DataFrames, but that can operate in parallel on datasets that do not fit into main memory. \n\n\nLow Level schedulers: Dask provides dynamic task schedulers that execute task graphs in parallel. These execution engines power the high-level collections mentioned above but can also power custom, user-defined workloads to expose latent parallelism in procedural code. These schedulers are low-latency and run computations with a small memory footprint. \n\n\nDifferent users operate at different levels but it is useful to understand both. This tutorial will cover both the high-level use of dask.array and dask.dataframe and the low-level use of dask graphs and schedulers. Attendees will come away\n\nAble to use dask.delayed to parallelize existing code\nUnderstanding the differences between the dask schedulers, and when to use one over another\nWith a firm understanding of the different dask collections (dask.array and dask.dataframe) and how and when to use them.\n\n \nDask is a flexible tool for parallelizing Python code on a single machine or across a cluster. It builds upon familiar tools in the PyData ecosystem (e.g. NumPy and Pandas) while allowing them to scale across multiple cores or machines. This tutorial will cover both the high-level use of dask collections, as well as the low-level use of dask graphs and schedulers.", "Jupyter Notebooks for geospatial data analysis\n \nJupyter Notebooks benefit geospatial data analysis in multiple ways since geo data access, manipulation and (interactive) visualisation can be combined in one workflow and programming environment.\nThis tutorial session presents practical examples of workflows with Jupyter Notebooks for Earth Observation and Climate data analysis. An introduction to Python\u2019s owslib package is given, which will showcase how standardised web services provide a time and cost-efficient way to access and process large volumes of environmental data. Hands-on examples will show how terabytes of open environmental data can be accessed, manipulated and visualized within one workflow without requiring data download. Different Python packages for data visualisation and geospatial data analysis will be harnessed.  \nThe first part of the tutorial is a walk-through session, where participants are able to run the example workflows alongside. In a second part, the participants will get challenged to setup their own geospatial workflow, from data access and manipulation up to data visualization, based on the tools and data services that were presented during the walk-through session\nSession participants require their own laptop with Jupyter installed. We are investigating the feasibility of setting up mybinder or JupyterHub for this workshop. Notebooks will be provided.\n \nLearn how Jupyter Notebooks are a highly beneficial tool for modern geospatial data analysis and for creating reproducible workflows, both for web-based access of large geospatial data and its effective manipulation and geospatial data visualisation with bokeh or jupyter widgets.", "Social Network Analysis with Python and NetworkX\n \nThis workshop will provide a hands on introduction to Social Network Analysis (SNA) without assuming any specific knowledge of graph theory or network analysis. We'll start with a brief introduction to the subject by highlighting the mathematical foundations of SNA. We'll then review the main data structures provided by NetworkX and learn how to use them to represent real world social networks. We'll also briefly cover how to plot simple networks using Matplotlib. We'll then focus on different levels and kinds of network analysis, and how to perform them using NetworkX. Finally, if time permits, we'll briefly review some network models both for the structure and for the dynamics of real world networks. \nYou can find a Jupyter notebook with all the materials for the workshop at: https://github.com/jtorrents/pydata_bcn_NetworkX. There are two notebooks in that repository, one has all solutions for the exercises and all the outputs for each cell. Please do not look at it if you plan to attend the workshop.    \nFor this workshop attendees will need to install NetworkX (>=1.11), Matplotlib (>=1.5), numpy (>=1.10) and have a working Jupyter Notebook environment. Some examples will also use Pandas (>=0.17) and Seaborn (>=0.7), but these packages are not essential. Only basic Python knowledge is assumed.\nOutline of the workshop:\n0. Brief Introduction to Graph Theory\n\nMathematical foundation of Social Network Analysis.\nWhy graphical representations usually doesn't help much.    \n\n1. Creating and Manipulating Graphs\n\nData Structures: Graphs, DiGraphs, MultiGraphs and MultiDiGraphs.\nAdding nodes and edges.\nAdding and updating node and edge attributes.\nGraph generators.\nVisualizing graphs using Matplotlib.\nCommon formats for reading and writing Graphs. \n\n2. Network Analysis\n\nBasic concepts: Degree.\nDistance measures: paths, simple paths, and shortest paths.\nNode centrality analysis: measures and their relation.\nAnalyzing groups and subgroups: Cliques, k-cores, components, and k-components.\n\n3. Bipartite Networks\n\nDefinition of bipartite networks and their use in modeling group affiliations.\nWorking with bipartite networks in NetworkX.\n\nhttps://github.com/jtorrents/pydata_bcn_NetworkX\n \nSocial Network Analysis (SNA) has a wide applicability in many scientific fields and industries. This workshop is a gentle introduction to SNA using Python and NetworkX, a powerful and mature python library for the study of the structure, dynamics, and functions of complex networks. Participants in this workshop should have a basic understanding of Python, no previous knowledge of SNA is assumed.", "Advanced Metaphors in Coding with Python\n \nTBD\n \nTBD", "Educational framework for  Black Box optimization methods design with scikitlearn and scipy\n \nMany pressing real world problems can be stated as problems of global optimization, where target function is given by values only (a black box function).  They range from robot parameter tuning and optimization of chemical processes to biological systems analysis.  Typical examples from financial engineering include model calibration, pricing, hedging, VaR/CVaR computation, credit rating assignment, forecasting, strategy optimization in electronic trading systems. The choice of the algorithm for decision making should depend on the type of the objective function and availability of a priori information. Such problems are best approached with a library of optimization methods to help study the nature of the problem. Several such libraries are readily available, such as Matlab Global Optimization Toolbox, that contains a minimal set of modules allowing to create a universe of optimization methods by making hybrids. We suggest to use use Python  scipy and scikit learn.  By varying parameters of the standard optimization procedures and making hybrids it is possible to obtain the whole universe of methods. For example, Nelder Mead algorithm (represented by scipy.optimize.fmin() function) needs a frequent restart  and can be efficiently combined with random search for solving problems of global optimization. Many well known standard tools such as principal component analysis  can be rewritten in a incremental (or online) form using machine learning techniques, where new data is incorporated at each iteration, making such algorithms suitable for big data handling.\n \nMany pressing real world problems can be stated as problems of global optimization, where target function is a  black box.  Such problems are best approached with a library of optimization methods to help study the nature of the problem. We show how to use Scipy.optimize and Scikit-learn modules to create global optimization methods with desired properties. ", "ITK in Biomedical Research and Commercial Applications\n \nThe Insight Segmentation and Registration Toolkit (www.itk.org) has become a standard in academia and industry for medical image analysis. In recent years, the ITK developers' community has focused on providing programming interfaces to ITK from Python and Javascript and making ITK available via leading applications such as Slicer and ImageJ. In this course we present best practices for taking advantage of ITK in your imaging research and commercial products. We demonstrate how script writing and interactive GUIs can be used to access the algorithms in ITK and the multitude of ITK extensions that are freely available on the web.\n \nThe Insight Segmentation and Registration Toolkit (www.itk.org) has become a standard in academia and industry for scientific, N-dimensional image analysis. In this course we present best practices for taking advantage of ITK in your imaging research and commercial products. ", "A Quickstart Guide to PyMC3\n \nThis tutorial will walk through some basic models to show you how easy it can be to use PyMC3. The models will be written so that it should be easy to extend them to real datasets.\nI\u00e2\u0080\u0099ll start with basic linear regression and logistic regression. I will then demonstrate using PyMC3 on hierarchical models.\nAlong the way, I\u00e2\u0080\u0099ll touch on the different samplers as well as a variational inference algorithm called ADVI that has recently been added to the library.\nI will also share some of the bugs I ran into and the solutions I found.\n \nPyMC3 is a powerful relatively new library for probabilistic models. The developers have given multiple talks describing probabilistic models, Bayesian statistics, and the features of the library. This tutorial aims to complement these talks by providing a practical guide to using PyMC3 with step-by-step implementations of some basic models and some issues you might encounter.", "Python Visualization for Exploration of Data\n \nVisualization of complex real-world datasets presents a number of challenges to data scientists. By developing skills in data visualization, data scientist can confidently explore and understand the relationships in complex data sets while undertaking analyses. Using the Python matplotlib, pandas plotting and seaborn packages attendees will learn how to do the following.\n\nExplore complex data sets with visualization, to develop understanding of the inherent relationships.\nCreate multiple views of data to highlight different aspects of the inherent relationships, with different graph types.\nUse plot aesthetics to project multiple dimensions. \nApply conditioning or faceting methods to project multiple dimensions\n\n \nVisualization is an essential method in any data scientist\u00e2\u0080\u0099s toolbox. Visualization is a key first step in the exploration of most data sets. Visualization is also a powerful tool for presentation of results and for determining sources of problems with analytics. This tutorial introduces attendees to the most commonly used Python visualization packages, matplotlib, pandas plotting and seaborn. ", "Setting up predictive analytics services with Palladium\n \nIn this talk, we will introduce Palladium, an open source framework for easily setting up predictive analytics services (https://github.com/ottogroup/palladium). It supports tasks like fitting, evaluating, storing, distributing, and updating (predictive) models. Core machine learning processes are compatible with the open source machine learning library scikit-learn and thus, a large number of scikit-learn\u00e2\u0080\u0099s features can be used with Palladium. Although being implemented in Python, Palladium provides support for other languages and is shipped with examples how to integrate and expose R and Julia models. For an efficient deployment of services based on Palladium, a script to create Docker images automatically is provided. This talk will cover the use of Palladium including an example where a simple classification service is set up. We will also show how Docker and Mesos / Marathon can be used to deploy and scale Palladium-based services. Having basic knowledge about Machine Learning and/or scikit-learn would be an advantage when attending this talk.\n \nWe will introduce Palladium, an open source framework for setting up predictive analytics services. It supports tasks like fitting, evaluating, storing, and distributing (predictive) models. Core ML processes are compatible with scikit-learn and a large number of scikit-learn\u00e2\u0080\u0099s features can be used. Besides the use of Palladium we will also show how to use it with Docker and Mesos / Marathon.", "Python and Julia: the best of friends?\n \nPython and Julia are often thought of as in competition for the same workspace. In this talk Malcolm Sherrington argues quite the reverse and in fact there is a tight connection between the two, one from which each benefits. It will focus on the cooperation between both programming languages and discuss what each has to learn from the other.\nThe talk will demonstrate their mutual interoperability by showing how Python and Julia can call each other seemlessly and with no overhead, concentrating on the areas in which each excels and contends that it is not all about speed!!!\n \nThe talk will focus on both the Python and Julia languages, concentrating on cooperation between the two rather than confrontation and bringing out what each can learn from the other.  There will be a demonstration of interoperability via  a Jupyter notebook, but no detailed knowledge of Julia (or Python) is necessary.  ", "pomegranate: fast and flexible probabilistic modeling in python\n \nIn this talk I will give an full tutorial for the python package pomegranate, which is a flexible probabilistic modeling package implemented in cython for speed. I will highlight several models it supports, specifically probability distributions, mixture models, naive Bayes, Markov chains, hidden Markov models, and Bayesian networks. At each step I will show that these models are both faster and more flexible than other implementations in the open source community along with code examples. In addition, I will show how to utilize the underlying modularity of the code to stack these models to produce more complicated ones such as mixtures of Bayesian networks, or HMMs with complicated mixture emissions. Lastly, I will show how easy it is to use the built-in out-of-core and parallel APIs to allow for multithreaded training of complex models on massive amounts of data which can't fit in data-- all without the user having to think about any implementation details. An accompany Jupyter notebook will allow users to follow along, see code examples for all figures presented, and make modifications.\n \nI will describe the python package pomegranate, which implements flexible probabilistic modeling in cython. I will highlight several supported models including mixtures, hidden Markov models, and Bayesian networks. At each step I will show that these models are both faster and more flexible than other implementations. In addition, I will describe the built-in out-of-core and parallel APIs.\nLink to slides: https://noble.gs.washington.edu/~maxwl/2017-07-05%20pydata%20pomegranate.pdf", "Twisted up in a Distributed Tornado - a beginners guide to async frameworks in python\n \nAt some point in a career in data science, software development or data engineering you will be looking to develop a piece of code that runs alongside another piece of code. For example, you might want to:\n\nUse all of your processors when running a script\nConsume social media data as it is created\nCreate a super simple task scheduler without an ugly while True loop\nUnderstand how Jupyter notebooks work\nWrite to a database and move on to the next task without waiting for confirmation\nCall a function and throw an exception if it takes too long\n\nThere are numerous ways of achieving such concurrency. If creating a project to run on one or a few machines which needs good exception handling then I find tools like Twisted, Tornado and Dask/Distributed ideal for the above tasks. \nAs a python developer I have always tended to work with Celery and felt intimidated by the different flow control when using async tools like Twisted and Tornado. \nIn this tutorial I will walk through the examples above and talk about how I went from Async novice to Dask/Distributed contributor in 6 months. The tutorial will concentrate mainly on practical use cases. Along the way we will encounter some bumps in the road which will hopefully aid attendees' understanding of the basic dos and don'ts around these tools.\nBy coming to this tutorial you will learn:\n\nWhat event-driven Python Frameworks are and why they are useful\nHow to overcome any fears you may have about event-driven Python programming\nEnough examples to be able to consider projects like Dask/Distributed and Tornado for your next project\n\n \nBy coming to this tutorial you will learn:\n\nWhat event-driven Python Frameworks are and why they are useful\nHow to overcome any fears you may have about event-driven Python programming\nEnough examples to be able to consider projects like Dask/Distributed and Tornado for your next project\n", "HDF5 and pandas\n \nDescription\nHDF5 is a hierarchical, binary database format that has become the de facto standard for scientific computing. While the specification may be used in a relatively simple way (persistence of static arrays) it also supports several high-level features that prove invaluable. These include chunking, ragged data, extensible data, parallel I/O, compression, complex selection, and more. Moreover, HDF5 bindings exist for almost every language - including two Python libraries (PyTables and h5py). This tutorial will cover HDF5 itself through the lens of both h5py and PyTables and will show how to use them in order to persist both NumPy and pandas containers.\nThis tutorial will discuss tools, strategies, and hacks for really squeezing every ounce of performance out of HDF5 in new or existing projects. It will also go over fundamental limitations in the specification and provide creative and subtle strategies for getting around them. We will also see how pandas can use HDF5 via its HDFStore module.  Overall, this tutorial will show how HDF5 plays nicely with all parts of an application making the code and data both faster and smaller. \nKnowledge of Python, NumPy, pandas, and basic HDF5 is recommended but not required.\nOutline\n\nIntro and setup (10 min)\nBasic datatypes (10 min)\nHomogeneous types (Arrays)\nCompound types (Tables)\n\n\nChunking (10 min)\nHow it works\nHow to properly select your chunksize\n\n\nMeaning in layout  (10 min)\nTips for choosing your hierarchy\n\n\nWhy you should always use compression (20 min)\nCompression algorithms (aka codecs) available\nChoosing the most adequate codec\nExercise\n\n\nQueries and Selections (30 min)\nPyTables.where()\nIndexed queries\nExercise\n\n\nIntegration with pandas (HDFStore) (20 min)\nStoring/loading dataframes\nQuerying a serialised dataframe\nExercise\n\n\n\n \nHDF5 is a hierarchical, binary database format that is extremely popular and includes features like chunking, ragged data, extensible data, parallel I/O, compression, complex selection, among others.  On its part, pandas is also the de facto standard for providing high-performance, easy-to-use data structures and data analysis tools in Python.  Together, HDF5 and pandas can be a winner couple.", "Introduction to Machine Learning with H2O and Python\n \nAbout H2O.ai\nH2O.ai is focused on bringing AI to businesses through software. Its flagship product is H2O, the leading open source platform that makes it easy for financial services, insurance and healthcare companies to deploy machine learning and predictive analytics to solve complex problems. More than 8,500+ organizations and 75,000+ data scientists depend on H2O for critical applications like predictive maintenance and operational intelligence. The company accelerates business transformation for 107 Fortune 500 enterprises, 8 of the world\u2019s 12 largest banks, 7 of the 10 largest auto insurance companies and all 5 major telecommunications providers. Notable customers include Capital One, Progressive Insurance, Transamerica, Comcast, Nielsen Catalina Solutions, Macy's, Walgreens, Kaiser Permanente, and Aetna.\nThis tutorial aims to demonstrate the basic usage of H2O with worked examples in Python. Code and data for the worked examples will be provided.\nLearning Objectives\nBy the end of the tutorial, participants will be able to:\n\nStart and connect to a local H2O cluster from Python.\nStart and connect to H2O cluster(s) on the cloud (e.g. AWS) (i.e. straight-forward distributed machine learning)\nImport data from Python data frames, local files or web.\nPerform basic data transformation and exploration.\nTrain classification and regression models using H2O machine learning algorithms.\nEvaluate model performance and make predictions.\n\nAgenda\n\nAbout H2O.ai\nH2O machine learning platform & algorithms\nH2O + Python API\nBasic Extract, Transform and Load (ETL) procedures\nWorked examples: classification and regression\n\n \nH2O.ai is focused on bringing AI to businesses through software. Its flagship product is H2O, the leading open source platform that makes it easy for financial services, insurance and healthcare companies to deploy machine learning and predictive analytics to solve complex problems.\nThis tutorial aims to demonstrate the basic usage of H2O with worked examples in Python.", "Modern NLP in Python\n \nAcademic and industry research in Natural Language Processing (NLP) has progressed at an accelerating pace over the last several years. Members of the Python data science community have been hard at work moving cutting-edge research out of papers and into open source, \"batteries included\" software libraries that can be applied to practical problems.\nIn this tutorial and live demo, we'll explore some of these tools for modern NLP in Python, including spaCy and gensim. Along the way, we'll learn about recent foundational advances in machine natural language representations, such as topic modeling with Latent Dirichlet Allocation (LDA) and word vector embedding with word2vec. Finally, we'll discover visualization tools to help us introspect and understand high-dimensionality natural language models, including pyLDAvis and t-SNE.\n \nAcademic and industry research in Natural Language Processing (NLP) has progressed at an accelerating pace over the last several years. Members of the Python community have been hard at work moving cutting-edge research out of papers and into open source, \"batteries included\" software libraries that can be applied to practical problems. We'll explore some of these tools for modern NLP in Python.", "Deep Language Modeling for Question Answering using Keras\n \nQuestion answering has received more focus as large search engines have basically mastered general information retrieval and are starting to cover more edge cases. Question answering happens to be one of those edge cases, because it could involve a lot of syntatic nuance that doesn\u00e2\u0080\u0099t get captured by standard information retrieval models, like BM-25 or LSI. Hypothetically, deep learning models are better suited to this type of task because of their ability to capture higher-order syntax. Two papers, \u00e2\u0080\u009cApplying deep learning to answer selection: a study and an open task\u00e2\u0080\u009d (Feng et. al. 2015) and \u00e2\u0080\u009cLSTM-based deep learning models for non-factoid answer selection\u00e2\u0080\u009d (Tan et. al. 2016), are recent examples which have applied deep learning to question-answering tasks with good results.\nFeng et. al. used an in-house Java framework for their work, and Tan et. al. built their model entirely from Theano. This tutorial will demonstrate how to replicate the models used by each group using the popular open-source framework Keras, adding custom functions to include recent advances from the neural networks community.\n \nNeural network models have revolutionized many areas of data analysis, but have yet to make their way into mainstream usage in a number of popular fields. Recent advances in question-answering have come largely from creative applications of deep learning. In this tutorial, I will demonstrate how to modify the open-source framework Keras to build some of these models.", "Intro to Julia\n \n\n \nThe Julia language has been increasingly used in the research community specially in areas related to data science. This tutorial aims at introducing attendees to Julia in the context of data science. This includes familiarizing the audience with the basics of Julia and illustrating the major differences with Python; also, going over writing Julia wrappers for other programming languages including", "Overview of the NumFOCUS Data Science Stack\n \nOverview of the sponsored tools in NumFOCUS' Data Science Stack\n \nOverview of the sponsored tools in NumFOCUS' Data Science Stack", "One in a billion: finding matching images in very large corpora\n \nimage-match started as an internal project.  We needed a way, given some target image, to find similar images downloaded by our web-crawler (think Tineye).\nSo not only did we need to support fast, accurate lookup for millions or even billions of images, we also needed to facilitate very high volume insertion -- around 10k images per second.\nIn my talk, I will cover:\n\nThe Problem: why is finding similar images hard?\nAlgorithm: based on this paper\nPerformance: but does it scale?\nAlternatives\n\n \nThe goal was not only to support high write volumes ( >10k/s) but also to support fast lookup of similar images (around 1-2s for over 1B images).  Though similar paid services and free image hashing libraries exist, this may be the first complete free open-source solution.  Available at: https://github.com/ascribe/image-match", "Open Work Space\n \n\n \n", "Effective Visual Studio\n \nAccording to Stack Overflow's developer survey for 2017, the majority of data scientists are working in Visual Studio, and with the latest release of Visual Studio 2017 including support for Python and R out of the box, there is a good reason for this. In this tutorial, we will walk through setting up Visual Studio 2017, its special features for Python and R developers, and the features it provides for all languages that will make you a more efficient, effective, and happier data scientist or engineer.\nBring your Windows laptop or virtual machine, and to prepare ahead of time, install Visual Studio 2017 Community (or Professional/Enterprise if you have access) from visualstudio.com, and select the \"Data Science and Analytical Applications\" workload.\n \nThis tutorial will walk through Visual Studio 2017, including how to set it up for use with Python and R, and the features it provides that will make you a more efficient, effective, and happier data scientist or engineer.", "One workshop that data scientists don't want you to attend...\n \nBy the end of the workshop you'll have your very own python app for streaming real-time news and detecting click bait. In the workshop we'll cover:\n - Streaming data from a REST API\n - Preprocessing textual data\n - Training a simple machine learning classifier for clickbait\n - Putting everything together in a scikit-learn pipeline\n - Analysing our results (which news source is the most clickbaity?)\n \nWith this one weird trick you can build a text processing pipeline!\nWe've all fallen for clickbait articles online. They pollute our news feeds and make it harder to filter out valuable information. In this workshop we'll stream news articles in real-time and detect clickbait using simple machine learning techniques. You won't believe what happened next...", "Convolutional neural networks for audio processing: starting pack\n \nNeural networks are increasingly popular in audio signal processing for topics as speech recognition or denoising. Scientific papers are usually accompanied by code repositories which rely on libraries as Theano or Tensorflow that can be interfaced from python. However, adapting a system to different tasks and data must take into account a set of pre-training routines and parameter debugging which we will discuss in this tutorial. Starting from the audio signals we introduce the data pre-training steps (feature computation, batch generation, normalization(  with examples in numpy or scipy. We summarize the core concepts in neural networks and we code an architecture with the Keras library. Finally, we learn how to visualize and debug parameters with TensorBoard.\nThe repository for this workshop can be found at the github page\n \nIn this tutorial we will present our experience with adapting neural network frameworks for audio processing tasks. Specifically we focus on pre-training routines such as data processing and on post-training parameter visualization and debugging.", "The path between developing and serving machine learning models.\n \nWhenever you have a machine learning module in your pipeline, persisting and serving the model is not yet a trivial task. This tutorial shows how an open source framework using several open source technologies could potentially solve the problem.\nMy journey started with this[1] question on StackOverflow. I wanted to be able to do my usual data science stuff, mostly in python, and then deploy them somewhere serving like a REST API, responding to requests in real-time, using the output of the trained models. My original line of thought was this workflow:\n\ntrain the model in python or pyspark or in scala in apache spark.\nget the model, put it in an apache flink stream and serve.\n\nThis was the point at which I had been reading and watching tutorials and attending meetups related to these technologies. I was looking for a solution which is better than:\n\ntrain models in python\nwrite a web-service using flask, put it behind a apache2 server, and put a bunch of them behind a load balancer.\n\nThis just sounded wrong, or at its best, not scalable. After a bit of research, I came across PipelineIO[2,3] which seems to promise exactly what I'm looking for. In this tutorial we use PipelineIO, to deply a cluster on the cloud, which gives us a JupyterHub to develop our method, and uses PMML to persist and deploy and serve the model. My own jurney and take from PipelineIO are documented github[4]. I'll use Amazon AWS, but PipelineIO uses Kubernetes and you can easily deploy in any environment in which you can use Kubernetes.\nIf you work in an environment in which you have different machine learning modules, which should be used in production in real time and as a part of a stream processing pipeline, this talk is for you.\n[1] https://stackoverflow.com/questions/42719953/how-to-develop-a-rest-api-using-an-ml-model-trained-on-apache-spark\n[2] https://pipeline.io\n[3] https://github.com/fluxcapacitor/pipeline\n[4] https://github.com/adrinjalali/pipeline-docs\n \nAs a data scientist, one of the challenges after you develop and train your model, is to deploy it in production where other systems would use the output of the model in real time. In this tutorial we use PipelineIO, to deploy a cluster on the cloud, which gives us a JupyterHub to develop our method, and uses PMML to persist and deploy and serve the model.", "Building Your First Data Pipelines\n \nData pipelines are hard. Too often we resort to retrofitting janky scripts, relying on keeping a readme up to data, etc.\nFirst, this proposal lays out the variety of tools that are available to build data pipelines. This talk will discuss why you should be using Luigi and how to use it in a variety of common use cases.\nNext, we will build a basic exploratory analysis using DC open data and Luigi to demonstrate the power of this concept and how it works with Jupyter.\nFinally, we'll retrofit a larger, more complex project to use Luigi to show how you can use it in bigger organizations.\n \nYou need a data pipeline. This talk will discuss the lifecycle of projects using Jupyter notebooks & Luigi as a data pipeline management tool for a variety of projects, from greenfield to retrofitting complex systems. It will included a hands on demo.  ", "Turning Jupyter Notebooks into Data Applications\n \nWe will start by finding and analyzing relevant data in a Jupyter notebook. We will drag-and-drop our notebook cells into a grid layout, and publish it as a standalone web app. We will add interactivity to our notebook and redeploy our app as we iterate.\nSee https://bit.ly/29QutMB for a complete overview and video of the finished application.\nAttendees should bring a laptop with a modern web browser preinstalled. We will provide the software and hosting necessary to complete the tutorial. All of the tutorial material will be made available on GitHub for review after the session. We will assume attendees are familiar with Jupyter Notebook and common libraries in the Python scientific ecosystem (e.g., pandas).\n \nTogether, we will use Jupyter notebooks and the new Jupyter dashboard and declarative widget extensions to create an interactive application. By doing so, we hope to demonstrate the value of the Jupyter ecosystem for rapidly building just-good-enough solutions.", "Popping Kernels: An Exploration of Kernel Development for Jupyter Notebooks\n \nJupyter Notebooks are web applications that allow users to create and share documents that contain live code, equations, visualizations and explanatory text. The live code portion of the notebook is particularly interesting, because the Notebooks give users the ability to execute live code in a variety of languages. When a user starts a Notebook, they connect to a kernel in the background that is responsible for executing the code and responding appropriately to the front-end. Kernels communicate to the front-end using the Jupyter Messaging Protocol and can be developed in their native language.\nThis talk will give individuals with no kernel experience and some Python experience, a brief introduction to the concepts they need to understand in order to develop kernels. This talk will also be useful to individuals who are looking for fun projects that will allow them to strengthen their skills in a particular programming language. Finally, for Jupyter Notebook users, this talk will demystify the process that powers the interactivity of the tool that they use.\nThis 45 minute talk will be (approximately) divided into the following segments.\n\u00e2\u0080\u00a2 Introduction (10 to 12 minutes) - An brief demo of a Jupyter Notebook connected to a Python kernel\n    \u00e2\u0080\u00a2 An explanation of the rational for building kernels for the Notebook\n    \u00e2\u0080\u00a2 An overview of existing kernels for the Jupyter Notebook\n\u00e2\u0080\u00a2 Introduction to the Jupyter Messaging Protocol (25 - 30 minutes)\n    \u00e2\u0080\u00a2 An introduction to the ZeroMQ library and its importance in the development of kernels\n    \u00e2\u0080\u00a2 An overview of the decoupling of the kernel backend from the (potentially multiple backends)\n    \u00e2\u0080\u00a2 An overview of the channels (shell, iopub, stdin, and control) used when communicating with the kernel and the messaging spec associated with each channel\n\u00e2\u0080\u00a2 Conclusion (3 minutes)\n\n    \u00e2\u0080\u00a2 Presentation of additional resources for exploration\n    \u00e2\u0080\u00a2 Overview of main takeaways from talk\n\n \nThis talk will give individuals with no kernel experience and some Python experience, a brief introduction to the concepts they need to understand in order to develop kernels. This talk will also be useful to individuals who are looking for fun projects that will allow them to strengthen their skills in a particular programming language.", "Data Applications with Bokeh\n \nComing Soon\n \nComing Soon", "Machine Learning at Scale\n \n1) Intro\na. Why is scikit-learn not enough?\nb. What is Spark?\nc. What is MLlib?\n2) Spark\na. Overview of Spark\nb. Overview of PySpark\nc. PySpark code sample\n3) MLlib\na. Overview of MLlib\nb. MLlib code samples\n \nPython machine learning libraries like scikit-learn are a fantastic resource but not always well suited to large datasets. How can we use Python for machine learning in such cases? This talk will introduce PySpark and MLlib as tools for distributed machine learning. We will discuss what these tools are, how they work, and cover some basic code examples of machine learning on a cluster.", "Beginners' Bootcamp (continued)\n \nSee Beginners' Bootcamp.\n \nSee Beginners' Bootcamp.", "Data Visualization and Exploration with Python\n \nVisualization of complex real-world datasets presents a number of challenges to data scientists. By developing skills in data visualization, data scientists can confidently explore and understand the relationships in complex data sets. Using the Python matplotlib, pandas plotting and seaborn packages attendees will learn to:\n\u2022   Explore complex data sets with visualization, to develop understanding of the inherent relationships.\n\u2022   Create multiple views of data to highlight different aspects of the inherent relationships, with different graph types.\n\u2022   Use plot aesthetics to project multiple dimensions. \n\u2022   Apply conditioning or faceting methods to project multiple dimensions\n \nVisualization is an essential method in any data scientist\u2019s toolbox and is a key data exploration method and is a powerful tool for presentation of results and understanding  problems with analytics. Attendees are introduced to Python visualization packages, Matplotlib, Pandas, and Seaborn. The Jupyter notebook can be downloaded at https://github.com/StephenElston/ExploringDataWithPython", "Test-Driven Data Analysis\n \nTDDA aims to bring the ideas and benefits of test-driven development to the arena of data analysis, augmenting those ideas as appropriate.\nThere are two central planks of TDDA at present:\n   1. The idea of a reference test, which is a lot like a system or integration test for an analytical process\n   2. The idea of using constraints to verify input, intermediate and output data for/from analytical processes.\nThe tdda library (available from Github and PyPI) provides tooling support for both of these, major current components being\n  - Support for writing tests, under unittest or pytest, than involve comparison of complex objects (e.g. graphs, dataframes etc.), possibly with variable components, and regenerating reference (\"expected\") results easily when they have changed (after verification!)\n  - Support for automatically generating suggested constraints from example datasets/data frames (including Pandas DataFrames)\n  - Support for verifying a dataset/dataframe against a set of constraints\n - Support for generating regular expressions from example strings, for possible use as constraints (or otherwise).\n(It will probably do more by May, but these things are there now!)\nThis tutorial will introduce users to using these ideas through the tdda library. Users will be able to use their own analytical processes and/or datasets, or to use example data that will be provided.\n \nThis tutorial will introduce attendees to the concepts of test-driven data analysis and practical, hands-on use of the tdda library (available from Github and PyPI) for (1) writing reference tests, and (2) generating and verifying constraints from data, using Pandas data frames.", "A Beginners Guide to Weather & Climate Data\n \nWeather is part of our every day lives. Who doesn\u2019t check the rain radar before heading out, or the weather forecast when planning a weekend away? But where does this data come from, what is it made of, and what can you do with it? The answer is a mix of measurements, models and statistics. This session will use Python notebooks with examples of how it is done:\n\nWeather is quantified by the temperature, humidity, wind, rainfall and radiation. These variables are all measured at thousands of locations worldwide and all data is collected and checked to improve the weather forecasts. Learn how to interpolate this data to create for instance global temperature maps to estimate the global monthly and annual temperature.\nThe observations are used to constrain the global weather forecast models. These models are complex and simulate the energy, water and carbon fluxes between and in the atmosphere, ocean and land. The output from the models is gridded data, which is generally stored in binary netcdf files. Learn how to load and process the gridded data into time series or maps for further analysis.\nThe raw model data is processed for you when you request weather data through a REST API. Learn how to use a weather API to train a supervised model with historical weather time series and then use the weather forecast data to do predictions.\nThis session provides you with a brief overview of the science behind weather and climate forecasts and gives you the tools to get started with weather data.\n\n \nWeather is part of our every day lives. Who doesn\u2019t check the weather forecast regularly? But where does the data come from, what is it made of? The answer is a mix of measurements and models. This session looks at observations, predictions and forecast models, and weather data as a variable to consider in machine learning models. Learn how it is done from several Python notebook examples.", "Doing frequentist statistics with Scipy\n \nThe session will cover:\n- Normality testing\n- Student's t-test and ANOVA\n- Wilcoxon rank sum and Kruskal-Wallis\n- Correlation\n- Univariate linear and logistic regression\n- Chi-square \n- p-value interpretation\n- Effect size calculation\n \nFrequentist statistical tests are still very common, and in some fields they continue to represent the technical standard. In this session we will cover the execution and interpretation of the most common tests using the SciPy.stats package, and plotting the results with Matplotlib and Seaborn. The focus will be on traditional approaches to the tests, not on Bayesian and bootstrapping approaches", "Interactive Data Visualization Tools in R\n \nWith interactive and reactive data visualizations, your audience directly engages with your data for stronger communication and better understanding. The Shiny apps can easily be launched directly to the web via shinyapps.io or Shiny Server to be run by anyone (they don't need to download your data or have R). This course assumes participants can already perform data analysis and visualization in R, but want to expand their skills with R Shiny.  Therefore, the class exercises focus on transcribing code from a static to an interactive presentation of data products and information.  Students must bring their own laptop with a current version of R and R Studio.\nOutline:\n- Overview of available interactive tools in R\n- Exercise to build a simple Shiny app\n- Shiny setup and orientation\n- Introduction to Shiny code structure\n- Overview of basic widgets\n- Example of conditional reactivity\n \nAn introduction to the R packages that generate interactive table and graphics.  The focus will be on R Shiny, but I will also summarize value of other options, such as leaflet, plotly, ggvis, and rCharts.", "Applied Time Series Econometrics in Python (and R)\n \nTime series data is ubitious, both within and out of the field of data science: weekly initial unemployment claim, tick level stock prices, weekly company sales, daily number of steps taken recorded by a wearable, just to name a few. Some of the most important and commonly used data science techniques to analyze time series data are those in developed in the field of statistics. For this reason, time series statistical models should be included in any data scientists\u00e2\u0080\u0099 toolkit.\nThis 120-minute tutorial covers the mathematical formulation, statistical foundation, and practical considerations of one of the most important classes of time series models, AutoRegression Integrated Moving Average with Explanatory Variables (ARIMAX) models, and its Seasonal counterpart (SARIMAX). Specific topics include\n\u00e2\u0080\u00a2 Common use cases of SARIMAX\n\u00e2\u0080\u00a2 The entire class of SARIMAX models, which include Autoregressive (AR) models, Moving Average (MA) models, Mixed Autoregressive Moving Average (ARMA) models, Autoregressive Integrated Moving Average (ARIMA) models, these models with explanatory variables (e.g. ARIMAX), and these models with seasonal components and explanatory variables (SARIMAX)\n\u00e2\u0080\u00a2 Mathematical formulation\n\u00e2\u0080\u00a2 Underlying assumptions of this class of model\n\u00e2\u0080\u00a2 Implementation of these models in Python and R, in which I will compare and contrast the two, using simulated and real-world time-series data, which includes\no Exploratory time series data analysis using histogram, kernel density plot, time-series plot, scatterplot matrix, plots of autocorrelation (i.e. correlogram), and plots of partial autocorrelation\no Statistical estimation and its options available in Python and R\no Simulation of these models\no Order selection (using the celebrated Box-Jenkins approach)\no Assumption testing and model evaluation\no Forecasting\nThis tutorial is suitable for data scientists who have working knowledge of the classical linear regression model, including its mathematical formulation and underlying statistical assumptions, and practical implementation of regression models using Python or R.\n \nTime series data is ubitious, and time series statistical models should be included in any data scientists\u00e2\u0080\u0099 toolkit.  This tutorial covers the mathematical formulation, statistical foundation, and practical considerations of one of the most important classes of time series models: the AutoRegression Integrated Moving Average with Explanatory Variables model and its seasonal counterpart.", "Spotting trends and tailoring recommendations: PySpark on Big Data in fashion\n \nMallzee is a fashion app where people can see products (clothes, shoes and accessories) and decide whether they like them or not. They can also buy products and create feeds of preferred brands and categories of items.\nWe have large amounts of data generated by the users when they scroll and search through products and we use it to understand the user. We want to give everyone meaningful recommendations on the items they might like, hence tailoring the experience to who they are.  We build pseudo-intelligent algorithms capable of extracting the style profile of a user and we crunch products data to match items to the user based on a classification model. The model is validated and statistical analyses are performed to determine the tipping point when recommendations are valuable to the user. \nThe talk will go through the steps we implement by showing how the full data stack of Python is used in achieving this goal and how it is interfaced with a Spark cluster through PySpark in order to run Machine Learning algorithms on big data.\n \nPredicting what people like when they choose what to wear is a non-trivial task involving several ingredients. At Mallzee, the data is variegated, large and has to be processed quickly to produce recommendations on products, tailored to each user. We use PySpark to crunch large sets of different data and create models in order to generate robust and meaningful suggestions.", "Deep learning tutorial - advanced techniques\n \nIn the last few years, deep neural networks have been used to generate state-of-the-art results in image classification, segmentation and object detection. They have also successfully been used for speech recognition.\nIn this tutorial we build on the basics, demonstrating some useful techniques that are useful in a practical setting. Along with tips and tricks found to be useful, we will discuss the following:\n\nactive learning; train a neural network using less training data\nusing pre-trained networks; using the body of a pre-trained network (e.g. an ImageNet network) and re-using it for localisation or locating objects not within the original training set\nhaving fun with neural networks; some of the fun techniques that have been demonstrated in the last couple of years\n\nPrior knowledge of deep learning, machine learning, linear algebra, a bit of calculus and the NumPy/SciPy stack would be helpful for participation. We will mainly focus on using the Theano toolkit along with the Lasagne neural network library.\n \nSome of the more advanced deep learning to help you get the best out of it in a practical setting. The main focus is on computer vision and image processing.", "A Quick Primer on TensorFrames: Apache Spark and TensorFlow Together\n \nThis session will provide a high-level primer on the burgeoning field of Deep Learning and the reasons why it is important. It will provide the fundamentals surrounding feature learning and neural networks required for deep learning; more specifically:\n\nWhat is Deep Learning?\nA primer on feature learning\nWhat is feature engineering?\nWhat is TensorFlow?\nIntroducing TensorFrames\nTensorFrames \u2013 quick start\n\nIn addition to reviewing the concepts, we will have demos and provide links to notebooks that attendees will be able to use.\n \nThis session will provide a high-level primer on the burgeoning field of Deep Learning and the reasons why it is important. It will provide the fundamentals surrounding feature learning and neural networks required for deep learning. As well, this session will provide a quick start for TensorFrames for Apache Spark.", "Easy Bayesian regularization for fitting financial time series and curves\n \nWhen you read 'Bayesian' I bet your first reaction is 'complicated and computationally expensive'; and 'regularization' all too often simply means 'a pesky parameter I have to set'. I hope to convince you they're both easier and more powerful than that. \nThis talk will show, on a variety of examples, an intuitive yet powerful way of fitting/smoothing financial time series and curves, using a combination of two ideas: firstly, interpreting bid-ask spreads as confidence intervals, and secondly, using Bayesian ideas to easily combine observed values with your a priori assumptions about behavior, such as 'curves must be smooth' or 'skewness is stickier than at-the-money vol' - even if neither skewness nor ATMV are actual model parameters.\nBeyond the above, still basically 'linear' approach, the talk will also cover some extensions such as dealing with hard inequality constraints and use of change detection for a responsive nonlinear smoother.\n \nThis talk will show, on a variety of examples, an intuitive yet powerful way of fitting/smoothing financial time series and curves, using a combination of two ideas: firstly, interpreting bid-ask spreads as confidence intervals, and secondly, using Bayesian ideas to easily combine observed values with your a priori assumptions about behavior.", "Despicable machines: how computers can be assholes\n \nWhen working on a new ML solution to solve a given problem, do you think that you are simply using objective reality to infer a set of unbiased rules that will allow you to predict the future? Do you think that worrying about the morality of your work is something other people should do? If so, this talk is for you.\nIn this brief time, I will try to convince you that you hold great power over how the future world will look like and that you should incorporate thinking about morality into the set of ML tools you use every day. We will take a short journey through several problems, which surfaced over the last few years, as ML and AI generally, became more widely used. We will look at bias present in training data, at some real-world consequences of not considering it (including one or two hair-raising stories) and cutting-edge research on how to counteract this.\nThe outline of the talk is:\n\nIntro the problem: ML algos can be biased!\nTwo concrete examples.\nWhat's been done so far (i.e. techniques from recently-published papers).\nWhat to do next: unanswered questions.\n\n \nThere's a widespread belief among machine learning practitioners that algorithms are objective and allow us to deal with the messy reality in a nice, objective way, without worrying about all the yucky human nature things. This talk will argue that this belief is wrong. Algorithms, just like the humans who create them, can be severely biased and despicable indeed.", "Blockchains for Artificial Intelligence\n \nIn recent years, big data has transformed AI, to an almost unreasonable level. Now, blockchain technology could transform AI too, in its own particular ways. Some applications of blockchains to AI are mundane yet useful, like audit trails on AI models. Some appear almost unreasonable, like AI that can own itself\u200a\u2014\u200aAI DAOs (decentralized autonomous organizations) leading to the first AI millionaires. All of them are opportunities. Blockchain technologies\u200a\u2014\u200aespecially planet-scale ones\u200a\u2014\u200acan help realize some long-standing dreams of AI and data folks. This talk will explore these applications.\n \nThis talk describes the various ways in which emerging blockchain technologies can be helpful for machine learning / artificial intelligence work, from audit trails on data to decentralized model exchanges.", "Machine Learning with Text in scikit-learn\n \nIt can be difficult to figure out how to work with text in scikit-learn, even if you're already comfortable with the scikit-learn API. Many questions immediately come up: Which vectorizer should I use, and why? What's the difference between a \"fit\" and a \"transform\"? What's a document-term matrix, and why is it so sparse? Is it okay for my training data to have more features than observations? What's the appropriate machine learning model to use? And so on...\nIn this tutorial, we'll answer all of those questions, and more! We'll start by walking through the vectorization process in order to understand the input and output formats. Then we'll read a simple dataset into pandas, and immediately apply what we've learned about vectorization. We'll move on to the model building process, including a discussion of which model is most appropriate for the task. We'll evaluate our model a few different ways, and then end by examining the model for greater insight into how the text is influencing its predictions.\nTo get the most out of this tutorial, participants should be comfortable working in Python, should understand the basic principles of machine learning, and should have at least basic experience with both pandas and scikit-learn. However, no knowledge of advanced mathematics is required.\nParticipants will need to bring a laptop with scikit-learn and pandas (and their dependencies) already installed. Installing the Anaconda distribution of Python is an easy way to accomplish this. Both Python 2 and 3 are welcome.\nAfter the tutorial, participants will be provided with a well-commented Jupyter notebook and a list of resources for going deeper into the material.\n \nAlthough numeric data is easy to work with in Python, most knowledge created by humans is actually raw, unstructured text. By learning how to transform text into data that is usable by machine learning models, you drastically increase the amount of data that your models can learn from. In this tutorial, we'll build and evaluate predictive models from real-world text using scikit-learn.", "Getting Started with Bokeh\n \nBokeh is a python interactive visualization library that uses web browsers for its presentation. Bokeh supports a wide variety of visualization tasks from basic exploration through to building advanced data applications. In this tutorial we'll cover Bokeh's basic concepts and go from plotting a scatter plot to building an interactive dashboard that can run a clustering algorithm from a dropdown - you'll be surprised how easy that is.\nWe will be working mostly in jupyter notebooks, but the application examples will be run from the command line. Due to limited time we won't be doing many exercises independently, but it would be great to bring a laptop with everything installed so you can code-along.\nNo prior knowledge of Bokeh is required but a basic knowledge of Python would be very helpful.\nRequirements & Setup\nI strongly recommend using Anaconda or Miniconda to install bokeh and can only provide troubleshooting on the day for conda installs due to limited time.\nInstalling with anaconda\nInstall anaconda https://continuum.io/downloads\nAnaconda should come with all the dependencies included, but you may need to update your versions.\nInstall with miniconda\nInstall miniconda https://conda.pydata.org/miniconda.html\nUse the command line to create an environment and install the packages:\n$ conda create -n bokeh_tutorial python=3.4\n$ source activate bokeh_tutorial\n$ conda install bokeh pandas jupyter\nOptionally:\n$ conda install scikit-learn\nDownload Bokeh sampledata\n$ python\nimport bokeh\nbokeh.sampledata.download()\nDownload Tutorial materials\nWe will use some of the Bokeh notebooks: https://github.com/bokeh/bokeh-notebooks/ which you can clone or download as a zip.\nMore tutorial materials to follow.\n \nBokeh is a python interactive visualization library that uses web browsers for its presentation. Bokeh supports a wide variety of visualization tasks from basic exploration through to building advanced data applications. In this tutorial we'll cover Bokeh's basic concepts and go from plotting a scatter plot to building an interactive dashboard that can run a clustering algorithm from a dropdown.", "Forecasting with the Kalman Filter\n \nThere are many problems where we understand how a system evolves through time if we know the value of state variables, but those variables may be unobservable. Kalman filtering is a method for refining knowledge about the state using new values of a variable that we can observe.  It has numerous applications from navigation to biomedical engineering to electrical power systems, and it's also a basic data analysis tool in econometrics.  However because it's so widely applied and has several variants, it can be hard to grasp the fundamentals.\nThis talk will explain the basics of the Kalman filter and then show how to set up computations using the implementation of state space time-series models in the statsmodels Python library.  We'll start with basic examples and progress to an application of forecasting electricity demand.  This talk will only assume knowledge of basic Python programming and some background in probability and statistics.\n \nThe Kalman filter is a popular tool in control theory and time-series analysis, but it can be a little hard to grasp.  This talk will serve as in introduction to the concept, using an example of forecasting an economic indicator with tools from the statsmodels library.", "Introduction to Julia\n \nIntroduction to Julia\n \nIntroduction to Julia", "Using small data in the client instead of big data in the cloud\n \nRather than using big data in the cloud, we at Transit App found it useful to to give as much data to client as possible and process it client-side. In order to keep transfer volume and performance reasonable, it makes sense to move from text based interchange formats like \u00e1\u00b4\u008as\u00e1\u00b4\u008f\u00c9\u00b4 or x\u00e1\u00b4\u008d\u00ca\u009f back back in time to using binary file formats. In this talk we will introduce some common libraries to make data compact, to make it fast, and how these libraries work. We will also discuss the approach of our own library for creating schema-less binary file formats, and how it helped us store complete transit schedules in small files on the phone and display their data instantly.\n \nRather than using big data in the cloud, at Transit App we send data sets to the client and process it there. To keep the data small, we\u00e2\u0080\u0099ve switched to using binary files instead of plain text formats like \u00e1\u00b4\u008as\u00e1\u00b4\u008f\u00c9\u00b4. In this talk we\u00e2\u0080\u0099ll discuss how you can turn data into binary, how it works, and our own approach to store complete transit schedules on the device and show their data instantly.", "Building a recommendation engine with Python and Neo4j\n \nIn this session Mark will show how to build a meetup.com recommendation engine using Neo4j and Python. \nOur solution will be a hybrid which makes uses of both content based and collaborative filtering to come up with multi layered recommendations that take different datasets into account e.g. we'll combine data from the meetup.com and twitter APIs. \nWe'll evolve the solution from scratch and look at the decisions we make along the way in terms of modelling and coming up with factors that might lead to better recommendations for the end user. \n \nIn this session Mark will show how to build a meetup.com recommendation engine using Neo4j and Python. Our solution will be a hybrid which makes uses of both content based and collaborative filtering using Neo4j to glue all the data together, Cypher to query the dataset and Python to do analysis and pre/post processing of data.", "Python Web Sraping\n \nDetailed Abstract:\nIn this tutorial, we will start with the understanding the basic components of web pages, HTML, CSS, JS etc. Then we will start building a fully functional web scraper in python:\n - retrieve page content through GET request\n - parsing HTML with BeautifulSoup\n - searching for instances through tags, classes and ids\n - searching for instances using CSS selectors\n - extracting and organizing information from the page\n - combining data into a dataframe and \nExtra credit:\n - Ways to scrape from dynamic web pages\n - Retrieving data using the Selenium web driver\n - Interactive data retrieval: search bars\nHands on:\n - Build a web scraper to retrieve top 100 grossing movies from boxofficemojo.com\n \nWhen you do data science projects, it's common to need to find data from the web. At Metis, one of our projects focuses on data collection using web scraping. In this tutorial, you will learn to write your own script to retrieve and extract information programmatically using Python packages and explore ways to extract data from a website so that you will have a fully functional Python web scrape.", "Ten Steps to Keras\n \nGoal of the Tutorial\n\nIntroduce main features of Keras APIs to build Neural Networks.   \nLearn how to implement simple and complex Deep Neural Networks Architectures using Keras.   \n\nDiscover Keras Implementation and Internals.\n\n\nNote: examples and hands-on exercises will be provided along the way.\n\n\nOutline (in ten Notebooks)\n\nMulti-layer Fully Connected Networks (and the backends)\nBottleneck features and Embeddings\nConvolutional Networks\nTransfer Learning and Fine Tuning\nResidual Networks\nRecursive Neural Networks\n[Variational] AutoEncoders and Adversarials\nMulti-Modal Networks\nCustom Layers and Optimisers \nInteractive Networks and Callbacks\n\n\nDescription\n\n\nMulti-layer Fully Connected Networks\nIn this notebook we will learn the basic building blocks of Keras APIs to create deep neural networks. We will learn how to use the Sequential object as well as Functional and keras.backend APIs. First examples of MLP and Fully Connected Networks will be presented.\n\n\nBootleneck Features and Embeddings \nAfter having played a bit with Keras APIs for building networks, we start learn how to inspect network's internals. In details, we will learn (1) how to iterate over layers; (2) how to initialise and get weights; (3) how to extract bottleneck features and create embeddings.\n\n\nConvolutional Networks\nThis notebook will teach how to build CNN (Convolutional Neural Networks). Convolutional, Pooling, DropOut layers will be presented, along with clear description on how to properly apply convolutions on images, depending on image_dim_ordering setting.\n\n\nTransfer Learning and Fine Tuning\nThe Keras implementation of some famous Deep Convolutional Networks  will be presented (i.e. keras.applications.vgg16, keras.applications.vgg19, and keras.applications.inceptionv3). We will learn how to leverage on these models for transfer learning and fine tuning using Keras Layer APIs.\n\n\nResidual Networks\nIn this notebook, Residual Networks will be presented. In particular, the Keras implementation of the residual network topology will be explained. Then, ResNet (keras.applications.resnet50) and InceptionV4 Keras implementation will be detailed.   \n\n\nRecursive Neural Networks\nRecursive Neural Networks (i.e. LSTM and GRU) are the main topic of this notebook. The Keras implementation of these two types of network will be presented along with working examples combining Word Embeddings and Convolutional Layers (i.e. keras.layers.convolutional_recurrent)\n\n\n[Variational] AutoEncoders and Adversarials\nThis notebook will be devoted to show how to implement AutoEncoders in Keras. In particular, the implementation of Stacked AutoEncoders, Variational AutoEncoders and Generative Adversarial Networks will be presented.\n\n\nMulti-Modal Networks\nMulti-Input and Multi-task Networks are the fundamental steps for advanced deep networks. This notebook will provide implementation recipes and examples. \n\n\nCustom Layers and Optimisers\nThis notebook will provide details and examples of Keras internals. In particular, we will learn how to implement a Custom Layer in Keras, and custom Activation functions, and custom optimisers. \n\n\nInteractive Networks and Callbacks \nIn this last notebook, keras.callbacks will be explained. Callbacks to track and monitor network performances during the training process will be built and integrated inside a web app. Finally, an example of keras-js will be described, detailing functions in Keras to export models and weights (in json and hdf5 formats).\n\n\nRequirements\nThis tutorial requires the following packages:\n\n\nPython version 3.5.x\n        - Python 3.4 is fine as well\n        - likely Python 2.7 would also be fine, but who knows? :P\n\n\nnumpy version 1.10 or later: https://www.numpy.org/\n\nscipy version 0.16 or later: https://www.scipy.org/\nmatplotlib version 1.4 or later: https://matplotlib.org/\npandas version 0.16 or later: https://pandas.pydata.org\nscikit-learn version 0.15 or later: https://scikit-learn.org\nkeras version 1.0 or later: https://keras.io\ntensorflow version 0.9 or later: https://www.tensorflow.org\nipython/jupyter version 4.0 or later, with notebook support\n\n(Optional but recommended):\n\npyyaml\nhdf5 and h5py (required if you use model saving/loading functions in keras)\n\nThe easiest way to get (most) these is to use an all-in-one installer such as Anaconda from Continuum. These are available for multiple architectures.\n \nIn this tutorial we will learn Keras in ten steps (a.k.a. Jupyter Notebooks). \nWe will warm up by learning how to create a multi layer network, and then we will go through more sophisticated topics such as implementing different types of networks (e.g. RNN, CNN), creating custom layers and discovering Keras internals. numpy proficiency and basic knowledge of Machine/Deep Learning are assumed.", "How to manage complexity in distributed applications.\n \nThe development of large-scale distributed applications is an engineering challenge by itself. Development has to be orthogonal to be scalable, as you may know if you have heard about the Mythical Man-Month and the Conway's law: trying to make your application faster may slow down your development. Managing complexity is a new technology trend, and NFQ and the Carlos III University of Madrid have developed a library to make large-scale distributed applications more sensible called pylm (pylm.readthedocs.io). Since this library has been already used in production, it is time to summarize what are the challenges one faces when building something more intricate than a Spark cluster.\nThis talk is about the value of developing in-house tools and obtaining deep technological insight opposed to the successive integration of trendy technologies. The latter is suitable to implement one-shot tools for isolated projects, but when facing a multi-year complex project, the former becomes a more solid ground for long-term maintenance. Complexity piles up nonlinearly, and the most popular tools nowadays cringe when they have to be tightly integrated, since in the long term it is impossible to isolate the technical and the human aspects of development.\nComplexity's weight is getting heavier in this scalability-obsessed world, and it's time to talk about it.\nThis project has been funded by the Spanish Ministry of Economy and Competitivity under the grant IDI-20150936, cofinanced with FEDER funds\n \nComplexity is tricky. Some years ago we got how to scale the performance of distributed applications, and that's why everyone is talking about Big Data.  But the challenge now is scaling the complexity within a fast-changing environment without penalizing the performance. These are the conclusions after one year developing a library trying to handle this issue, and using it in production.", "Finding Lane Lines for Self Driving Cars\n \n\nIntroduction to the OpenCV library (loading images, plotting with matplotlib, etc\u2026)\nStarting with single images, introduce Gaussian blur, region of interest filtering, canny edge detection, Hough transform and draw the lane lines.\nCreate a lane line detection pipeline with those functions; extrapolating the lines to represent the boundaries of the lane.\nProcess dash cam video using the single image techniques with the pipeline.\nStitch together images from the processing pipeline to create a sweet video!\n\n \nSelf-driving cars might not be in our everyday lives yet, but they are coming! Analyzing images and figuring out where the lane lines are on a given roadway is one of the core competencies of any respectable self-driving car. Humans do this with ease and this talk will show you how to find these lines using Python and OpenCV.", "Interactive multi-scale time series exploration with matplotlib\n \nPlease come with python3, matplotlib, IPython, and pandas installed on your computer.  This tutorial should work in both the notebook and from the IPython prompt, but will be taught primarily from the prompt.\n \nThis tutorial is an introduction to how to matplotlib's event handling system to build an tool for interactively exploring multi-scale time series data.\nThe primary example will be how to 'drill down' through summary data sets to view the underlying data using hourly weather data from NOAA.", "Introduction to Julia for Pythonistas\n \nThis workshop will introduce the Julia language to those coming from a Python background.\nTopics will include:\n\nInstalling Julia\nWorking at the REPL\nBasic data types\nDefining your own types\nTypes vs. Classes\nMultiple Dispatch\nCommon pitfalls for Python users\n\n \nMany Python users are curious about Julia, but the language is still evolving, and best practices are not yet widespread. This tutorial will introduce the Julia language, with a focus on two key areas \u2014 multiple dispatch and the type system \u2014 that often trip up users coming from dynamic languages.", "Deconstructing Feather\n \nCSV files are great for storing tabular data, this format is both human and machine readable. But this is not always the best format. Robustly parsing CSV files is challenging, see for example, the number of optional parameters accepted by the read_csv function in Pandas. CSV files also lose type information, since all fields must be strings or numeric, encoding categorical variables is difficult. \nThe Hadoop and Spark communities have solved this data serialization problem many times over. Among the more popular file formats are Avro, Parquet, and Arrow. Each is optimized for a different use, Avro for JSON-like data, Arrow for tabular data, and Parquet for columnar data. Similar to the CSV format, these new formats are supported by a wide range of reader/parsers, ensuring data written by an Hadoop task and later be read by PySpark.\nWes McKinney and Hadley Wickham recently created the Feather format, aimed at making it easier to share data between languages, specifically Python and R. Feather is most similar to Arrow, although Feather is meant for on-disk representation instead of in-memory. Among the many advantages of using Feather instead of CSV files to cross the language boundary is the incredible speedup that comes from removing the parser. Aside from some metadata, reading a Feather files does not require any parsing, the on-disk representation of various column types such as integer, floating point, and categorical are mirrors of their in-memory representation in Python and R.\nIn this talk, we'll do a deep dive on the Feather format, manually deconstructing the on-disk representation with some struct and NumPy magic.\n \nFeather, Avro, Parquet, Arrow... all these new file formats, what's wrong with good old CSVs? In this talk, we will cover why CSV files are not always the optimal storage format for tabular data and what optimizations each of these formats make. We will do a deep dive on Feather, a cross-language format created by Wes McKinney and Hadley Wickham. ", "Keynote: Empowering people by democratizing data skills\n \n\n \nComing soon.", "What every data scientist should know about data anonymization\n \nOutline:\n\nLook at some of the examples where data anonymization was broken and identify what went wrong\nWhat is the state of the art for data anonymization and can you be sure to be safe if you follow it? \nHow does anonymization affect the possibilities for data mining/machine learning on the data? \n\nThis talk is aimed at people who want release open data or want to share sensitive data between departments. \n \nThere are numerous examples of data anonymization gone horribly wrong - the most prominent one might be the netflix prize, where researchers were able to uniquely identify users by combining netflix user data with imdb reviews. Let's learn from their mistakes and look at some of the measures you can take to better anonymize data before you share it with others.", "JupyterHub: Deploying Jupyter Notebooks for students and researchers\n \nThe Jupyter Notebook is an interactive web-based tool for interactive programming and writing code-centric documents. Being a web-based environment, the notebook server can be run remotely, not just on your local machine. JupyterHub is a multi-user server, aimed at helping research groups and instructors host notebook servers for their users or students. By default, JupyterHub uses the local system users and PAM authentication, but it can be customized to use any authentication system, including GitHub, CILogon, Shibboleth, and more. The way single-user servers are spawned can also be customized to use services such as Docker, Kubernetes, or HPC cluster queuing systems. The tutorial will cover a basic deployment of JupyterHub on a single machine, then extending it to use docker and GitHub authentication, as well as general best practices for JupyterHub deployment.\n \nLearn to deploy JupyterHub! JupyterHub is a simple, highly extensible, multi-user system for managing per-user Jupyter Notebook servers, designed for research groups or classes. We will cover deploying JupyterHub on a single server, as well as deploying with Docker using GitHub for authentication.", "Vocabulary Analysis of Job Descriptions\n \nIn the initial analysis of a data set it is useful to gather informative summaries. This includes evaluating the available fields, by finding unique counts or by calculating summary statistics such as averages for numerical fields. These summaries help in understanding what is in the data itself, the underlying quality, and illuminate potential paths for further exploration. In structured data, this a straightforward task, but for unstructured text, different types of summaries are needed. Some useful examples for text data include a count of the number of documents in which a term occurs, and the number of times a term occurs in a document. Since vocabulary terms often have variant forms, e.g. \u201cperforms\u201d and \u201cperforming\u201d, it is useful to pre-process and combine these forms before computing distributions. Oftentimes, we want to look at sequences of words, for example we may want to count the number of times \u201cdata science\u201d occurs, and not just \u201cdata\u201d and \u201cscience\u201d. We will use the pandas Python Data Analysis Library and the Natural Language Toolkit (NLTK) to process a data set of job descriptions posted by employers in the United States, and look at the difference in vocabularies across different job segments.\ngithub repo\n \nWe will explore how to do some basic natural language processing (NLP) on the text including tokenization and stemming to combine word forms, and stop word removal and sentence detection to examine word sequences (n-grams). We will then look at the distribution of the vocabulary terms and n-grams in our data set using term frequency and inverse document frequency (TF.IDF).", "Seeing the invisible - Bayesian Mixture Models\n \nMixture Models are a form of unsupervised learning that can be used to obtain 'fuzzy' clustering - i.e. probabilities of each observation belonging to each cluster. Probabilistic programming enables us to build a model on small data and estimate many unknown parameters (optimal number of clusters, cluster probabilities, cluster centres/means and standard deviation...) whilst passing the uncertainty along into the final parameter estimates, so for example, we'll get a range of likely values for a cluster mean, not just a point estimate. \nAs wifi point id is in fact known, we effectively have information about true clusters to test against. Also, our 2-dimensional longitude-latitude data lends itself well to common-sense checks. We'll compare the results of 3 popular algorithms: Markov Chain Monte Carlo simulation (mcmc), Variational Bayes and MAP.  \nWe'll use PYMC3 for modelling and Folium for visualisation. Aimed at anyone apart from absolute beginners in python, only basic statistics knowledge is required.\n \nEstimating wifi point locations using Bayesian Mixture Model on mobile phone sensor data. Visualising results and comparing against known truth.", "Machine Learning to know if you R coming\n \nSome users are more likely to cancel their bookings than others, depending on several factors/conditions.  Knowing these factors and how many bookings will be canceled will help hotels to optimize their incomes and maximize their occupancy.\n \nPredicting Hotel Reservation Cancellations with R language. \nKnowing in advance bookings that will be canceled before the arrival date is crucial in the hotel sector. \nWe will examine the determinant characteristics of reservation cancellation, the constrains of this project and how the probability of cancellation was estimated.", "Gold standard data: lessons from the trenches\n \nIt is often said that rather than spending a month figuring out how to apply unsupervised learning to a problem domain, a data scientist should spend a week labelling data. However, the difficulty of annotating data is often underestimated. Gathering a sufficiently large collection of good-quality labelled data requires careful problem definition and multiple iterations. In this talk, I will describe three case studies and lessons learnt from them. Each case shows several aspect of the process that should be considered in advance to ensure the project is successful. \nCase study 1: word embeddings\nMethods for representing words as vectors have become popular in recent years. Historically, these have been evaluated by correlating the \"similarity\" of word pairs, as predicted by a model, to that provided by a human judge. A typical data set consists of word pairs and a similarity score, e.g. cat - dog = 80% and cat - democracy = 21%. \nQuestions:\n\nIs the task clearly defined? \n\nWhat exactly is word similarity? We all have an intuitive understanding, but a lot of corner cases exist, e.g. What rice - cooking and big - small? Each annotator may interpret your instructions differently. Can you guarantee they have not misunderstood the task?\n\nIs the task easy to do?\n\nBecause it is so hard to provide annotators with exact instructions, they will often disagree. The similarity of tiger\u2013cat ranges from 50% to 90% in a popular data set.  If human cannot agree what the right answer for a given input is, how can a model ever do well?\n\nDo you have quality control in place?\n\nHow should one deal with poor-quality annotation? Do you have a mechanism for identifying annotator errors or consistently under-performing annotators? \n\nHow much data can you get?\n\nCan you get enough data? How much do you need? What if you have to data by unreliable annotators?\nCase study 2: symptom recognition in medical data\nThe task is to identify mentions of symptoms in notes taken by a doctor during routine exams, e.g. **Abdominal pain**. Causes may include **acute bacterial tonsilitis**. No allergies.\n\nDo you need an expert (linguist/doctor/banker)?\n\nCan you find subject experts? Can you communicate to them exactly what the task is, keeping in mind they will not be NLP experts?\n\nPeople issues\n\nWhat if annotators do not show up, or take two-week breaks between sessions?\n\nCan you measure inter-annotator agreement?\n\nCan you quantify annotator agreement? In the case of word similarity, this is reasonably easy. This gets trickier when the annotation unit has complex structure (e.g. it is a phrase). See submission by Alexandar Savkov for more details.\n\nDo you have a process for resolving conflicts?\n\nIf two of the annotators disagree, can the conflict be resolve or should the data be discarded?\nCase study 3: boilerplate removal in HTML pages\n\nWhat is the data for?\n\nThe distinction between content and boilerplate is blurry and may change depending on what you plan to do next.\n\nTooling\n\nDo you need specialist software? Do you have to write it yourself?\n \nThe first stage in a data science project is often to collect training data. However, getting a good data set is surprisingly tricky and takes longer than one expects. This talk describes our experiences in labelling gold-standard data and the lessons we learnt the hard way. We will present three case studies from natural language processing and discuss the challenges we encountered.", "The Five Kinds of Python Functions\n \nGarden Variety functions we all know and love. The ideals of idempotency and no hysteresis.\nBreaking the no-hysteresis rule with callable objects. This can be used for caching previous results. Or it can be used for evil by making functions which are no longer idempotent.\nLambda objects and reasons why an anonymous function with only a single expression can be useful.\nGenerator functions and the iterator interface. These can help us avoid creating large data structures in memory. We'll touch on some of the optimization techniques that allow generators to be used recursively.\nWe'll also look at the object model and why we might want to wrap common methods in functions as purely syntactic sugar.\n \nWe'll look at the wide variety of ways that we can leverage Python functions. This will show provide helpful background in ordinary functions, as well as callable objects and lambdas. We'll look closely at how to use generator functions, also.  The fifth type of function is a function wraps a special method, like len().", "Data Science Driven Business Insights with Python and PySpark\n \nMost XaaS offerings in the cloud provide a discover-try-buy user experience, how to predict which users have the highest propensity to be customers, or predict customers churn, or provide service recommendations become important to the XaaS. We will present how to use Python on SaaS/PaaS to do data science driven actionable business insights.\nThe details include how to get progression gating issues from the funnel, how to build the solution using various SaaS offerings such as SPSS, Spark, and dashDB on IBM Bluemix, how to use PySpark to access and analyze data in dashDB through the Spark-as-a-service environment on Bluemix. Moreover, we will present how to use Python and PySpark integrated in the Jupyter notebook to predict sales leads, service recommendations and analyze churns with data science algorithms.\nTake Aways:\nHow we do data science driven for business\n\n\nActionable business insights through progression funnel\n\n\nData science solution architecture using SaaS/PaaS\n\n\nData science Survival analysis of XaaS user churn behavior analysis using PySpark, SPSS and R\n\n\nSales leads prediction using Python\n\n\nService Recommendation using PySpark\n\n\n \nMost XaaS offerings in the cloud provide a discover-try-buy user experience, how to predict which users have the highest propensity to be customers, or predict customers churn, or provide service recommendations become important to the XaaS. We will present how to use Python on SaaS/PaaS to do data science driven actionable business insights. ", "Adding Image and Voice Intelligence to Your Apps with Microsoft Cognitive Services\n \nCognitive Services is a set of APIs that use the power of Machine Learning to enhance your application. Using these APIs, you can quickly add image recognition and analysis; facial recognition, speech recognition, text-to-speech capabilities, and many other features to your application. \nIn this presentation, you will learn about the capabilities of these APIs, how to test them, and how to call them via a REST web service and using some helpful libraries.\n \nCognitive Services is a set of APIs that use the power of Machine Learning to enhance your application. Using these APIs, you can quickly add image recognition and analysis; facial recognition, speech recognition, text-to-speech capabilities, and many other features to your application. ", "JupyterLab\n \nProject Jupyter provides building blocks for interactive and exploratory computing. These building blocks make science and data science reproducible across over 40 programming language (Python, Julia, R, etc.). Central to the project is the Jupyter Notebook, a web-based interactive computing platform that allows users to author data- and code-driven narratives - computational narratives - that combine live code, equations, narrative text, visualizations, interactive dashboards and other media. \nThe fundamental idea of JupyterLab is to offer a user interface that supports interactive workflows that include, but go far beyond, Jupyter Notebooks. In JupyterLab, users can arrange multiple notebooks, text editors, terminals, output areas, etc. on a single page with multiple panels, tabs, splitters, and collapsible sidebars with a file browser, command palette and integrated help system. The codebase and UI of JupyterLab is based on a flexible plugin system that makes it easy to extend with new components.\n \nThis talk provides an early view of JupyterLab, an evolution of the Jupyter Notebook that provides a modular and extensible user interface within the context of a powerful workspace.", "Visualizing FragDenStaat.de\n \nMotivation\nFragDenStaat.de offers an API with which you can scrape more than 12000 public freedom of information requests. Each request contains information about: when it was sent, to which public body and to which jurisdiction it was sent, how long it took the public body to complete the request, whether the request was successful and if not, why so.\nVisualizing all these aspects of the data set might lead to new insights about this transparacy process.\nData preparation\nHow I scraped the data set and organized it using pandas.\nData visualization\nHow I visualized the data set using pandas and matplotlib.\nVisualizations\nThe results of my work so far.\n \nFragDenStaat.de is a website that helps people to make freedom of information requests in Germany. Starting in 2011, they collected over 12000 of these requests. Using pandas and matplotlib, I tried to bring life into this interesting data set.", "Introduction to Deep Learning & Natural Language Processing\n \nWe would cover the following: 1) What is deep learning? 2) Motivation: Some use cases where it has produced state-of-art results 3) Basic building blocks of Neural networks (Neuron, activation function, back propagation algorithm, gradient descent algorithm) 4) Supervised learning (multi-layer perceptron, convolution neural networks, recurrent neural network) 5) Introduction to word2vec 6) Introduction to Recurrent Neural Networks 7) Text classification using RNN 8) Impact of GPUs (Some practical thoughts on hardware and software) Broadly, there will be three hands-on modules 1) A simple multi-layer perceptron - to understand basics of neural networks (everything will be coded from scratch) 2) A text classification problem and a text generation problem: This will be solved using Recurrent Neural Networks. The data and software requirements are posted on the github repository. The repository for this workshop: https://github.com/rouseguy/intro2deeplearning/ The slides for this workshop are available at: https://speakerdeck.com/bargava/introduction-to-deep-learning \n \nThis workshop will provide an introduction to deep learning for natural language processing (NLP). It will cover some of the common deep learning architectures, describe advantages and concerns, and provide some hands-on experience. ", "Keynote (McKinley & Livestream in Cascade)\n \n-\n \nKatrina Riehl is a Senior Manager for the Global Data Science team at HomeAway.com.  Over the last decade, Katrina has worked extensively in the fields of scientific computing, machine learning, data mining, and visualization. Katrina received her PhD in Computer Science with a concentration in Artificial Intelligence from the University of Texas at Dallas.", "Building Web-based Analysis & Simulation Platforms with React/Redux, Flask, Celery, Bokeh, and Numpy\n \nThe purpose of this stack is to be able to rapidly create web-based environments for users to interact with the results of analytical and simulation processes (without needing to retrain one's self as a web programmer!)\nThis tutorial is composed of the following pieces:\n\n\nbuilding a simple simulation using Numpy. For the purposes of this tutorial, we model a very simple Monte Carlo simulation with a number of user-controllable, tweakable algorithm inputs and model parameters. The simulation is chosen to be simple enough to present and code quickly. The purpose of this tutorial is not building Monte Carlo simulations but packaging them into lightweight production systems.\n\n\nCelery for launching and managing the above simulation jobs. This tutorial will not cover all aspects of Celery. It will merely show how the tool can be used as a job management system.\n\n\nFlask as a very thin JSON API layer. The tutorial will make use of Flask plugins for quickly building JSON APIs. This is the thinnest and least interesting component of the tutorial and won't be covered in great depth. \n\n\nReact + Redux for a slick, simple single-page app. Attendees are expected to be least familiar with Javascript and the React ecosystem. The tutorial will spend a fair amount of time on this component, and will cover setting up build environment using Babel (for JSX transpilation) and Gulp as a build system.\n\n\nBokeh for presenting graphical results from the simulation. This component may be cut based on time considerations.\n\n\nIf time permits, it might also be possible to discuss the use of React Native to quickly build mobile apps using the same infrastructure.\n \nWhat use is analytical code if it can't be integrated into a business workflow to solve real problems?\nThis tutorial is about integrating analytical work into a real production system that can be used by business users.\nIt focuses on building a web-based platform for managing long-running analytical code and presenting results in a convenient format, using cutting-edge combination of tools.", "Happiness inside a job: a social network  analysis\n \nIn this talk, we will show how to analyze social network data to predict employee turnover and employee happiness. This talk is a summary of different notebooks about social network analysis that will be available on Github. Please note that all the methodology and details we are using, are explained in depth in the notebooks.\nWe will cover the following topics:\n\n\nFrom tables to graphs: How to use pandas and networkx to merge several DataFrames into a graph, and different criteria for creating a graph representation of a database.\n\n\nGraph features: How to extract new features using social network analysis techniques such as Non-Negative Matrix Factorization and graph centrality metrics. We will explain what these features represent, and how they relate to other types of features that can be extracted directly from the database.\n\n\nImproving ML models: How to use graph features to boost machine learning models. We will talk about different feature selection techniques that can be used to filter out the most significant set of graph-based features using scikit-learn.\n\n\nVisualization: How to visualize social interactions using bokeh, and how visualization techniques can be used to check data integrity.\n\n\nWe will also review common mistakes that can happen during the modeling process and, how to prevent invalid data from getting leaked into your machine learning models. To conclude we compare the merits of using a Python framework versus R Language in terms of development time and computational performance. Data set provided by myhappyforce.com\n \nIn this talk, we will show how to analyze social network data from a mobile phone application to predict employee turnover and employee happiness. We will look into the process of extracting new features from a dataset using social network analysis techniques. We will also show how these features can be visualized and used to boost machine learning models.", "Data Science & Data Visualization in Python. How to harness power of Python for social good?\n \nIn my talk I will primarily focus on answering/offer the answer to these questions: \n\u2022   Why we need data science and why more and more people should be really interested in analyzing data and data visualization? (motivation)\n\u2022   What is data science and how to start doing it in Python? (introduction of procedures, tools, most popular IDE-s for Python, etc.)\n\u2022   What tools for data analysis and data visualization Python offers? (in each stage of analysis the best libraries will be shown for the specific purpose; as for data visualization we will focus particularly on Bokeh, Seaborn, Plotly and use of Jupyter Notebook and Plotly)\n\u2022   How to \"unlock\" the insight hidden in data through Python and how to use it to transform not only public administration or business, but ultimately the transformation of the whole society and economy towards the insight & knowledge based? (potential of data science)\n\u2022   Open Data, Open Government Partnership, Open Public Administration & all the advantages of Open Data Science & Python. Data-Driven Approach. Everywhere. Now. (the end of talk +vision)\n \nPython as an Open Data Science tool offers many libraries for data visualization and I will show you how to use and combine the best. I strongly believe that power of data is not only in the information & insight that data can provide us, Data is and can be really beautiful and can not only transform our perception but also the world that we all live in.", "Learn how to Make Life Easier with Anaconda\n \nAnaconda is the leading platform for Open Data Science powered by Python. Although born within the Python community, Anaconda has expanded in scope to support both R & Python to enable Data Science as a team sport and to get stuff done.\nThis tutorial (aimed at novices) is tailored to get you started using Anaconda. If you're not already using Anaconda, bring your laptop and we'll get you set up. You don't need to have anything pre-installed, but it may be helpful to have Anaconda downloaded (or perhaps Miniconda, the lightweight version of Anaconda for faster downloading) before the tutorial.\nYou'll get an overview of the Anaconda ecosystem and a more in-depth discussion of conda and Anaconda Cloud. You'll have a chance to experiment hands-on with both conda and Anaconda Cloud:\n\n\nYou will create and manage tailored working environments sensibly with the help of conda.\n\n\nYou will use Anaconda Cloud to share Jupyter notebooks, codes, and environments with collaborators to ease the burden of working with other people's notebooks & scripts.\n\n\n \nBring your laptop & get started with Anaconda, the leading platform for Open Data Science powered by Python. You'll get hands-on experience using conda & Anaconda Cloud to share notebooks, packages, codes, & environments and to get stuff done. No software pre-installation required (although installing Anaconda beforehand may be helpful).", "Snakes on a Cloud: \"Data science, meet DevOps\"\n \nSnakes on a Cloud: \"Data science, meet DevOps\"\nRunning an always-on database service such as IBM Cloudant is a complex task. Accomplishing this requires a mix of development and operations informed by data.\nVarious data sources such as application logs, system metrics, and project management systems are used every day by Cloudant engineers.\nFirst I'll describe how DevOps works at Cloudant, and how data is used to meet our service needs. Break through the buzzwords and see the reality of a running service.\nI'll demonstrate a data processing engine called Forecast, which combines various sources at Cloudant for analysis. Forecast allows engineers to go beyond manual inspection by using Pandas and NumPy to analyze information. We'll see how we can realize new insights from our service metrics with tooling and automation.\nFinally we'll examine data using Jupyter Notebooks as provided by IBM Bluemix's data science platform. I'll walk us through visualization of Forecast's output and we'll learn how Bluemix can make data science simpler.\n \nDiscover how IBM Cloudant engineers use data to inform development and operations every day! Sources such as Splunk and Graphite are pivotal for meeting our service needs. I will demonstrate a data processing engine based on Pandas and NumPy that can help tame these sources. Then discover how you can use Jupyter notebooks with IBM's Bluemix platform to do more with data.", "Don't Live Patch Your CPython Interpreter\n \nComing Soon!\n \nComing Soon!", "Closing the Machine Learning Loop using Python\n \nClosing the Machine Learning Loop using Python\nThe What Section:\n\nWhat is Machine Learning?\nWhat does that mean for programmers?\nWhat is a feedback loop?\n\nThe How Section:\n\nPlanning for improvement\nArchitecture\nKeeping Track\nWhat is and is not important\nEvaluating behavior\n\nExample\n\nWalk through example\n\nQ&A\n \nThis session provides insight into developing machine learning systems that improve through closing the feedback loop, re-evaluate accuracy and generating more intelligent output given each iteration of data.", "Practical Word2vec in Gensim\n \nWe will walk through code that solves a practical problem for a restaurant review site - a user wants to see reviews covering all aspects of the restaurant(food, restaurant, price etc). We will load, clean and pre-proces a large amount of Yelp Restaurant reviews and see which reviews are similar even though they have no words in common.\nLaptop with gensim and word2vec Google News model is required (2Gb download). More details on github\n \nA hands-on introduction to the Natural Language Processing open-source library Gensim from its maintainers. This is not a theoretical maths talk - we will walk through the main function calls and discuss which business problems can be solved by word2vec and related techniques like Word Movers Distance.", "Using Python with Hadoop  to create production ready big data applications\n \nFor practical and economical reasons the open source Apache Hadoop ecosystem has become the de facto solution for Big Data and machine learning at scale.\nAlthough Hadoop is extremely powerful for analysing data, it was initially designed as an engineer's tool and allowing non technical user to interact with it can be challenging. Many business intelligence solutions coming from the RDBMS era keep failing the Hadoop test.\nIn this talk we are proposing to walk trough a practical example, demonstrating how to combine Python and Hadoop in order to create production grade big data applications.\nWe will start by going through some of the APIs that allow Python to be used as the glue to hold the different components of the Hadoop ecosystem together.\nMoving on to visualisation and dashboarding. We will demonstrate how to use some of the most popular Python libraries such as Bokeh, Matplotlib in a big data context and for more data intensive jobs how to produce json output that can be consumed by javascript.\nFinally we will show an example of Flask based web application running on Hadoop/Spark.\n \nA practical end to end walk through on how to use Python for creating an Hadoop production pipeline for big data:\nData analysis - Python APIs for Hadoop Streaming, Pyspark...\n Interactive visualisation and reporting dashboards - Bokeh, Matplotlib... and D3\n* Python web service to run Hadoop/Spark based applications.", "Scalable Data Science in Python and R on Apache Spark\n \nIn the world of Data Science, Python and R are very popular. Apache Spark is a highly scalable data platform. How could a Data Scientist integrate Spark into their existing Data Science toolset? How does Python work with Spark? How could one leverage the rich 10000+ packages on CRAN for R?\nWe will start with PySpark, beginning with a quick walkthrough of data preparation practices and an introduction to Spark MLLib Pipeline Model. We will also discuss how to integrate native Python packages with Spark.\nCompare to PySpark, SparkR is a new language binding for Apache Spark and it is designed to be familiar to native R users. In this talk we will walkthrough many examples how several new features in Apache Spark 2.x will enable scalable machine learning on Big Data. In addition to talking about the R interface to the ML Pipeline model, we will explore how SparkR support running user code on large scale data in a distributed manner, and give examples on how that could be used to work with your favorite R packages.\nPresentation:\nhttps://www.slideshare.net/felixcss/scalable-data-science-in-python-and-r-on-apache-spark\n \nIn the world of Data Science, Python and R are very popular. Apache Spark is a highly scalable data platform. How could a Data Scientist integrate Spark into their existing Data Science toolset? How does Python work with Spark? How could one leverage the rich 10000+ packages on CRAN for R?", "KEYNOTE: Automated factchecking in the era of fake news\n \nFactchecking is just one solution to the multifaceted problem of fake news. The factcheckers fight is valiant but how can they keep up in such tumultuous times? Join Full Fact, the UK's independent factchecking charity, to discuss how they plan to make factchecking dramatically more effective\nwith technology that exists now.\n \nJoin Full Fact, the UK's independent factchecking charity, to discuss how they plan to make factchecking dramatically more effective with technology that exists now.", "Feature Importance and Ensemble Methods : a new perspective\n \nI - Feature importance in ensemble algorithms - state of the art\n1) Feature importance in sklearn/xgboost : basically counts the occurrences of a feature in all the weak learners\n2) Construction of the trees in xgboost : if the trees are deep enough, every feature is going to be used\n3) Global feature importance is a misleading : a given feature might be critical for a given subpopulation but completely irrelevant for another (ex : multi-class classification)\nII - Xgboost real feature importance\n1) Prediction influence : first splits influence the prediction more than last splits, so the importance of a feature must be weighted by the discrimination it provides\n2) Point-to-point feature importance : following the path of a given prediction, it is possible to weigh the importance of every used feature\n3) A relevant assessment of feature importance : explanation of a given prediction, and aggregation on a set of data points\nIII - Implementation and examples\n1) Point-to-point feature importance illustration and implementation explanation\n2) Evolution of feature importance with respect to learning iterations\n3) Noisy variables cancellation\nIV - Limits and ways forward\n1) A word on correlated variables\n2) Is there a compromise performance/interpretation ?\n \nEnsemble methods are extremely performant in terms of prediction, but lack easy interpretation. Feature importance is not only counting up how many times a feature has been used in a weak learner, but also by how much this feature contributes to the result. Detailed example and implementation are provided in a jupyter notebook in python for the library \"xgboost\" of extreme gradient boosting.", "Patsy: The Lingua Franca to and from R\n \nCreating linear and logistic models in R is dead simple. If your numpy/panda-fu isn\u2019t all that great than it\u2019s a lot harder to do in Python. In R, for instance, you can declare a model with a formula as simple as y ~ x1 + x2. But in Python, you have to split out your target and input variables and make sure that the matrices work within the scikit-learn API. \nIn this talk I will introduce the Patsy package for describing and creating statistical models in Python. I\u2019ll walk through how to implement a logistic regression with Patsy and scikit-learn and I\u2019ll emphasize Patsy as a bridge for those who want to better understand Python and/or R.\n \nHow to build R-like statistical models in Python with Patsy and scikit-learn.", " Getting started with H2O on Python  \n \n\n \nH2O helps Python users make the leap from single machine based processing to large-scale distributed environments. Hadoop lets H2O users scale their data processing capabilities based on their current needs. Using H2O, Python, and Hadoop, you can create a complete end-to-end data analysis solution. In this presentation the speaker will show - and highlight a start to end process of how to design an algorithm, optimize, and implement it using H2O. The use case will focus much on the security work done between H2O and Capital One.", "Keynote: Data, Decision Making, and Being Human\n \nIn this talk, I will weave together a personal data story, how it led me to be an advocate for the importance of mentoring others, and why mentoring matters for healthy, inclusive, and productive communities.\nThrough looking at data, both quantitative and qualitative, I'll draw some life conclusions from the data. Some of these conclusions will fall prey to the failures of bias, small sample size, and inadequate data cleaning. From these data failures, I'll share valuable lessons which shaped my five keys of \"high impact\" mentoring:\n\nShow up\nListen\nQuestion\nAct\nRecharge\n\nWe'll wrap up with a critical look at how mentoring benefits you personally while also bringing more humanity to our data science community and the important work we do.\n \nOur precious data. We are collecting more than ever before. Today, we're analyzing large amounts. The results tell a story and imply truth. In our rush to validate our beliefs, we often experience \u00e2\u0080\u009cData Fail\u00e2\u0080\u009d. Want to make a lasting impact on others while building your own skills? Share your lessons from failure and be a mentor. You may be surprised by your impact on humanity .", "It's Not Magic: Explaining Classification Algorithms\n \nAs organizations increasingly make use of data and machine learning methods, people throughout those organizations need to build a basic \"data literacy\" in those topics. In this session, data scientist and instructor Brian Lange provides simple, visual and equation-free explanations for a variety of classification algorithms, geared towards helping anyone understand how they work. He also shows how the concepts explained can be pulled off using Python library Scikit Learn in a few lines. The talk will cover SVMs, k-Nearest Neighbors, decision tree learners, and ensemble and boosting models and maybe more as time allows. He can also explain how to pick which method to start with, depending on your problem. \n \nAs organizations increasingly make use of data and machine learning methods, people must build a basic \"data literacy\". Data scientist & instructor Brian Lange provides simple, visual & equation-free explanations for a variety of classification algorithms geared towards helping understand them. He shows how the concepts explained can be pulled off using Python library Scikit Learn in a few lines.", "Building domain specific databases using Python for prototypes during take-off\n \nAt Smyte we're solving infrastructure-heavy problems with a tiny engineering team. We've had to build custom databases in order to efficiently solve problems, but don't have the luxury of a ton of development time. In this talk I'll go through how we used Kafka & Python to implement a prototype Sliding HyperLogLog server. It held up for the six months we needed to, and some early decisions made it trivial to port the server to our new efficient C++ & RocksDB stack.\n \nSmyte is blocking fraudsters, spammers, scammers and harassers through analysis of web and mobile event data. There are hundreds of new database options cropping up, and while most of them are great they all make a range of different design choices which may/may not align with what you need. ", "Dealing with TBytes of Data in Realtime\n \nData processing often splits into two disjunct categories: Classic access to RDBMS with SQL and ORMs is well-understood and convenient, but often scales poorly after considerable GBytes of data. Big data approaches are powerful, but  complex to set up and to maintain. In a test setup we tried a compromise of both: What happens if you glue more than 1000 single SQL databases into a huge cluster?\nThanks to access to an unused IaaS cluster, we had the opportunity to research the behavior of many nodes clustered together. Data loading becomes a real challenge, while maintenance and monitoring such a drove of containers was no longer possible manually. We investigated the effect of changing container-vm-ratios. For our experiments, we used Crate, an open source, highly scalable, shared-nothing distributed SQL database, that comes with Python client connectors and support for several ORMs.\nWe share unexpected experiences about data schema design with the attendees, will explain some tweaking options that turned out to be effective, and would like to campaign for more open data projects.\n \nData processing often splits into two disjunct categories: Classic access to RDBMS is well-understood, but often scales poorly after considerable GBytes of data. Big data approaches are powerful, but  complex to set up and to maintain. In a test setup we tried a compromise of both: What happens if you glue more than 1000 single SQL databases into a huge cluster? We learned a whole lotta lessons!", "Bayesianism and Survival Analysis\n \nIf you come from a traditional statistical or econometric background, Bayesianism can seem like a totally new and different way of approaching statistics.  It can feel like there is no middle ground, you either need to commit to it entirely or move on.  \nBut neither of these are the case.  Rather than having to entirely replace our models with Bayesian ones, we can augment particular parts of the models using Bayesian techniques.\nBy the end of the tutorial you will be able to do exactly this when looking at modelling life time value or expected life times.  \nThe tutorial will focus on modelling the effectiveness of advertising as this is something that affects a wide range of businesses, but the techniques we cover are widely applicable. \nWe will begin by looking at how Bayesian ways of thinking can be very useful in helping flag up when we are violating the core assumptions of our models.  We will then look at how Bayesian techniques can be used to essentially provide modelling short cuts, i.e. how they allow us to do some things much more easily than we would otherwise be able to do them. \n \nAt first encounter Bayesianism can easily seem like a totally new and intimidating way of doing statistics.  However, in this tutorial you will learn that you don\u00e2\u0080\u0099t need to make a big commitment to gain real benefits from using some Bayesian techniques.  We will model the effectiveness of advertising using survival analysis and you will see how Bayesian tools can complement traditional models", "Using Scattertext and the Python NLP Ecosystem for Text Visualization\n \nCheck out the video of the talk: https://www.youtube.com/watch?v=H7X9CA2pWKo.\nNotebooks and presentation for this talk are available from https://github.com/JasonKessler/Scattertext-PyData.\nMotivation and introduction\n\nWhat's the matter with word clouds?\nHow to read a plot made by Scattertext\n\nHow to make your own plots\n\nPreparing a Pandas data frame with your data set\nPlotting with Scattertext, and fine tuning plots for interpretability and speed\n\nScattertext and the Python NLP ecosystem\n\nVisualizing emotions using Empath.\nUsing word vectors from spaCy and elsewhere see how topic-specific language differs.\nVisualizing topic models from scikit-learn.\n\nLinks\n\nSource code for the package is hosted on Github at github.com/JasonKessler/scattertext.\nFor more information, please see the paper which will appear as a 2017 ACL Demo here.\n\n \nScattertext is a Python package that lets you compare and contrast how words are used differently in two types of documents, producing interactive, Javascript-based visualizations that can easily be embedded into Jupyter Notebooks.  Using spaCy and Empath, Scattertext can also show how emotional states and words relating to a particular topic differ.", "Bayesian optimisation with scikit-learn\n \nChoosing the right parameters for a machine learning model is almost more of an art than a science. Proper model selection plays a huge part in the performance of a machine learning model. It is remarkable then, that the industry standard algorithm for selecting model parameters, is something as simple as random search.\nRandom search works well when we can sample the validation loss cheaply. However, in some settings it can sometimes take on the order of hours, or maybe even days, to get a single sample from the validation loss. In those cases, it feels very wasteful to not guide our search by previous samples.\nBayesian optimisation uses Gaussian processes to model the loss function. Using the Bayesian paradigm, we can then update our model, when we have computed the validation loss for a new set of parameters. I will talk you through the inner workings of the algorithm, so that we can better understand how it explores the parameter space.\nBut how do we use this algorithm in our day to day work? I'll briefly show you how you can implement this algorithm on top of scikit-learn, and how you can apply it to tune a machine learning model. I'll also discuss some common gotcha's and other things to watch out for.\n \nAlways wondering why your models are not winning any Kaggle competitions? Randomly guessing model parameters might work for some, but not everybody gets that lucky! In this talk, we'll look at a way of optimising machine learning models, than random search.. We'll build up some intuition about how Bayesian optimisation with Gaussian processes works, and how we can implement it using scikit-learn.", "Analyzing code contributions to the CPython project using NetworkX and Matplotlib.\n \nI analyze cooperation on the CPython project by analyzing the code contributions that each participant on the project has done through time. I model these contributions as a succession of bipartite networks where the bipartite node sets are contributors and source code files; each contributor is linked to the source files to which they have contributed, weighted by the number of lines of source code added. Analyzing the structure of these networks using NetworkX and Matplotlib I found an hierarchy of nested groups of developers that corresponds to the developers that do most of the code contributions in the CPython project. This hierarchy, on the one hand, reflects the empirically well established fact that in FOSS projects only a small fraction of the developers account for most of the contributions. And, on the other hand, refutes the naive views of early academic accounts that characterized FOSS projects as a flat hierarchy of peers in which every individual does more or less the same. I argue that the structure of CPython's cooperation network can be characterized as an open elite, where the top levels of this hierarchy are filled with new individuals at a high pace. This feature is key for understanding the mechanisms and dynamics that make FOSS communities able to develop long term projects, with high individual turnover, and yet achieve high impact and coherent results as a result of large scale cooperation.\nYou can download the slides for this talk from https://github.com/jtorrents/thesis/blob/master/presentations/pydata_bcn/cpython_code_contributions.pdf\n \nIt's a well established fact that only a small fraction of developers account for most code contributions to FOSS projects. The CPython project is not an exception, but analyzing code contributions through time reveals that the people that contribute the most is not always the same. I model CPython's contribution dynamics as cooperation networks and analyze them using NetworkX and Matplotlib.", "A word is worth a thousand pictures: Convolutional methods for text\n \nThe go to architecture for deep learning on sequences such as text is the RNN and particularly LSTM variants. While remarkably effective, RNNs are painfully slow due their sequential nature. Convolutions allow us to process a whole sequence in parallel greatly reducing the time required to train and infer. One of the most important advances in convolutional architectures has been the use of gating to concur the vanishing gradient problem thus allowing arbitrarily deep networks to be trained efficiently. \nIn this talk we'll review the key innovations in the  DenseNet architecture and show how to adapt it to text. We'll go over \"deconvolution\" operators and dilated convolutions as means of handling long range dependencies. Finally we'll look at convolutions applied to [translation] (https://arxiv.org/abs/1610.10099)  at the character level. \nThe goal of this talk is to demonstrate the practical advantages and relative ease with which these methods can be applied, as such we will focus on the ideas and implementations (in tensorflow) more than on the math.\n \nThose folks in computer vision keep publishing amazing ideas about you to apply convolutions to images. What about those of us who work with text? Can't we enjoy convolutions as well? In this talk I'll review some convolutional architectures that worked great for images and were adapted to text and confront the hardest parts of getting them to work in Tensorflow .", "Julia Tutorial\n \nJulia is a programming language oriented around the just in time (JIT) compiler technology. This tutorial will be an introduction to Julia and the core concepts that make it a good choice for certain types of problems.\nWe will cover the following topics (and probably more):\n\nBasic Julia syntax\nTypes/Multiple Dispatch\nPackages\n\"Pro tips\" for good performance\n\n \nJulia is a programming language oriented around the just in time (JIT) compiler technology. This tutorial will be an introduction to Julia and the core concepts that make it a good choice for certain types of problems.", "More than Words: Business Applications of Recurrent Neural Networks\n \nRecurrent neural networks have recently achieved spectacular results in many different natural language processing tasks, but their utility in more practical applications is largely undocumented. For companies that follow a subscription business model, sequential data can often be found in abundance, seemingly making recurrent neural networks a perfect fit for many prediction tasks. Unfortunately, resources describing how to leverage the power of recurrent neural networks in non-language settings are generally lacking. At Red Hat, we\u00e2\u0080\u0099re using recurrent neural networks to tackle a number of different business goals, including predicting customer churn and prioritizing support cases. During this talk, you\u00e2\u0080\u0099ll learn about Red Hat\u00e2\u0080\u0099s full deep learning pipeline, from data collection (e.g., mining website logs on Hadoop clusters and pulling data from SQL databases), to data preprocessing (e.g., dimensionality reduction in scikit-learn), to prediction. By the end, you\u00e2\u0080\u0099ll have the foundation necessary to begin implementing your own recurrent neural network solutions.\n \nRecurrent neural networks have recently achieved spectacular results in many different natural language processing tasks, but their utility in more practical applications is largely undocumented. During this talk, you\u00e2\u0080\u0099ll learn about how Red Hat is leveraging the power of recurrent neural networks to make informed business decisions from sequential customer data.", "Mind the Gap! Bridging the pandas \u00e2\u0080\u0093 scikit-learn dtype divide\n \nScikit-Learn typically operates on NumPy arrays, which are homogeneous (have a single data type).\nOn the other hand, pandas DataFrames are heterogeneous and may contain columns with different data types.\nAdditionally, pandas has implemented several extension dtypes like Categoricals and datetimes with timezones, that can't be stored natively in NumPy arrays.\nUsers of these libraries must be careful when crossing from pandas types to NumPy types.\nWe'll introduce the two systems, noting the vast area of agreement where pandas reuses NumPy's type system.\nHowever, the interesting cases are the relatively small areas where they differ. \nNext, we'll look at methods for converting from a pandas extension dtype to something suitable for scikit-learn.\nDepending on the statistical properties of your problem, one of several options may be appropriate.\nWe'll cover factorization and dummy (one-hot) encoding.\nFinally, we'll implement a custom scikit-learn Transformer for use in pipelines.\n \nThis talk briefly introduces the two different data models used by Scikit-Learn (NumPy arrays) and pandas DataFrames.\nWe see why this can cause problems for users of these libraries.\nFinally, we discuss strategies for managing the differences.", "Community sustainability in Wikipedia: a review of research and initiatives\n \nIntro\n\nPersonal introduction\nWikipedia is both an encyclopedia and a community\n\nA brief history of the Wikipedian community\n\n2001-2004: a small, tight-knit community with few explicit rules\n2004-2007: massive popularity, many new problems, many new policies\n2007-2011: post-peak, the rise of algorithmic governance\n2011-present: the Wikimedia Foundation\u00e2\u0080\u0099s expanded roles\n\nIssues that Wikipedia faces:\n\nManaging encyclopedic quality at scale when anyone can contribute\nRetaining and socializing newcomers\nSystemic bias and inequalities in participation by underrepresented groups\nRelationship between professional staff and volunteer community\n\nInitiatives:\n\nLocal and/or thematic \u00e2\u0080\u0098edit-a-thons\u00e2\u0080\u0099 for newcomers and marginalized groups\nA/B testing new automated rejection messages sent to newcomers\nNew interfaces for supporting mentoring and productive socialization\nNew spaces for newcomers to learn from veterans and each other\nMaking more low-risk ways for readers & newcomers to contribute\nEngaging community members in machine learning classifiers (for spam, etc )\n\nBroader lessons learned and food for thought\n\nEven the best planned, most well-intentioned initiatives can fail\nMaking governance more modular, rather than site-wide changes\nMaintaining healthy relationships between volunteers and professional staff\n\n \nWikipedia relies on one of the world\u00e2\u0080\u0099s largest open collaboration communities. Since 2001, the community has grown substantially and faced many challenges. This presentation reviews research and initiatives around community sustainability in Wikipedia that are relevant for many open source projects, including issues of newcomer retention, governance, automated moderation, and marginalized groups.", "Accelerating Python Analytics by In-Database Processing\n \nThe Python ecosystem is very rich and provides intuitive tools for data analysis. However, most Python libraries require the data to be extracted from the database to working memory and resources are limited by computational power and memory. Analyzing a large amount of data is often impractical or even impossible. Ibmdbpy is an open-source Python package, developed by IBM, which provides a Python interface for data manipulation and machine learning algorithms such as Kmeans or Linear Regression to make working with databases more efficient by seamlessly pushing operations into the underlying database for execution. This does not only lift the memory limit of Python, but also allows users to profit from performance-enhancing features of the database management system. Ibmdbpy is designed for IBM DB2 and IBM dashDB, a database system available on IBM BlueMix, the IBM cloud application development and analytics platform. Via remote connection, user operations can benefit from in-database specific features, such as columnar technology and parallel processing, without having to interact with the database explicitly. Some in-database functions additionally use lazy loading to load only parts of the data that are actually required to further increase efficiency. Keeping the data in the database also avoids security issues that are associated with extracting data and ensures that the data that is being analyzed is as current as possible. Ibmdbpy can be used by Python developers with very little additional knowledge, since it imitates the well-known interface of Pandas library for data manipulation and Scikit-learn library for machine learning algorithms. Ibmdbpy provides a great runtime advantage for operations on medium to large dataset, i.e. on tables that have 1 million rows or more. Providing a Python interface for databases allows to bridge the gap between data warehousing platform and end-user environment, so that developers can benefit both from the expressivity of Python and from the speed-up provided by SQL execution in the database, which can be run on a cluster. In this talk, we will show the advantages of such approach for scaling Python analytics and do a short demo of data analysis with ibmdbpy.\n \nPython Analytics can be accelerated using SQL- Pushdowns to benefit from in-database performance-enhancing features such as column-stores and parallel processing. We will show the benefits of this approach and present ibmdbpy, a prototype which provides a Python interface for data manipulation and in-database algorithms in IBM DB2 and IBM dashDB on Bluemix .", "KEYNOTE: Laser ranging in a new dimension\n \nGravitational waves are a prediction from Einstein's theory of gravity: when very compact and massive objects such as black holes collide, they produce strong gravitational waves, but until recently we did not have an instrument sensitive enough to measure them. In September 2015 the LIGO detectors achieved the first detection of a gravitational wave and could estimate the parameters of the black holes that had produced the signal, a billion years ago in a galaxy far away. LIGO and other detectors will now be improved further to detect more signals from black holes and other elusive objects, kickstarting a new type of astronomy.\nAt the core of the LIGO observatories are very large laser interferometers. The concept for these interferometers is well known but has been enhanced with new technology. It typically takes several years of work after the interferometers have been first turned on, until they reach their full sensitivity. Numerical simulations play an important role in the process. In recent years we have developed new Python-based tools to model the optical systems. Such tools must remain accessible to the experimentalists in charge of the instruments, to be used effectively in large collaborations.\nI will give a short introduction to the LIGO project and present examples from our work to support the LIGO detectors with numerical modelling tools. I will mention how the evolution of our work over the last ten years led to Python as a language of choice. \n \nRecently the LIGO project announced the first detection of a gravitational wave. This achievements is a triumph for experimental physics, after decades of effort on developing an instrument sensitive enough to react to the tiny push and pull from a gravitational wave. I will present examples from our work on modelling and understanding the complex laser interferometers of LIGO.", "Provenance for Reproducible Data Science\n \nIn science, results that are not reproducible by peer scientists are valueless and of no significance. Good practices for reproducible science are to publish used codes under Open Source licenses, perform code reviews, save the computational environments with containers (e.g., Docker), use open data formats, use a data management system, and record the provenance of all actions. \nThe provenance of data provides detailed information about the origin of that data. That includes information about ownership and both actions and modifications performed on the data. With provenance information, data will be traceable and users can be confident in quality of the data. To specify and store provenance information, W3C has standardized the provenance model PROV. Using PROV and associated implementations, users can record provenance of data analytics processes. The provenance information are directed acyclic graphs that can be analysed to get insight into the data analytics processes.\nThe talk covers\n * Introduction to provenance and PROV\n * Modelling provenance for data processing \n * Python APIs for provenance recording\n * Provenance recording for Jupyter notebooks\n * Storing provenance in graph databases\n * Analysis of provenance information\n \nIn science, results that are not reproducible by peer scientists are valueless and of no significance. Among other practices, recording the provenance of data facilitates to reproduce results in data science and users can be confident in quality of the data. The talk shows how to record and to analyse provenance using the provenance model PROV for Python data analytics processes.", "Static Type Analysis for Robust Data Products\n \nAs a dynamically typed language, Python is an extremely flexible tool that allows to write code quickly and concisely. This flexibility makes Python a popular tool for R&D and prototyping, but what about bringing Data Science in production? When comparing Python to statically typed languages, one of the downsides is that many type-related errors are not captured until runtime.\nThis talk discusses the steps taken by the Python community to promote static type analysis, in particular the semantic definition of type hints and the adoption of mypy as type checking tool. \nThe audience will learn about static typing for Python, its pros and cons, and how to adopt static type analysis in your workflow. Since the focus is on building and deploying data products, static type analysis is discussed as a mean to improve the robustness of your data products.\n \nThis talk discusses static type analysis applied to Python data products, its pros and cons, and overall how to adopt type checking tools (i.e. mypy) in your workflow.", "TensorFlow Wide & Deep: Advanced Classification the easy way\n \nDeep learning has already revolutionized machine learning research, but it remains out of reach for many developers. However, tools already exist today that enable leading-edge machine learning for many problem domains.\nIn this talk, we will go on an adventure to build a machine learning model that combines the benefits of linear models with deep neural networks. You will also gain some intuition about what is happening under the hood, and learn how to use this model for your own datasets!\nTo accomplish this, we will use TensorFlow, an open-source machine learning library with a full Python interface. It has become the most popular machine learning library on GitHub, and the community around it is growing rapidly.\n \nIn this talk, we will go on an adventure to build a machine learning model that combines the benefits of linear regression models with deep neural networks. You will also gain some intuition about what is happening under the hood, and learn how you can use this model for your own datasets.", "On Bandits, Bayes, and swipes: gamification of search\n \nThe first part of the talk is about active learning:\n\nWhat is it?\nHow does it work?\nWhat are the different flavours of active learning?\nDoes it help me solve my problem?\n\nThe second part presents a case study where we developed an engaging and fun way to search for your dream car.\nThe case study\n- uses an intuitive interface (swipes),\n- actively and efficiently explores and learns the user's preferences (with multi-armed bandits, an active learning algorithm),\n- incorporates priors (clustering and Bayes to avoid popularity biases)\n- is scalable.\nThe talk is less about specific tools and libs - even though everything was done with python\u2019s usual suspects (numpy, scipy, sklearn, flask, etc.) - but how to make active learning work for you.\n \nThe talk will show how to use active learning to work with Small Data. Active learning is an underappreciated subfield of ML where the algorithm actively gathers labeled data, e.g. it can query the user for the most informative data. I will discuss the basics of active learning theory, and look at a case study showing how to use active learning and tailor it to a practical problem.", "Building a Data-Driven Dialogue: From Filling Potholes to Disrupting the Cycle of Incarceration\n \nBuilding a Data-Driven Dialogue: From Filling Potholes to Disrupting the Cycle of Incarceration\n \nBuilding a Data-Driven Dialogue: From Filling Potholes to Disrupting the Cycle of Incarceration", "Integrating Scala/Java with your Python code\n \nThis talk will:\n\n\nLook in detail at the Python Spark bindings (PySpark) to show how Python-JVM interop used by Apache Spark actually works.\n\n\nDemonstrate a package that we have designed (spylon) which generalizes the above operations (Scala class reflection, type conversions etc.).\n\n\nExplain drawbacks and pitfalls of our approach. \n\n\nThe examples will be focused on PySpark and Scala but the concepts generalize to use with any other architecture where Python support is not yet fully available (e.g., Cassandra, ElasticSearch, etc.) or any internal JVM Projects.\nThe talk will not require any knowledge of Scala or Java from the audience.\n \nOccasionally Python-focused data shops need to use JVM languages for performance reasons. Generally this necessitates throwing  away whole repositories of Python code and starting over or resorting to interop architectures (e.g., Apache thrift) which increase system complexity.  We provide a technique (and a new library, spylon) for interoperating easily with the JVM from Python. ", "Keynote: Using Data Science for Social Good: Examples, Opportunities, and Challenges\n \nComing Soon!\n \nComing Soon!", "Searchable datasets in Python: images across domains, experiments, algorithms and learning\n \nIntroduction\nImage capture turned into an ubiquitous activity in our daily lives, but mechanisms to organize and retrieve images based on their content are available only to a few people or to very specific problems. With the significant improvement in image processing speeds and availability of large storage systems, the development of methods to query and retrieve images is fundamental to simple human activities like cataloguing and complex research such as synthesizing materials. Content-Based Image Retrieval (CBIR) systems use computer vision techniques to describe images in terms of its properties in order to search similar samples given an image itself as a query, instead of keywords. For this reason, the system works independently of annotations, which can be time consuming and impossible in some scenarios, e.g. high-throughput imaging instruments.\nWhile much work in CBIR has targeted ads and recommendation systems, our pyCBIR allows general purpose investigation across image domains and experiments. Also, pyCBIR contains different distance metrics, and several feature extraction techniques, including a Convolutional Neural Network (CNN).\nProposed Methodology\nWe proposed a CBIR tool using python program language called pyCBIR. This tool is composed by six feature extraction methods and ten distances (see Figure 1). Searches occur based on a single image (or a set of images) as query, then pyCBIR retrieves and rank the most similar images according to the user selected parameters.\n\nFigure 1. Flowchart of the proposed methodology: https://vis.lbl.gov/~daniela/2016/pyData/cbir.png\nRegarding the feature extraction methods in Figure 2, pyCBIR calculates the following sets of attributes: Gray Level Co-occurrence Matrix (GLCM), Histogram of Oriented Gradients (HOG), First Order Texture Features (FOTF), Local Binary Pattern (LBP). We also implemented two CNN-based schemes for image characterization. The first scheme uses a CNN without the last layer (classification layer), retaining the convolution results as features - this is a common approach among new CBIR systems. The second scheme proposes the use of the class probabilities as descriptor, an original contribution of our work, which also achieved competitive results in comparison to the other feature extraction methods - we call this scheme CNN with probabilities or CNNp. For example, let DB be a database with 5 classes, the CNNp will return 5 probabilities for each DB image, which will be the DB feature vectors. For each retrieval image (RI), we will use the trained CNNp to compute the RI probability. Next, we can compute  different distances, as listed in Figure 2, between feature vectors and return the most similar. \n\nFigure 2. Graphic interface: https://vis.lbl.gov/~daniela/2016/pyData/pyCBIR.png\nExperiments\nWe carried out several experiments using classical image databases for CBIR problems, such as: CIFAR-10, Describable Textures Dataset (DTD) and MNIST, as well as scientific datasets containing microscopic images. Current experiments pointed out that descriptors like HOG and LBP are very sensitive to the parameters choice, and the absence of parameters that work well in all databases. The proposed CNN scheme for feature extraction, CNNq, requires only two parameters: number of epochs and learning rate. These parameters showed less sensitivity, as we illustrated in Figure 3 and 4.      \n\nFigure 3. pyCBIR results for public dataset (DTD): https://vis.lbl.gov/~daniela/2016/pyData/textureDTD.png\n\nFigure 4. pyCBIR results for dataset of microscopic images: https://vis.lbl.gov/~daniela/2016/pyData/fiberMicroscopy.png\n \npyCBIR is a new python tool for content-based image retrieval (CBIR) capable of searching relevant items in large databases, given unseen samples. While much work in CBIR has targeted ads and recommendation systems, our pyCBIR allows general purpose investigation across image domains. Also, pyCBIR contains ten distance metrics, and six bags of features, including a Convolutional Neural Network.", "Classifying Search Queries without User Click Data\n \nTraditionally, search queries are classified into different categories by analysing user-behavior with the help of search logs. Instead of focussing on the search logs and by analysing queries with the help of machine learning models such as LSTM, it is possible to get a very decent model to classify the search queries. \nThis talk focuses majorly on LSTMs and their usefulness when it comes to search query classification. We also discuss how we can accurately classify search queries in hundreds of categories using open source data available online and how this can be combined with LSTMs to provide a much stable and better result.\n \nThis talk discusses how machine learning/data mining techniques can be applied to classify search terms that people use in search engines like Google, Bing, Yahoo etc. The talk focuses on machine learning techniques such as LSTM (long short term memory) rather than traditional ways like analysing user-behaviour with the help of their search logs. ", "Building Data Pipelines in Python\n \nThis talk discusses the process of building data pipelines, e.g. extraction, cleaning, integration, pre-processing of data, in general all the steps that are necessary to prepare your data for your data-driven product. In particular, the focus is on data plumbing and on the practice of going from prototype to production.\nStarting from some common anti-patterns, we'll highlight the need for a workflow manager for any non-trivial project.\nWe'll discuss the case for Luigi as an interesting option to consider, and we'll consider where it fits in the bigger picture of deploying a data product.\n \nThis talk discusses the process of building data pipelines, e.g. extraction, cleaning, integration, pre-processing of data, in general all the steps that are necessary to prepare your data for your data-driven product. In particular, the focus is on data plumbing and on the practice of going from prototype to production.", "Monitoring Displacement Crises with Python: A Humanitarian Project by Data for Democracy\n \nData for Democracy is a community of civic minded volunteer technologists, programmers and data scientists working on everything from understanding propaganda, to reducing urban traffic fatalities, to making election data available to the public. One of the main projects is focused on tracking online reports of internally displaced persons (IDPs) - people forced to flee from conflict or disaster but who remain within their original country of residence. The team has been been working in response to a call by the International Displacement Monitoring Centre, for solutions to the problem of collecting and analysing data from different news sources about situations involving IDPs.\nThe group has developed a Python back-end that scrapes web pages, extracts content, tags and filters articles by topic, and retrieves key information such as the number of people displaced. The solution makes full use of the spectrum of packages available in the Python toolkit, including newspaper to parse online articles,  gensim for powerful, efficient topic modelling, scikit-learn for article classification, and sqlalchemy for database handling. This talk will provide an overview to the technical approach used by the multidisciplinary and international team to tame the messy unstructured data and provide a prototype product that can be used by humanitarian analysts to monitor displacement crisis information.\nThe presentation will also highlight the challenges and successes that come with working in a group of volunteers spread across multiple timezones, disciplines, and experience levels, to create a data product for a sector that has traditionally been slow to make use of technology. Through the story of this project, the motivations and wider efforts of the volunteer-led Data for Democracy community will also be highlighted, showing the power that data practitioners have to make a difference.\n \nData for Democracy is a civic engagement hub, for volunteer data scientists to carry out work with social impact. This talk will focus on the work of one team, who have built a web scraper and natural language processing pipeline to track and analyse online reports of people displaced by conflict and disaster. It will also reflect on the challenges faced using data in the humanitarian sector.", "Machine Learning in Financial Credit Risk Assessment\n \nCredit Risk assessment is a general term used among financial institutions to describe the methodology used to determine the likelihood of loss on a particular asset, investment or loan. The objective of assessing credit risk is to determine if an investment is worthwhile, what steps should be taken to mitigate risk, and what the return rate should be to make an investment successful. \nBuilding a Credit Risk Prediction Model as accurate as possible becomes essential, as it allows the institution to provide fair prices to the customers while ensuring predictable and minimal losses. We build our Credit Risk Model by combining data gathered from the customer\u2019s application on our online platform with their credit history provided by different credit agencies. \nIn this talk, we will cover the research and development behind our recently created Credit Risk Model. We will discuss the definition of the target, the variable selection procedure, the different machine learning models built and how we optimise their hyper-parameters, as well us some of our latest research in model stacking and deep learning. \nOur development and Modelling pipeline is built in Python, using Pandas, Numpy, Scikit-Learn, XGBboost, Keras, Matplotlib and Seaborn. We combine the use of machine learning algorithms with data visualisation to better understand the variables and our customers, and to convey the message to different stakeholders within and outside the company.\nThroughout the talk, we will focus both on the intellectual rationale of the research and the utilisation of the different python tools to accomplish each task, highlighting both the problems encountered and the solutions devised.\n \nRisk management is paramount to any lending institution, allowing it to perform well-informed decisions while originating loans. In this talk, I will describe our research and development approach to build our Credit Risk Prediction Model. I will browse over our target definition, feature optimisation, model building and tuning and our experience with model stacking.", "Running jupyter notebook remotely in a docker swarm cluster\n \nThe solution is based on running a docker container with a browser and a VNC server. All the remote access to the notebooks is done using Apache Guacamole a clientless remote desktop gateway. Everything is running on a dynamic docker swarm cluster of 20 nodes. As a lateral effect, this solution it also allows a real-time collaboration between users in a way that multiple users can access at the same time the same desktop (but they have to fight for the mouse and the keyboard).\nhttps://github.com/jordeu/pytalks/tree/master/20170520_pydata_jupyter_in_a_docker_cluster\n \nThe current version of Jupyter Notebook computes the document state at the browser side, this is a problem if you run long jobs in a remote notebook from a laptop. If you close the browser you lose all the output of the current running cell. I will explain how  we solved this problem in our lab.  This solution it also allows a \"walkie-talkie\" like real-time collaboration.", "Kickstarting projects with Cookiecutter\n \nWriting a Python script from scratch is fairly straightforward if you have some experience working in Python. You can usually get by with very little boilerplate code. Starting a new Python project, however, can be tiring if you decide to stick to best practices and plan on submitting it to PyPI later on.  It requires great diligence and occasionally gets pretty cumbersome when if you're creating new tools on a regular basis.\nSo why not use a template for your projects?\nCookiecutter is a command-line utility that creates projects from templates. It is free and open-source software distributed under the terms of a permissive BSD-3 license. With around 150 individual contributors, more than 1000 public templates on GitHub, and multiple talks at conferences, it is fair to say that there is a solid community around it.\nIn this talk, I will give an introduction on Cookiecutter, how to install it, how to use it in the CLI, and finally how to author your own template. You can use Cookiecutter for all sorts of projects: command-line scripts, Django webapps, and even non-Python projects.\nThe community has authored and published several templates for Data Science projects, for instance widget-cookiecutter. I will demonstrate how to use Cookiecutter to create a custom Jupyter widget using that template.\nThe goal of this talk is to teach how to integrate Cookiecutter into your own workflow and share learnings in the form of templates with your team at work and the open source community.\n \nCookiecutter is a command-line utility that generates projects from templates.  You can use Cookiecutter to create new Python projects and to generate the initial code for different types of applications. In this talk, I will give an introduction to Cookiecutter, how to install it from PyPI, how to use it in the CLI, and finally how to author your own template.", "Keynote: How Open Data Science Opens the World of Innovation\n \nComing soon.\n \nInnovation today appears to be instantaneous in large part due to open source technology. Open Data Science is no exception. Python, a pillar in the Open Data Science bedrock, is well positioned to harvest innovation in software and with Anaconda, it\u00e2\u0080\u0099s also well positioned to capitalize on the latest hardware innovations. Anaconda and Intel are blazing a path for the Python community to take advantage of cognitive computing, including machine learning and deep learning. \nIn this keynote, Peter and Robert will talk about how Open Data Science\u00e2\u0080\u0093\u00e2\u0080\u0093a connected ecosystem of data, analytics and compute\u00e2\u0080\u0093\u00e2\u0080\u0093streamlines the path to high performance and innovation to achieve breakthrough results. ", "Let it shine - telling your data story\n \nYou work for weeks, maybe months, on a new analysis: collecting data, cleaning it, extracting your features and modeling it, and you finally come up with some meaningful results... What happens next? How many people dig into and explore your work and your results?\nBokeh is a data visualization library that gives you the power of custom interactive visualizations that you can publish on the web, but lets you build them in python.\nStaying in the language of your analysis, means that you can move seamlessly into presentation. In this talk I will:\n\ndiscuss how visualizations can be a tool to present your analysis in a way that is understandable to non-specialists without dumbing down your content\nshow how to build-up your data story with Bokeh including: an overview of Bokeh; how to get started with it; how to share your work; and, \nshowcase building up a sophisticated interactive graphic using Bokeh\n\nI have marked this talk as Novice, but it will have a mix of content. If you're comfortable in Python then I aim to leave you with some immediately useful skills to start presenting your data to a wider audience. If you're just getting started I hope this talk will give you tools to start thinking about how to communicate your work successfully as you move into data science.\n \nYou work for weeks on a new analysis, collecting, cleaning data, modeling it, you arrive at a new result. What happens next? How many people explore your results--and wider work? Bokeh is a data visualization library that gives you the power of web-based custom interactive visualizations, but lets you build them in Python. We'll talk about moving seamlessly from analysis to engaging presentation.", "Fizz Buzz in Tensorflow\n \nFizz Buzz is a ubiquitous, nearly trivial problem used to weed out developer job applicants. Recently I wrote a joking-not-joking blog post about a fictional interviewee who solves it using neural networks. After the blog post went viral, I spent a lot of time thinking about Fizz Buzz as a machine learning problem. It turns out it's surprisingly interesting and subtle! I'll talk about how and why. The talk will double as a nice introduction to TensorFlow.\nCurrent thinking as to outline of talk:\n- Introduction to Fizz Buzz\n- Introduction to TensorFlow\n- Feature Selection\n- Fizz Buzz via Logistic Regression\n- Fizz Buzz via Simple Multilayer Perceptron\n- Fizz Buzz via Deep Learning\n- What I Learned About Fizz Buzz\n- What I Learned About Tensorflow\n- What I Learned About Machine Learning\n \nFizz Buzz is a ubiquitous, nearly trivial problem used to weed out developer job applicants. Recently I wrote a joking-not-joking blog post about a fictional interviewee who solves it using neural networks. After the blog post went viral, I spent a lot of time thinking about Fizz Buzz as a machine learning problem. It turns out it's surprisingly interesting and subtle! I'll talk about how and why.", "Time Series for Python with PyFlux\n \nThis talk will introduce the PyFlux library for time series analysis in Python. I will walk through the modelling and inference options in an accessible, high-level way. I will focus on score-driven (GAS) models, which are a new flexible alternative to traditional time series models. This Python library represents the first comprehensive implementation of a GAS library, a model type that has the potential to be as widespread as ARIMA models. I will demonstrate the usefulness of the models through some fun examples in PyFlux, such as a power rating model for NFL football teams, as well as some finance examples, which I shall attempt to make fun (challenge accepted).\nThe preliminary outline of the talk will be as follows:\n\nBrief introduction to the PyFlux library\nIntroduction to GAS models for time series\nGAS models as an extension of ARIMA models (with examples)\nGAS models as an extension of state space models (with examples)\nInference options for GAS models (with examples)\nA simple GAS model for predicting NFL games\nConclusion\n\nAfter this talk, you will be able to:\n\nUnderstand GAS models, and their benefits versus traditional time series models.\nEstimate and run predictions with GAS models, using PyFlux\nUnderstand the different types of inference that can be used for time series problems, including variational inference.\nPerform multiple methods of inference on time series problems using PyFlux\n\n \nPyFlux is a new library for time series analysis for Python. It brings together a vast array of time series models, including recent models such as score-driven models and variational state space models,  as well a flexible choice of inference options, including black box variational inference. In this talk I will introduce some of the features, with some fun applications to sports modelling.", "Python and TouchDesigner for Interactive Experiments\n \nTouchDesigner is a visual programming software made by a company called Deriviative. I use this software with Python to make complex interactive installations which are hybrid of art and science. I use the same technique to make science experiments so the lab can test the validity of neurofeedback paradigms. I want to share some of the techniques I used in our brain driven installation, My Virual Dream, because I think there is huge potential to make very practical and very fantastical things. Quickly. This mixture of touch designer and Python could give scientists access to powerful audio visual tools usually associated with complex game engines or complex coding from scratch, in order to create limitless experiments. Even better, one can create usable modules to string into new experiments or share with eachother. \n \nI will dismantle the guts of an interactive multiplayer brain data driven installation we made last year. It uses Python modules to handle data from the users to create a new experiences every time. I will discuss the bennefits of using Python with visual programming to create very complex experiences quickly, which can be useful for interactive art and science experiments alike. ", "To the Web and Beyond\n \nWorking from the premise that the fruits of your data explorations find\ntheir natural home on the web, this talk will discuss a few modern\nwork-flows that deliver your data to the browser. I'll be aiming to\ndistil a few of the things I learned in the last year while writing the\nbook 'Data-visualisation with Python and JavaScript' and highlighting\nsome of the key changes that have occurred in the Python and JS dataviz\necosystems in that little time.  \nIf you take dataviz seriously I think you have to engage and make\nfriends with JavaScript. The good news for Pythonistas is that Modern JS\n(EcmaScript 6) has 'borrowed' more than a few of Python's clothes, which\nmakes moving between the two languages that much easier. I'll\ndemonstrate a few of these, showing bilingual development is getting\nless and less arduous.  \n \nWorking from the premise that the fruits of your data explorations find\ntheir natural home on the web, this talk will discuss a few modern\nwork-flows that deliver your data to the browser. I'll be aiming to\ndistil a few of the things I learned in the last year while writing the\nbook 'Data-visualisation with Python and JavaScript'.", "Designing for Guidance in Machine Learning\n \nMachine learning systems are getting better and better at tasks that we used to think only humans could be good at. I can write a program to look at an image of an animal and tell you whether it\u2019s a cat or dog, for example. The measure of my program\u2019s quality is its performance on new inputs: how many images does it classify correctly without ever having seen them before? How I accomplish that\u2014what exactly it is about the images that allows me to distinguish a cat from a dog\u2014is almost irrelevant compared to how well my program does on the task.\nBut what if I don\u2019t just want my system to be good at recognizing that you\u2019re showing it a picture of a cat, I also want to be able to give you clear instructions on the top n things you can do to turn that cat into a dog? Now all of a sudden I have to start thinking differently about the information that my model uses to do the classification. I need features that are still automatable (I can tell a computer how to measure them and use them as inputs to a machine learning algorithm) but are also explainable (a lay person can understand what they mean and how to change their values). I may also find issues in my training/test data that I might not have noticed if I hadn\u2019t had to do the same amount of introspection. \nIn this talk, I\u2019ll walk through a toy machine learning problem end-to-end, showing how our approach has to change when we add the requirement of producing actionable guidance. The content will be targeted at people who have an interest in machine learning, but experience isn\u2019t necessary.\n \nHow does the task of developing a machine learning system change when we not only have to predict outcomes from inputs, but also guide users to make their inputs better? Using practical examples in Python, I'll explore some of the lessons we've learned building an augmented writing platform at Textio.", "Scale out from the very beginning\n \nIn this talk the authors journey of making the pool of Dark Data available to teams with quite different goals is reflected, emphasising on creating a simple and robust set of tools matching each other and addressing the several needs of the teams based mainly on solutions such as dask distributed, dask based dataframes, bokeh and flask. \nThe key to success was to prevent structuring too much at the very beginning and postpone this task into the several projects of the users consuming the results of these services giving them the freedom to create and use their own models.\nIt is shown how we implemented a distributed filesystem scanning utility to crawl for data in our 1.5 PB storage system every night ending up in a simple, yet useful table of contents, and how this result set is processed further to fulfill all the project teams requirements.\nThese services are for example used to\n\nfind expensive duplicates of datasets\ncreate customer as well as product and service orientated views on the available data\nfind data suitable to test algorithms, software and procedures, and to derive current performance\nserve training and education material\nshow the usage frequency of the datasets to support an optimised data tiering process\n\nFinally the involved procedures helped to gain more awareness of the value the available data had, both helping to build more trust in Big Data based solutions and to reduce the volume of the data itself that is available online, which in turn keeps the corresponding costs at a reasonable rate.\n \nMost companies a very well aware of the potential behind Big Data solutions today and happily start collecting every piece of information building huge pools of Dark Data. How could Data Science teams create an initial overview on what's available? A simple search strategy, optimised and refined to scale could be a promising way to start.", "The Secret Life Of Rolling Pandas\n \nThe talk will cover:\n\nA quick overview of the Rolling and Expanding objects in pandas. Why straightforward implementations, even clever ones using advanced NumPy tricks, are inefficient.\nHow is Series.rolling().sum() implemented. How the same idea can be extended to higher dimensions with the help of the inclusion-exclusion principle to yield summed area tables.\nHow are Series.rolling().var() and friends implemented. The importance of numerically stable algorithms: why Welford's method is the better choice, even if it's a little slower. How can the same ideas be generalized to higher order moments, and used to parallelize computations.\nHow are Series.rolling().max() and .min() implemented. The beauty of clever algorithms. The use of specialized data structures: a ring buffer as a deque.\nHow are Series.rolling().median() and .quantile() implemented. More specialized data structures: the skip list, or why randomization can be your friend.\n\n \nPandas provides a powerful set of functions to compute statistics on rolling windows. We will go beyond the convenient interface, and peek at the algorithmic gems that implement these operations efficiently: summed area tables, Welford's method, skip lists, ring buffers, deques... will all get their minute of fame, and attendees may learn a thing or two they can use in their everyday coding.", "Evaluating Topic Models\n \nSupervised models are trained on labelled data and optimised to maximise an external metric such as log loss or accuracy. Unsupersived models on the other hand typically try to fit a predefined distribution to be consistent with the statistics of some large unlabelled data set or maximise the vector similarity of words that appear in similar contexts. Evaluating the trained model often starts by \"eye-balling\" the results, i.e. checking that your own expectations of similarity are fulfilled by the model.\nDocuments that talk about football should be in the same category and \"cat\" is more similar with \"dog\" than with \"pen\". Is \"cat\" more similar to \"tiger\" than to \"dog\"? Ideally this information should be captured in a single metric that can be maximised. Tools such as pyLDAvis and gensim provide many different ways to get an overview of the learned model or a single metric that can be maximised: topic coherence, perplexity, ontological similarity, term co-occurrence, word analogy. Using these methods without a good understanding of what the metric represents can give misleading results. The unsupervised models are also often used as part of larger processing pipelines, it is not clear if these intrinsic evaluation measures are approriate in such cases, perhaps the models should instead be evaluated against an external metric like accuracy for the entire pipeline.\nIn this talk I will give an intuition of what the evaluation metrics are trying to achieve, give some recommendations for when to use them, what kind of pitfalls one should be aware of when using topic models and the inherent difficulty of measuring or even defining semantic similarity concisely.\nI assume that you are familiar with topic models, I will not cover how they are defined or trained. I talk specifically about the tools that are available for evaluating a topic model, irrespective of which algorithm you've used to learn one. The talk is accompanied by a notebook at github.com/mattilyra/pydataberlin-2017\n \nUnsupervised models in natural language processing (NLP) have become very popular recently. Word2vec, GloVe and LDA provide powerful computational tools to deal with natural language and make exploring large document collections feasible. We would like to be able to say if a model is objectively good or bad, and compare different models to each other, this is often tricky to do in practice.", "JupyterLab: Building Blocks for Interactive Computing\n \nProject Jupyter provides building blocks for interactive and exploratory computing. These building blocks make science and data science reproducible across over 40 programming language (Python, Julia, R, etc.). Central to the project is the Jupyter Notebook, a web-based interactive computing platform that allows users to author data- and code-driven narratives - computational narratives - that combine live code, equations, narrative text, visualizations, interactive dashboards and other media. JupyterLab (still in alpha) goes beyond the notebook and provides a modular, extensible, and performant web application with a set of reusable components, including a notebook, console, text and code editor, and terminal. Users can arrange multiple notebooks, text editors, terminals, output areas, and custom components using panels, tabs, splitters, and collapsible sidebars. JupyterLab also provides a file browser, command palette and integrated help system. The codebase and UI of JupyterLab is based on a flexible plugin system provided by PhosphorJS that makes it easy to extend with new components. In this talk, we will demonstrate the JupyterLab interface, its codebase and recent developments, and describe how it fits within the overall vision of Project Jupyter.\n \nJupyterLab is an extensible web application (still in alpha) for scientific computation. Users arrange multiple notebooks, editors, terminals, and custom components with tabs, splitters, and sidebars. JupyterLab also has a file browser, command palette and quick help system. We demonstrate JupyterLab and recent developments and show how JupyterLab fits in the vision of the Jupyter project.", "Reach More People: SMS Data Collection with RapidPro\n \nWhen participants arrive, we will have an SMS poll ready for them to take via their phones or their Twitter accounts and they can choose to participate.\nIntro: What is RapidPro?\nSMS is the number one tool used globally to access information. We'll give a quick tour of the RapidPro SMS web application to show users what it does.\nHow is SMS used in 2016?\nConnect communication channels: Communicate with users over SMS or via social media channels.\nBuild workflows: A beautiful GUI lets anyone create complex workflows without need for custom web development.\nManage survey audience: A large toolkit for managing contacts.\nCollect & analyze data: All data is available for export via the API.\nDeep Dive\nDuring this part of the talk, we\u00e2\u0080\u0099ll give a detailed dive into how we built each step of the poll we used in class with RapidPro.\nWe\u00e2\u0080\u0099ve used interesting examples in the live demo poll which provide a great way to learn about RapidPro\u00e2\u0080\u0099s extensive functionality.\nThis section of the talk will reflect on a lot of what we've already learned during the introduction and give a more detailed description for each component of RapidPro's functionality.\nAPI & Dashboards\nAn overview of the RapidPro API with examples of why to use it\nAn overview of 2 applications that we\u00e2\u0080\u0099ve built that use the RapidPro API to create data visualizations\nHosting RapidPro\nIdeas to get you started using RapidPro \nWe can help\n \nInterested in gathering data via SMS but don't know how to get started? Learn more about RapidPro, UNICEF's visual open-source SMS platform.\n- How SMS apps are used for surveys, crises, elections and data tracking\n- How to use RapidPro to jump start your own data gathering application\n- Using RapidPro's API to create your own data visualizations\nBYO device so you can participate live!", "Data Engineering Architecture at Simple\n \nSimple's Data Engineering team has spent the past year and a half building data pipelines to enable the customer service, marketing, finance, and leadership teams to make data-driven decisions.\nWe'll walk through why the data team chose certain open source tools, including Kafka, RabbitMQ, Postgres, Celery, and Elasticsearch. We'll also discuss the advantages to using Amazon Redshift for data warehousing and some of the lessons learned operating it in production. For each of these tools we'll cover some of the best Python libraries to use for interacting with them. \nFinally, we'll touch on the team's choices for the languages used in the data engineering stack, including Scala, Java, and Clojure in addition to Python. \n \nA walk through Simple's Data Engineering stack, including lessons learned and why we chose certain tools and languages for different parts of our infrastructure. ", "The \"NumPy\" Approach\n \nComing Soon\n \nComing Soon", "Plumbing in Python: Pipelines for Data Science Applications\n \nThe data flow library presented in this talk provides a thin abstraction layer between data pipeline declarations and specific execution backends. As exceptions are the rule, the library allows the user to introduce limited control flow into pipelines. At the same time, it also offers composability of pipelines, as many of our projects share similar building blocks.\nIn this talk we will show how using this library leads to a more functional style of programming, which improved the speed of our iterations. This shift in development style, already in the early stages of model development, includes clear separation of I/O operations and data transformations as well as the separation of data flow control and actual computations. We will also look into some additional benefits of this paradigm change, namely concurrency and testability.\n \nBringing data science models from development to production can be a daunting task. To reduce the overhead in this process and to improve flexibility, we introduced a Python data flow library at Blue Yonder which we will present in this talk.", "AlzHack: Data Driven Diagnosis of Alzheimer's Disease\n \nAlzHack is a collaborative citizen science project undertaken by a small but diverse group of data scientists. We will discuss the challenges encountered in discovering and acquiring suitable data, describe how we cleaned and merged multiple data sources, and how it was possible to extract meaningful features from within.\nWe will cover textual feature extraction, examining; amongst other methods, part-of-speech tagging, readability calculations, locality sensitive hashing as well as sentiment analysis, all in Python 3. \nIn addition we will show how a variety of machine learning techniques (including text clustering and classification) were used; with the aim of distinguishing diagnosed Alzheimer's sufferers from their healthy peers solely based on samples of their written correspondence. \nThis will be followed by a look at changepoint and ramp detection on noisy time series data; deployed to identify subtle changes in signals obtained from correspondence of individuals over time; thus allowing a form of non-medical, 'early warning' style detection of Alzheimer's disease.\nFinally we will address the tough task of scaling up a small, collaborative data science project to become an extremely powerful, widely available self-diagnosis tool.\n \nAlzheimer's disease is a form of dementia that affects over 44 million people globally. Unfortunately the condition is very hard to detect in its early stages. It is usually diagnosed by a simple questionnaire test, an approach that can only detect Alzheimer's disease many years after its onset. The challenge set in this project was earlier detection using Python and data science.", "Automatic Citation generation with Natural Language Processing\n \nIn this project we use natural language processing to investigate ways to generate recommendations for citations for new patents. Using a topic modeling approach and cosign similarity we attempt to recommend patents similar to the text of a potential new patent. This would allow inventors and engineers to quickly reference patents similar to their own and easily cite them. \nFrom an architectural stand point we use big query to handle data ingestion and pre-processing. The data management functionality of big query is key because there are millions of patents some of which may contain hundreds of pages of text. After pre processing in big query the bulk of our analytic work is done in spark through the python wrapper which allows for easy parallelization of the analytical portion.\n \nCan citations write themselves? Using a topic modeling approach we model similarity between patents from the US patent database via cosine similarity. Mining the text of millions of patents for similarities allows us to algorithmically recommend possible citations for new patents. We use python, pyspark and big query to parallelize complex operations over millions of rows of data.", "Variational Inference and Python\n \nThe state of the nation\nThere are currently three big trends in machine learning: Probabilistic Programming, Deep Learning and \"Big Data\". Inside of PP, a lot of innovation is in making things scale using Variational Inference. In this talk , I will show how to use Variational Inference in PyMC3 to fit a simple Bayesian Neural Network. I will also discuss how bridging Probabilistic Programming and Deep Learning can open up very interesting avenues to explore in future research.\nProbabilistic Programming\nProbabilistic Programming allows very flexible creation of custom probabilistic models and is mainly concerned with insight and learning from your data. The approach is inherently Bayesian so we can specify priors to inform and constrain our models and get uncertainty estimation in form of a posterior distribution. Using MCMC sampling algorithms we can draw samples from this posterior to very flexibly estimate these models. PyMC3 and Stan are the current state-of-the-art tools to consruct and estimate these models. \nOne major drawback of sampling, however, is that it's often very slow, especially for high-dimensional models. That's why more recently, variational inference algorithms have been developed that are almost as flexible as MCMC but much faster. Instead of drawing samples from the posterior, these algorithms instead fit a distribution (e.g. normal) to the posterior turning a sampling problem into and optimization problem. ADVI -- Automatic Differentation Variational Inference -- is implemented in PyMC3 and Stan, as well as a new package called Edward which is mainly concerned with Variational Inference.\nIn this talk we'll apply these methods of Variational Inference to regression and neural network problems, and explain the advantages for solving big data problems in probabilistic programming. You'll leave this talk with methods you can apply in your own work, and will showcase some of the new features in PyMC3 and Edward.\nThe speakers are both contributors to PyMC3.\n \nRecent improvements in Probabilistic Programming have led to a new method called Variational Inference. This is an alternative method to the standard method of Markov Chain Monte-Carlo. \nWe'll discuss these methods in PyMC3 and Edward, explain the theory and the limitations and apply these methods to realistic examples.", "DeepCare Chatbot - Generating answers to customers using a hybrid approach of Deep Learning and NLP\n \nThe DeepCare chatbot is capable of learning to answer customer questions. Using a hybrid approach of NLP and Deep Learning, it tries to combat logical fallacies that occur in pure deep learning bots, while still coming up with unique answers. \nIn particular, it uses a sequence-to-sequence (seq2seq)  long-short-term-memory LSTM deep learning model to capture intricacies in questions. As organisations cannot afford a bot making logical mistakes, verification through NLP is used. This two-step model prevents the downside of \"no control\" on deep learning, as well as the too static nature of classical rule based NLP models, and thus enables potentially higher quality answers.\nA live demo will be available at https://deepcare.online on the day of the talk.\n \nThe DeepCare chatbot is capable of learning to answer customer questions. Using a hybrid approach of NLP and Deep Learning, it tries to combat logical fallacies that occur in pure deep learning bots, while still coming up with unique answers. A live demo will be available at https://deepcare.online on the day of the talk.", "\u201cWhich car fits my life?\u201d  - mobile.de\u2019s approach to recommendations\n \nAt  mobile.de, Germany\u2019s biggest car marketplace, a dedicated team of data engineers and scientists, supported by the IT project house inovex is responsible for creating intelligent data products. Driven by our company slogan \u201cFind the car that fits your life\u201d, we focus on personalised recommendations to address several user needs. Thereby we improve customer experience during browsing as well as finding the perfect offering.\nIn an introduction to recommendation systems, we briefly mention the traditional approaches for recommendation engines, thereby motivating the need for sophisticated approaches. In particular, we explain the different concepts including collaborative and content-based filtering as well as hybrid approaches and general matrix factorisation methods. This is followed by a deep dive into the implementation and architecture at mobile.de that comprises ElasticSearch, Cassandra and Mahout. We explain how Python and Java is used simultaneously  to create and serve recommendations. \nBy presenting our car-model recommender that suggests similar car models of different brands as a concrete use-case, we reiterate on key-aspects during modelling and implementation. \nIn particular, we present a matrix factorisation library that we used and share our experiences with it. We conclude by a brief demonstration of our results and discuss the improvements we achieved in terms of key performance indicators. Furthermore, we use our use case to exemplify the usage of deep learning for recommendations, comparing it with other traditional approaches and hence providing a brief account of the future of recommendation engines.\n \nAs Germany\u2019s largest online vehicle marketplace mobile.de uses recommendations at scale to help users find the perfect car. We elaborate on collaborative & content-based filtering as well as a hybrid approach addressing the problem of a fast-changing inventory. We then dive into the technical implementation of the recommendation engine, outlining the various challenges faced and experiences made.", "Creating Python Data Pipelines in the Cloud\n \nIntroduction\n\nWhat is a data pipeline\nExplanation of why we use data pipelines\n\nTools of the trade\n\nLuigi\nAirflow\nNative cloud data pipelines\nCloud Dataflow\nAWS Data Pipeline\n\n\n\n- Description of Real-world data example\n- Implementation of data pipeline using the tools above and comparison of the various tools.\n- Conclusions\n \nMy talk will be an analysis of the various approaches to creating data pipelines the public cloud using Python.I will compare and contrast using various Python libraries such as Luigi, Airflow and native cloud frameworks such as Cloud Dataflow (Google), AWS Data Pipeline to create a real world data pipeline in Amazon AWS and Google Compute Engine. ", "Python, C, C++, and Fortran Relationship Status: It\u00e2\u0080\u0099s Not That Complicated\n \nIt\u00e2\u0080\u0099s commonly advised to write all of one\u00e2\u0080\u0099s code in Python, and then fix performance bottlenecks with a compiled language like C. But how does one get Python and C talking? \nSuppose you\u00e2\u0080\u0099d like to use Python more often, but you have 20 years of homegrown Fortran libraries and you don\u00e2\u0080\u0099t want to abandon that investment. Can Python use your Fortran libraries? What if your code has to be written in C in order to talk to a scientific instrument, but you\u00e2\u0080\u0099d rather work in Python. Can you get Python to talk to your hardware driver? Can you have your cake and eat it too? \nThe answers are yes, yes, and yes. This talk will give you an overview of your many options for getting Python to call and exchange data with code written in a compiled language. \nThis will include popular choices like the ctypes standard library module, Cython, and writing an extension in C, as well as some less obvious choices like SWIG, embedding Python, and Boost.Python. I\u00e2\u0080\u0099ll discuss some of the pros and cons of each technique and suggest scenarios which play to their respective strengths.\nThe goal of this talk is to make attendees aware of choices they may not know they have, and when to prefer one over another.\n \nOne of Python\u00e2\u0080\u0099s strengths is that it can use code written in compiled languages like C, Fortran and C++. \nThis talk gives an overview of your many options for getting Python to call and exchange data with code written in a compiled language. The goal is to make attendees aware of choices they may not know they have, and when to prefer one over another.", "Pyglmnet: A Python package for elastic-net regularized generalized linear models\n \nGeneralized linear models (GLMs) are powerful tools for multivariate regression. They allow us to model different types of target variables: real, categorical, counts, ordinal, etc. using multiple predictors or features.\nPyglmnet: https://github.com/pavanramkumar/pyglmnet is a newly launched and fast growing Python package for GLMs with state of the art elastic net regularization and a growing list of link functions. It aims to mimic the functionality of the widely popular R package: glmnet. The API documentation is inspired by Python\u00e2\u0080\u0099s Scikit Learn and the design is mindful of both R and Scikit Learn users. It has easy interoperability with various scikit learn tools for preprocessing, cross validation, scoring, etc.\nHere is a talk outline. \nRegularized multivariate regression models\nI will setup the basic GLM model, walk through the most popular variants, and show how the model parameters can be estimated by maximizing the log-likelihood of the data.\nIn problems with a large number of predictors, only a small fraction of them are useful. Therefore, penalization of complex models \u00e2\u0080\u0094 called regularization \u00e2\u0080\u0094 is important to prevent overfitting to the finite dataset. I will describe one popular regularization method called elastic net.\nOptimization\nOnce the problem is setup, the GLM parameters are estimated by maximizing the penalized log-likelihood. This problem is solved using proximal gradient descent with shrinkage. I will introduce the intuitions behind gradient descent and show how modern Python tools such as Sympy and Theano can be useful to calculate gradients symbolically or automatically.\nPython implementation and examples\nI will walk through the Python implementation, demonstrating the GLM class, and methods, along with applications to a real biological dataset.\nPre-requisites\nWorking knowledge of freshman level calculus and linear algebra. Exposure to linear regression and optimization would be useful. Strong interest in machine learning.\nLearning goals\nAfter the talk, you will be able to:\n\n\nAppreciate the versatility of GLMs in data analysis\n\n\nSetup the optimization problems using log likelihoods\n\n\nUnderstand the intuition behind regularization and the elastic net penalty\n\n\nUnderstand gradient descent\n\n\nUse symbolic differentiation to compute gradients with sympy\n\n\nAppreciate the power of automatic differentiation with Theano\n\n\nApply pyglmnet to your favorite prediction problem\n\n\n \nIn the era of big data, and high performance computing, generalized linear models (GLMs) have come to be widely applied across the sciences, economics, business, and finance. Although a popular R package exists, there is no equivalent in Python. I developed pyglmnet to address this need. In this talk, I will give a theoretical overview of GLMs and demonstrate the package with examples.", "A Practical Introduction to Airflow\n \nAirflow is a popular pipeline orchestration tool for Python that allows users to configure complex (or simple!) multi-system workflows that are executed in parallel across any number of workers. A single pipeline might contain bash, Python, and SQL operations. With dependencies specified between tasks, Airflow knows which ones it can run in parallel and which ones must run after others. Airflow is written in Python and users can add their own operators with custom functionality, doing anything Python can do.\nMoving data through transformations and from one place to another is a big part of data science/engineering, but there are only two widely-used orchestration systems for doing so that are written in Python: Luigi and Airflow. We\u00e2\u0080\u0099ve been using Airflow (https://pythonhosted.org/airflow/) for several months at Clover Health and have learned a lot about its strengths and weaknesses. We use it to run several pipelines multiple times per day. One includes over 450 heavily linked tasks! \nWe will use this talk to give a practical introduction to Airflow that gives people the information they need to decide whether Airflow is right for them and how to get started. We will cover things such as:\n\nHow Airflow schedules run\nHow to build pipelines and tasks\nBuilt-in capabilities and UI\nExtension options\nOperations and deployment\n\nAbout Clover Health: Clover Health is reinventing the health insurance model by using its data and analytics platform to identify at-risk members and partner with providers to accelerate care coordination, improve health outcomes and reduce avoidable costs. Built with technology at its core, Clover aggregates and structures data from a wide range of sources \u00e2\u0080\u0093 from primary care providers and lab results, to customer service interactions and home visits \u00e2\u0080\u0093 for continuous, real-time monitoring. For more information, visit www.cloverhealth.com.\n \nAirflow is a pipeline orchestration tool for Python that allows users to configure multi-system workflows that are executed in parallel across workers. I\u00e2\u0080\u0099ll cover the basics of Airflow so you can start your Airflow journey on the right foot. This talk aims to answer questions such as: What is Airflow useful for? How do I get started? What do I need to know that\u00e2\u0080\u0099s not in the docs?", "Single-source Python 2/3\n \nOverview\nEven though the first version of Python 3 has been released in 2008,\nat many places Python 2 or at last Python 2.7 will remain important for\nthe foreseeable future. Therefore, many users need to maintain\na library or an application for both major Python versions.\nThere are several strategies to support Python 2 and 3.\nSince maintaining two separate versions is not a real option,\ntwo major strategies have been established:\n1.) automatic translation via 2to3 or\n2.) the single-source approach with or without the help of an\n    external library\nIn the beginning, it was though that the users will swiftly move to Python 3.\nThis implies a one-time effort to port your Python 2 code to Python 3.\nAs it turned out, this use case not as common as expected.\nTherefore, the automatic translation approach was moved into the install step.\nThis technique is somewhat problematic because the translation is done\nat the time the user installs a library on th user's machine.\nOver time the single-source approach became much more viable option.\nThis tutorial focuses on this approach.\nOutline\n\n\nOverview of important Python 2/3 differences\n\nprint function\niterators everywhere\nno apple-orange comparisons\nintegers without limits\nstrings are always unicode\nnew syntax\nre-structuring of the standard library\nbuild-in functions\n\n\n\nKeep your tests ready\n\n\nWriting your own compatibility layer\n\nuse modern Python 2 (exceptions, next)\nfuture\nversion testing\nre-defining built-ins\nstandard lib packports\nhandling strings and bytes\nIO\n\n\n\nUsing python-future.org\n\nstaring from scratch\nstarting from Python 2\nstarting from Python 3\ncatching exceptions\nautomatic conversion\nporting philosophy\n\n\n\nRequirements\nYou should have Python 2.7 and Python 3.5 installed.\nPlease install https://python-future.org:\nconda install future\n\nor\n\npip install future\n\n \nUsing a single source code to support Python 2 and Python 3 is a useful\nstrategy to maintain backward compatibility and support the newest\nversion of Python at the same time. This tutorial gives an overview\nover the major differences between Python 2 and 3.\nBased on these difference, two approaches are introduced.\nThe creation on your own compatibility library is the first method.\nIt has the advantage that there is no extra dependency.\nThe disadvantage of this approach is that it you need to\ndive deep into the topic and that it can become a substantial effort.\nThe second approach is to use an existing library.\nThis tutorial  introduces python-future.org that makes many\nPython 3 features work in Python 2", "Classifying train and car journeys using telematics data\n \nAs insurers are moving away from traditional demographics factors to\nprice car insurance products, telematics is becoming an increasingly\npopular solution to assess individual driver risk, optimise motor\ninsurance premiums and encourage safer driving habits. As MyDrive\nSolutions is using a mobile application as an option to capture data,\nit is crucial for the company to be able to distinguish between train\nand car journeys to ensure that policy holders' driving behaviours are\nassessed fairly.\nIn this talk we will go through a methodology to classify train and\ncar journeys, reviewing the various tools used to accomplish each task\nand highlighting the challenges encountered.\n \nIn this talk we will review a methodology to classify train and car\njourneys using telematics data and go through the challenges often\nencountered when working on this type of projects.", "Practical Optimization for Stats Nerds\n \nPractical Optimization for Stats Nerds\nIntroduction\nFormat of the talk\nThis talk takes familiar stats models and explores them from an optimization perspective. It then shows how those same optimization technologies can be used in decision modeling.\nKey take-aways\n\nMany statistical techniques are based on some sort of optimization.\nOptimization has lots of uses, such as solving decision models.\nLearning to structure problems you already know for optimization solvers is a great way to understand them!\n\nModel 1: Least Squares\n\nData generation and problem statement\nLeast squares using scikit-learn\nLeast squares using calculus\nLeast squares using cvxopt and quadratic optimization\n\nApplication 1: Portfolio Optimization\nExtends the quadratic optimization model used to solve least squares to solve Markowitz portfolio optimization.\nModel 2: Support Vector Machines\n\nData generation and problem statement\nSVMs using scikit-learn\nSVMs using PuLP and linear optimization\n\nInterlude: Problem Shapes\nWhat is general problem structure in statistical inferences problems? How does that differ from problem structure of optimization problems? How do we convert these to problems we can solve?\nModel 3: Clustering\n\nData generation and problem statement\nClustering using scikit-learn\nClustering using PuLP and integer optimization\n\nApplication 2: Pedicab Sharing\nDemonstrates a realistic approach to solving vehicle routing using integer optimization and Python.\n \nMany models important to inferential statistics and machine learning use some form of optimization under the hood. For example, least squares regression and support vector machines are both implemented as simple optimization models. With the right tools in your hands, optimization can do so much more! This talk shows how to implement familiar statistical models directly using optimization solvers.", "High-Performance Distributed Tensorflow:  Request Batching and Model Post-Processing Optimizations\n \nIn this completely demo-based talk, Chris will demonstrate various techniques to post-process and optimize trained Tensorflow AI models to reduce deployment size and increase prediction performance.\nFirst, we'll use various techniques such as 8-bit quantization, weight-rounding, and batch-normalization folding, we will simplify the path of forward propagation and prediction.  \nNext, we'll loadtest and compare our optimized and unoptimized models - in addition to enabling and disabling request batching.\nLast, we'll dive deep into Google's Tensorflow Graph Transform Tool to build custom model optimization functions.\n \nIn this completely demo-based talk, Chris will demonstrate various techniques to post-process and optimize trained Tensorflow AI models to reduce deployment size and increase prediction performance.", "Extending Jupyter with Google Cloud Storage file system backend\n \nsrc-d/jgscm is a Jupyter file system backend for Google Cloud Storage. It allows to work with notebooks and other files directly in Google Cloud Storage. It's codebase is rather small and simple thanks to Jupyter's modular architecture. We start from the very basics, revise how Google Cloud Storage works and how Jupyter deals with the virtual file system and end up with the complete production-tested backend implementation. The described ideas are rather versatile and can be re-used to create backends for similar cloud storage systems, e.g. Amazon.\nSee more here\n \nJupyter has a modular architecture which allows extending it in numerous ways. This talk presents src-d/jgscm, Google Cloud Storage Jupyter file system backend which provides the ability to work with notebooks and other files directly in Google Cloud Storage. The described approach can be used for writing similar backends, e.g. for Amazon Cloud.\nSee more here : https://egorbu.github.io/pydata_2017_barcelona/index.html", "Patterns for Collaboration between Data Scientists And Software Engineers\n \nCollaboration between data scientists and software engineers can have the following issues:\n\u2022 Different tools used between data scientists and engineers (more interactive vs more automated, for example ipython notebook vs command line)\n\u2022 If getting the latest data requires ops/engineering knowledge then the analysis may be done in \"stale\" data or a too-small subset of the data (As an example: data scientists working with manual exports ) \n\u2022 Regression testing/parameter tuning/evaluation of results/backfills and other common scenarios in data-driven applications also require more engineering knowledge. The engineers are in the best position to provide tools and processes for the data science team, but it can happen that this potential goes untapped\nThose issues lead to more time to production, unhappiness in the data science team if they end up fighting with operations work instead of doing mostly the work they like,  less trustworthy results and less trust between teams in general. If collaboration is done right however,  data science and engineering teams can have a very good symbiotic relationship where each person takes advantage of their strengths towards a common goal.\nSome collaboration patterns to foster a good relationship between data scientists and engineers are the following:\n\u2022 Continuous evaluation \u2013 making sure the data science algorithm\ncontinues to give good results with every commit (or combinations of\ncommits, in case there is several repositories with different data\nscientists working on them)\n\u2022 Report templating \u2013 data scientists can work with jupyter\nnotebooks with an extension  that allows those ipynb\nfiles to be used as templates (ie, where some variable values can be filled in later). Those notebooks can then be applied to\ndifferent datasets to quickly diagnose issues.\n\u2022 Data API \u2013 have a well documented API for the data scientists to\nhave easy access to the data so that they can do their exploration\nwithout needing the software engineering team to manually provide\nexports\n\u2022 Some flexibility regarding tools \u2013 if domain experts prefer to use\nSFTP to upload files to the server for analysis, let them. Too much flexibility can be an anti-pattern.\n \nThe talk is going to present, with examples, how  a software engineer team can work\ntogether with data scientists (both in-house and external collaborators) in\norder to leverage their unique domain knowledge and skills in analyzing\ndata, while supporting them to work independently and making sure that their work can be  constantly tested/evaluated and easily integrated into the larger product.", "Bayesian Network Modeling using R and Python\n \nBayesian Networks are increasingly being applied for real-world data problems. They provide the much desired complexity in representing the uncertainty of the predicted results of a model. The networks are easy to follow and better understand the inter-relationships of the different attributes of the dataset. As part of this talk, we will look into the existing R and Python packages that enable BN learning and prediction. The pros and cons of the available packages will be discussed as well as new capabilities that will broaden the application of BN networks.\n \nBayesian Networks (BN) are increasingly being applied for real-world data problems. They provide the much desired complexity in representing the uncertainty of the predicted results of a model. The networks are easy to follow and better understand the relationships of the attributes of the dataset. As part of this talk, we will look into the existing R and Python packages that enables BN usage.", "Open Data, Networks and the Law\n \nCitation networks have recently been a topic of interest to network scientists. Court Listener, an open data initiative, provides the network of law case citations as well as the text of (almost every) court case in the US. This network data set provides a rich array of questions that are of interest to legal scholars as well as network scientists. \nCan we determine which cases are the most influential in our legal system? Can we understand how legal doctrine evolves? We will discuss what we learned about how the network of law cases evolves and what this means for legal practitioners. \nInspired by this data set we develop new statistical methodology to model how networks evolve. We also provide new techniques to asses the goodness of fit for both standard and novel probabilistic network growth models.\nWe also discuss what we learned from this project about advancing undergraduate statistics education and how it can interface with industry or other areas of academia. This project involves a wide swath of the data science process from acquiring/cleaning data, to building up Python infrastructure required to analyze a complex data set, and finally to developing new statistical theory for network data. We believe that data science research projects like this one are ideal for undergraduate (and graduate) statistics students to get comprehensive training in the computational techniques required for the real world.\n \nWhat does network science have to say about the law? Can we determine which are the most  the most influential cases in our legal system? Can we understand how legal doctrine evolves?  Using tools from network statistics and data provided by Court Listener (an open legal data project), we analyze the network of law case citations.", "High Frequency Trading in MMORPG Markets using Luigi, Pandas, and Scikit-learn\n \nIn this talk I\u00e2\u0080\u0099ll describe the system I developed to implement a basic algorithmic trading strategy in the in-game market of EVE Online, an online, multi-player video game. The system is built on top of three main Python packages: I use \n\nLuigi to extract, transform, and load data from an API into a local database  \npandas to manipulate the downloaded data and generate features  \nscikit-learn to analyze the generated features and identify promising items to trade on the in-game market\n\nI\u00e2\u0080\u0099ll walk through the advantages (and pitfalls) of using each of these tools in this simple model, with a particular emphasis on approaching Luigi pipelines as a novice. Finally, I\u00e2\u0080\u0099ll talk about how the same components used in this toy model might be generalized to real-world applications, and domains beyond finance. \nI\u00e2\u0080\u0099ll also briefly touch on how to access EVE Online\u00e2\u0080\u0099s rich economic data. EVE Online offers a particularly appealing sandbox for this kind of economic model building for several reasons. First, the in-game economy is entirely player-driven: all items, commodities, and raw materials exchanged on the market are produced by players, and prices are therefore almost entire determined by supply and demand. Second, the game includes an extensive API which exposes data about current and historical market prices. Finally, the in-game market is completely unregulated, which allows for market manipulations that are forbidden in the real world.\n \nIn this talk I\u00e2\u0080\u0099ll describe the system I developed to implement a basic algorithmic trading strategy in the in-game market of an online, multi-player video game. Using this toy model, I\u00e2\u0080\u0099ll walk through the steps involved in setting up a data pipeline with Luigi, analyzing the resulting data with pandas, and identifying important factors and features with scikit-learn. ", "Nipype: a framework for developing efficient and reliable medical imaging data analysis pipelines\n \nHuman brain imaging has seen an enormous growth in terms of data analysis methods in the past 25 years. However, many of those methods are implemented in a plethora of different labs using heterogenous tools, languages, and data management standards. \u00e2\u0080\u008bThe Neuroimaging in Python (Nipy) community provides an ecosystem of scientific tools that build on the PyData stack. The \u00e2\u0080\u008bNipype \u00e2\u0080\u008bproject \u00e2\u0080\u008bunifies the way hundreds of brain imaging tools can be run\u00e2\u0080\u008b and\u00e2\u0080\u008b also provides \u00e2\u0080\u008b\u00e2\u0080\u008bmeans to quickly prototype image analysis pipelines as well as deploy them at scale both at high performance clusters as well as the cloud. Nipype is being used in labs, hospitals and SaaS companies around the world and has a strong open source community of developers.\n \nThe \u00e2\u0080\u008bNipype \u00e2\u0080\u008bproject \u00e2\u0080\u008bunifies the way hundreds of brain imaging tools can be run\u00e2\u0080\u008b and\u00e2\u0080\u008b also provides \u00e2\u0080\u008b\u00e2\u0080\u008bmeans to quickly prototype image analysis pipelines as well as deploy them at scale both at high performance clusters as well as the cloud. Nipype is being used in labs, hospitals and SaaS companies around the world and has a strong open source community of developers.", "BigchainDB : a Scalable Blockchain Database, in Python \n \nThis talk describes BigchainDB. BigchainDB fills a gap in the decentralization ecosystem: a decentralized database, at scale. It points to performance of 1 million writes per second throughput, storing petabytes of data, and sub-second latency.\nThe BigchainDB design starts with a distributed database (DB), and through a set of innovations adds blockchain characteristics: decentralized control, immutability, and creation & movement of digital assets. BigchainDB inherits characteristics of modern distributed databases: linear scaling in throughput and capacity with the number of nodes, a full-featured NoSQL query language, efficient querying, and permissioning. Being built on an existing distributed DB, it also inherits enterprise-hardened code for most of its codebase.\nScalable capacity means that legally binding con- tracts and certificates may be stored directly on the blockchain database. The permissioning system enables configurations ranging from private enterprise blockchain databases to open, public blockchain databases. BigchainDB is complementary to decentralized processing platforms like Ethereum, and decentralized file systems like InterPlanetary File System (IPFS).\nThis talk describes technology perspectives that led to the BigchainDB design: traditional blockchains, distributed databases, and a case study of the domain name system (DNS). We introduce a concept called blockchain pipelining, which is key to scalability when adding blockchainlike characteristics to the distributed DB. We present a thorough description of BigchainDB, a detailed analysis of latency, and experimental results. The talk concludes with a description of use cases.\n \nThis talk describes BigchainDB. BigchainDB fills a gap in the decentralization ecosystem: a decentralized database, at scale. It has big-data performance levels, a querying system, and a permissioning system that supports public and private versions. It's complementary to decentralized processing platforms like Ethereum, and decentralized file systems like IPFS. BigchainDB is written in Python.", "Interactive Visualization in Jupyter with Bqplot and Interactive Widgets\n \n\n \n", "PixieDust - make Jupyter Python Notebooks with Apache Spark Faster, Flexible, and Easier to use\n \nPixieDust is a new Python open source library that helps data scientists and developers working in Jupyter Notebooks and Apache Spark to be more efficient. PixieDust speeds up data manipulation and display with features like:\n Automated local install of Python and Scala kernels running with Spark\n Realtime Spark Job progress monitoring directly from the Notebook\n Use Scala directly in your Python notebook. Variables are automatically transferred from Python to Scala and vice-versa \n Auto-visualisation of Spark DataFrames using popular chart engines like Matplotlib, Seaborn, Bokeh, or MapBox \n Seamless integration to cloud services\n Create embedded apps with your own visualisations or apps using the PixieDust extensibility APIs\nCome along and learn how you can use this tool in your own projects to visualise and explore data effortlessly with no coding. If you prefer working with a Scala Notebook, this session is also for you, as PixieDust can also run on a Scala Kernel. Imagine being able to visualise your favourite Python chart engines from a Scala Notebook! This session will end with a demo combining Twitter, Watson Tone Analyser, Spark Streaming, and some fun real-time visualisations - all running within a Notebook.\n \nPixieDust is a new Python open source library that helps data scientists and developers working in Jupyter Notebooks and Apache Spark to be more efficient. PixieDust speeds up data manipulation and visualisation with features like auto-visualisation, and  integration with cloud services. Come along and learn how you can use this tool in your own projects  to visualize and explore data effortlessly", "Using Random Forests in Python\n \nOutline\n\nIntro (5 minutes)\nWhat are random forests, how are they used, and what Python software is available for using them?\nWhat strengths do they have relative to other models (scalability and applicability to a broad range of problems)?\nForest Internals (15 minutes)\nDecision Trees (5 minutes)\nPresentation of the decision tree model, the building block of random forests.\n\n\nEntropy Minimization (5 minutes)\nExplanation of how decision trees are tuned using entropy minimization.\n\n\nBuilding Forests from Decision Trees (5 minutes)\nExplanation of how decision trees are aggregated to form random forests.\n\n\nIllustrative Examples (10 minutes)\nRegression on non-linear functions (5 minutes)\nClassification with unscaled features (5 minutes)\n\n \nThis talk will be an exposition of machine learning with random forests for Python programmers. The talk will cover the internals of how random forests are implemented, applications that are well suited to the use of random forests, and Python code samples to demonstrate their use.", "Web Scraping with Python\n \nWeb Scraping is the process of extracting useful information from the web automatically. \nIt is widely used for various purposes like:\n- Lead generation\n- Price monitoring on e-commerce sites\n- Product reviews collection to do sentimental analysis\n- Social profiles retrieving for recruiters\n- Scraping blogs & forums to keep up with trends and news\n- Maintaining up-to-date real estate databases\n- News aggregators\n- Reputation monitoring\n- Events listings\n- Websites changes detection\n- Etc.\nIn this talk we will present some of the most powerful tools for extracting data from web pages using Python. We will learn how to use Python to request information from a web server, how to perform basic handling of the server\u2019s response, and how to begin interacting with a website in an automated fashion.\n \nWeb Scraping is widely used to get data from websites automatically. Whether it be contact information, news or selling prices, we rely on web scraping techniques as they allow us to collect large data with minimal effort.\nIn the Python ecosystem we have a lot of different tools for extracting data from web pages, and in this talk we will present some of the most powerful ones.", "Developments in Test-Driven Data Analysis\n \nTest-driven data analysis fuses and builds upon the ideas of test-driven development and reproducible research to support higher quality data analysis.\nFoundational concepts are: * Level 0: Reference Tests * Level 1: Automatic constraint discovery and validation.\nThis talk will extend these to cover tight constraints on string fields with\n  * automatically discovered regular expressions with rexpy\n  * constraints between datasets\nand probably more.\nBackground material:\n\nPyCon UK Talk, Cardiff, Test-Driven Data Analysis https://www.youtube.com/watch?v=FIw_7aUuY50\nBlog: https://tdda.info, especially posts https://www.tdda.info/the-new-referencetest-class-for-tdda and https://www.tdda.info/constraint-discovery-and-verification-for-pandas-dataframes\nOverview: https://www.predictiveanalyticsworld.com/patimes/four-ways-data-science-goes-wrong-and-how-test-driven-data-analysis-can-help/\nIn terms of some of the new material that will be covered in this talk, see\nhttps://www.tdda.info/introducing-rexpy-automatic-discovery-of-regular-expressions\nhttps://rexpy.herokuapp.com\n\n \nTest-driven data analysis fuses and builds upon the ideas of test-driven development and reproducible research to support higher quality data analysis. This talk will extend the foundation parts of TDDA with extensions including tight constraints on string fields with automatically discovered regular expressions and automatically discovered relationships between datasets.", "Scaling up to Big Data - Devops for Data Science\n \nThe migration of running R or Python locally on a single machine to a cluster environment can be tricky. While there are many tools and resources available that make the launching of a cluster relatively easy, they are not focused or optimized to the specific use case of analytics using R and/or Python, but mostly on operations.\nImagine this scenario: you are a data scientist at a small organization. There is no devops support and you need to start setting up your environment for big data processing and analysis. You start a cluster in the cloud (on your favorite provider), you log on, and you want to run an R script that you\u00e2\u0080\u0099ve developed in your laptop. Sounds easy, right? Well have you considered the following?\n\nYou need to have R and all necessary additional packages installed on every node on the cluster\nThere are several ways to run R on hadoop\nHow do you set file permissions?\nIf you want to use RStudio as your interface/front end, you need to configure it, especially if you want it to talk to the underlying cluster\nIf there are multiple people that are going to be using the cluster simultaneously, you need to configure that\nWhat if you want to use Anaconda Python?\nHow can you make an ipython notebook talk to Spark?\n\nCome and learn about devops tips and tricks to optimize your transition into the big data world as a data scientist. This is a how-to session intended to raise awareness of some of the typical technical issues that can cause headaches. This session is not intended to be a  sysadmin session, but hopefully give you an additional understanding of concepts you need to know, including tools such as Ansible for automating your setup.\n \nScaling up R/Python from a single machine to a cluster environment can be tricky. While there are many tools available that make the launching of a cluster relatively easy, they are not focused or optimized to the specific use case of analytics but mostly on operations. Come and learn about devops tips and tricks to optimize your transition into the big data world as a data scientist.", "Scalable  Patient Records De-duplication using machine learning\n \nObjective:\nTo produce an improved identification of a continuously increasing patient records database. \nProblem:\nProper identification of duplicated patient information remains an arduous problem for hospitals, pharmacies and service providers. Simple matching of these records does not result in the correct identification of existing duplicates for various reasons such as noisy and incomplete records.\nMethodology:\nIn this work, we build on top of a python package that uses active learning to create a labeled data set, train a linear regression model on that sample and then derive predicate rules to speed up the pairwise comparisons of records before matching them. This existing enhanced methodology improves duplicates identification with a high level of performance but doesn't scale effectively.\n  In order to provide scalability, our application creates an initial model on a sample of the data and then updates the model as more batches of records are added to the database, all the while de-duplicating those records by merging them with existing entries.\n  Our contributions consist of:\n1.  Applying a cluster updating model to properly add additional batches of records to the existing cluster when applicable or creating new cluster. Each unique patient is represented by a cluster.\n2.  Produce a scalable solution capable of processing millions or records and updating the cluster representatives and records assignments, all under the constraints of the patient information protection rules.  \n \nSimple matching to identify duplicates in patient records produces numerous errors for various reasons. To improve the identification of duplicates, we built an incremental model on top of an existing machine learning based  Python package. We made the model updatable and scalable to accommodate an ever increasing patient record file.", "Deploying Machine Learning using sklearn pipelines\n \nSklearn pipelines have advantages during both the development and deployment lifecycle of data science models. This talk will focus on what Sklearn pipelines are, how you benefit from using them in your workflow, and an example of a technical implementation (with some practical tips). \nSome the advantages include:\n\nReusable feature engineering\nMaintable and readable code\nCreating objects which are deployable to production environments\nEase of changing and redeploying quickly and cleanly\n\n \nSklearn pipeline objects provide an framework that simplifies the lifecycle of data science models. This talk will cover the how and why of encoding feature engineering, estimators, and model ensembles in a single deployable object.", "Moving Forward Through The Darkness\n \n\ntraining models from data is just like painting pictures from the world. \n\nfrom the real world problem to the machine learning problem\n\nproblem from e-commerce\nproblem from news platform\nproblem from online and offline retailers\nproblem from construction of trading strategies\n\n\n\nmodeling types and modeling procedures \n\n\nthe blindness in modeling procedures ? and how to break through the blindness ?\n\nthe blindness coming from problem\n\nthe blindness coming from data\n\nusing POS data to find potential users ?\n\"delicious\" is not delicious ?\n\n\n\nthe blindness coming from coordinates of data\n\nthe blindness coming from mathematics? a story about 103 - 100 = 6 - 3 ?\nthe blindness coming from the methods converting making data to vector\nprobabilistic modeling, language modeling and infinite dimensional space\n\n\n\nthe blindness coming from frameworks of solvers (Supervised, Unsupervised, Semi-supervised)\n\nnew labels are coming all the way ?\nwhen orange-apple binary classifier meet banana ?\nthe blindnesses of clustering methods\nSemi-supervised learning and active learning\n\n\n\nthe blindness coming from algorithms and objective function of solvers\n\nthe blindness of k-means clustering \n\n\n\nthe blindness coming from parameters of solvers\n\nhow to control the weights of weighted classifier ?\n\n\n\n\n\nHow to control your model moving forward through the darkness ?\n\n\n \nThe blindness of modeling and how to break through.\nTraining models from data is just like painting or capturing pictures from the world. From different angles, you will see the different pictures. The pictures are just the approximations of the world! So do the models! There must be some blindnesses behind the approximations. In this talk, we will point out several types of blindnesses in modeling procedures and introduce the solution of breaking through them.", "Let's play Space Invaders!\n \nNeural Networks became recently very hot topic because of some very eye-catching developments like Deep Dream and successes like AlphaGo winning with Go master.\nI will show how I taught my Deep Q-Network (DQN) to play Space Invaders and explain principles behind DQN.\nGoing beyond this example I will show some exciting advances and applications of Deep Reinforcement Learning like attention.\n \nNeural Networks became recently very hot topic because of some eye-catching developments like Deep Dream and successes like AlphaGo winning with Go master.\nI will show how I taught my Deep Q-Network (DQN) to play Space Invaders and explain principles behind DQN and show other exciting advances in Deep Reinforcement Learning.", "A/B Testing: Harder than just a color change\n \nA/B testing is a common practice for websites...but where do you begin? This data-driven approach allows you to launch experiments and features with confidence. So how do you prepare, launch, and analyze an A/B experiment? How do you know for how long to keep it running? What about which metrics to track?\nThis talk will present a procedure developed to run an A/B experiment, from planning the task and understanding the key metrics to analyzing the results. We will cover both simple and more complex case study, which help us understand the challenges involved in running experiments.\nThis talk will cover a topic that will enable developers to make more data-driven decisions but has not been covered at Pycon. By providing case studies as motivation and a procedure to implement A/B testing this talk will excite the audience. Yelp runs multiple experiments on different aspects and the Transaction Platform team has gotten unique experience of needing to create experiments with limited traffic which will be discussed in the talk.\n \nIs your Product Manager asking you to test out different text or button colors? Not sure where to start? This talk will contain methodology and two case studies from Yelp\u00e2\u0080\u0099s Transaction Platform on how to properly run an experiment and get the best result. Learn about how to run a simple button color experiment, avoid pitfalls, test, and analyze the results with confidence. Statistical confidence!", "Pandas, Pipelines, and Custom Transformers\n \nFor data science in python, the pandas DataFrame is a common choice to store and manipulate data sets. It has named columns, each of which can contain a different data type, and an index to identify rows and assist in joining. The scikit-learn package is the major machine learning library in python. It has implementations for a wide variety of popular feature engineering, supervised, and unsupervised machine learning algorithms. Perhaps even more importantly to its success, scikit-learn provides a uniform interface for these transformers and estimators, making it easy to swap out one for another.\nMany scikit-learn transformers will take and return pandas DataFrames, but some only return numpy arrays. This means losing the column names and row indices. A few important examples include the meta-transformers Pipeline and FeatureUnion. The Pipeline chains together transformers to be applied in order. The FeatureUnion combines the results of transformers that can be applied in parallel. With these, the entire feature engineering process can be stored in one object and easily applied to new data sets.\nLuckily, scikit-learn also provides the ability to write your own custom transformers. It is as simple as defining a new class that implements the fit and transform methods. We can use this to create pandas-friendly versions of the Pipeline and FeatureUnion, as well as add transformations that are not already provided.\n \nUsing pandas and scikit-learn together can be a bit clunky. For complex preprocessing, the scikit-learn Pipeline conveniently chains together transformers. But, it will convert your DataFrame to a numpy array. In this talk, we will walk through pandas DataFrames, scikit-learn preprocessing and Pipelines, and how to use custom transformers to stay in pandas land.\nGitHub Link: https://github.com/jem1031/pandas-pipelines-custom-transformers", "A beginner's guide to data analysis in cosmology using Jupyter Notebook\n \nAs cosmology has entered the era of precision measurement, academics face the exciting challenges of analysing large datasets, many of these common to other areas of Big Data. In recent years Python has evolved as a standard tool in astronomy and cosmology due to the availability of open source statistical analysis and machine learning packages that allow the development of robust data analysis pipelines and powerful visualisations. In this session, I\u2019d like to demonstrate how a beginner to the field of cosmology can use open data along with the Jupyter Notebook to visualise and analyse real cosmological data, using easy to implement code examples from well known python packages such as AstroML, scipy, healpy, numpy, pymc, scikit-learn and matplotlib.\n \nCurrent cosmology experiments face exciting computational challenges in statistics and machine learning, with large amounts of data to process. In this beginner's session, we will use Jupyter Notebook to visualise and analyse real cosmological data, using easy to implement code examples from well known python packages such as AstroML, scipy, healpy, numpy, pymc, scikit-learn and matplotlib.", "The Neuroscience Lab: A Tour through the Eyes of a Pythonista\n \nBiologists have always built their own hardware to perform unique experiments; today, new data collection and analysis techniques demand that they develop their own software as well. Often, this means that researchers without a formal education in programming must perform a wide variety of programming tasks in order to perform cutting-edge research.  As a \u201creal\u201d programming language, Python is an excellent fit for scientists, not only for its oft-celebrated readability, active scientific community, and cross-platform support, but also for its versatility across a wide variety of domains. In this talk, we will celebrate this versatility of Python by sharing the research being done in our neuroscience laboratory, focusing how Python packages, applications, and plugins have become the tools of choice for everything from real-time machine vision and 3D graphics to data analysis and the presentation of scientific results.\n \nPython is an excellent fit for scientists; it is readable, has an active scientific community, and, most importantly, is versatile across a wide variety of programming tasks.  In this talk, we will celebrate this versatility of Python by sharing the research being done in our neuroscience laboratory, focusing on Python\u2019s involvement in each step of our scientific research.", "Towards Pythonic Innovation in Recommender Systems\n \nCollaborative Filtering is the most commonly used Recommender System because of its ability to process sparse data. Many implementations of the basic Collaborative Filtering algorithms are available, however, recent advances in the field are often less documented and known.\nIn order to provide a comparative analysis of available libraries and define minimal requirements for a performance comparison of Recommender System algorithms, the talk will be structured in two parts. \nIn the first part, the speaker will introduce the last years of relevant literature in the field of Recommender System, giving an overview on advanced tensor approaches, such as those used for Context- and Time-aware Recommender Systems. \nIn the second part of the talk, existing Python Collaborative Filtering libraries are reviewed.\nAmong others, the following important criteria will be considered: availability of state of the art and benchmark algorithms, parallel computation, maintenance, and easiness of use.\n \nRecommender Systems are nowadays ubiquitous in our lives. Although many implementations of basic algorithms are well known, recent advances in the field are often less documented. \nThis talks aims at reviewing available Recommender Systems libraries in Python, including cutting edge Time- and Context-aware state of the art models.", "How I learned to time travel, or, data pipelining and scheduling with Airflow\n \nThe power of any reporting tool breaks based on the data behind it, so when our data warehousing process got too big for its humble origins, we searched for something better. After testing out several options such as Drake, Pydoit, Luigi, AWS Data Pipeline, and Pinball, we landed on Airflow, an Apache incubating batch processing pipelining and scheduler tool originating from Airbnb, that provides the benefits of pipeline construction as directed acyclic graphs (DAGs), along with a scheduler that can handle alerting, retries, callbacks and more to make your pipeline robust. This talk will discuss the value of DAG based pipelines for data processing workflows, highlight useful features in all of the pipelining projects we tested, and dive into some of the specific challenges (like time travel) and successes (like time travel!) we\u00e2\u0080\u0099ve experienced using Airflow to productionize our data engineering tasks. By the end of this talk, you will learn\n\npros and cons of several Python-based/Python-supporting data pipelining libraries\nthe design paradigm behind Airflow, an Apache incubating data pipelining and scheduling service, and what it is good for\nsome epic fails to avoid and some epic wins to emulate from our experience porting our data engineering tasks to a more robust system\nsome quick-start tips for implementing Airflow at your organization\n\n \nData warehousing and analytics projects can, like ours, start out small - and fragile. With an organically growing mess of scripts glued together and triggered by cron jobs hiding on different servers, we needed better plumbing. After perusing the data pipelining landscape, we landed on Airflow, an Apache incubating batch processing pipelining and scheduler tool from Airbnb.", "The modern research skill set: Using Vagrant, Ansible, and Python to support researchers\n \nModern research practice asks researchers to engage with information in new ways through the use of digital technologies. The landscape of this skillset is rapidly changing and difficult to pinpoint at an interdisciplinary level. The modern researcher is expected to navigate digital tools that are not unique to the work that they do on a daily basis within their discipline and to be able to share that work in meaningful ways with collaborators. \nOpen science has become increasingly relevant to modern scientific practice and reflects the development of the modern research skill set. There is a rising tide of policy that requires researchers to navigate open methodology in order to gain access to grant funding. Despite a shift towards support of open science in major policy making bodies, such as the OECD, the training that early career researchers receive has not caught up with this fast changing policy landscape. \nThis talk will define a modern research skill set, its relevance to the principles of open science, and the need for a better understanding of open practice and of tools and strategies that improve the reproducibility of scientific findings. We will show how the NCSU Libraries are preparing the next generation of researchers through basic training in core elements of the modern research skill set through the NCSU Libraries Summer of Open Science, a 12 week series of workshops and meetups for building baseline technical skills in command line usage, Python programming, web publishing, and scholarly identify management. This series was developed and taught by librarians working in instruction, open science, makerspaces, and software development. We\u00e2\u0080\u0099ll also show how we are simplifying the learning experience for novice data scientists using Vagrant and Ansible to provision reproducible computing environments for instructional use.\n \nThe NCSU Libraries are supporting the next generation of researchers through basic training in core elements of the modern research skill set like Python and scientific computing. We\u00e2\u0080\u0099ll show how we are simplifying the learning experience for novice data scientists using Vagrant and Ansible to provision reproducible computing environments for use in our Summer of Open Science workshop series.", "What Data Analysts Wish Application Developers Knew\n \nUsing a fictional scenario we'll talk through what data to save, how to save it, and how to make sure it's best able to be leveraged for use and action in the future. Or, in techo-speak: tracking instrumentation, available tools and libraries, and bugs to avoid.\n \nData analysts frequently do not get to participate in the app development process despite being some of its biggest stakeholders. This talk focuses on general guidelines and best practices for application developers on what they can do to optimize data content and quality available for analysis. ", "Machine Learning vs. The Flu: Creating better Flu Vaccines with Python and Machine Learning\n \nEvery year, millions of people are affected by the flu. Every year, we take a vaccine to possibly gain immunity from that year\u00e2\u0080\u0099s flu. However, the vaccines may not always work. In recent years, people have made attempts at predicting how the influenza virus has changed. For example, every year, world health officials try to predict what drugs to include in the coming year\u00e2\u0080\u0099s vaccine. However, many of the methods used have been called \u00e2\u0080\u009cquestionable\u00e2\u0080\u009d by the National Institutes of Medicine.  So, my goal in this project was to come up with a better way for scientists to make flu vaccines.\nIn my talk, I will show how I used Python to implement my flu prediction algorithm, as well as how I used libraries to make my algorithm more efficient and simpler in every step. \nTo create my algorithm, I first had to obtain the genetic sequences of the flu from the flu database. These were in a FASTA file format. I originally read the file in as a simple .txt file, but using the Biopython library to read the genetic sequences, which are over 1700 base pairs long, was much easier and had built-in functions to quickly parse FASTA files. \nNext, I used a variety of machine learning algorithms. I originally tried writing them on my own; however, I found this would be tedious, and started using scikit-learn algorithms. Eventually, I found that the random forests algorithm performed best. In addition, I will explain how I evaluated each model\u00e2\u0080\u0099s performance and why the random forest performed the best.\nWhat the audience will learn:\nI will explain the benefits and the drawbacks of using the Biopython and scikit-learn functions. I will cover interpreting and preprocessing the gene sequences, encoding the data, fitting the data to the model and then evaluating the model\u00e2\u0080\u0099s performance and selecting the best model.\n \nMillions of people are affected by the flu each year. I wanted to create a better way for scientists to make flu vaccines. My solution was to predict future flu genetic sequences so that scientists could easily analyze flu sequences and create vaccines. The main topics will include the use of Biopython and scikit-learn in a scientific environment, as well as data preprocessing and model selection.", "Bayesian Optimization and it's application to Neural Networks\n \nHave you ever failed to train a Neural Network? Spent hours, to get it even learn anything?\nIf you ask an expert on how she does this, the answer might be something like:\n\"It needs a lot of experience and some luck\".\nIf you know this problem, then this talk is for you. Also, let's steer our luck with ML!\nWhen tuning hyperparameters, an expert has built a model, that means some expectations on\nhow the output might change on a certain parameter adaption. For example, what happens to your\nConvolutional Neural Network if you set the dropout from 0.5 to 0.25.\nBayesian Optimization is a method that is able to build exactly this kind of model.\nIt uses for example Gaussian Processes to take decisions on which parameter-change\nmight bring you the most benefit, and if it does not, the model is adapted accodingly.\nThis talk will be about the fundamentals of Bayesian Optimization and how it can be used to train ML Algorithms in Python.\nTo this end we'll consider it's application to Neural Networks. The NNs will be implemented in keras, the Bayesian Optimization will be optimized with hyperas/hyperopt.\nI am planning to split this talk 50:50 into theory and practice.\n \nThis talk will be about the fundamentals of Bayesian Optimization and how it can be used to train ML Algorithms in Python.\nTo this end we'll consider it's application to Neural Networks. The NNs will be implemented in keras, the Bayesian Optimization will be optimized with hyperas/hyperopt.", "Statistically Solving Sneezes and Sniffles (Step by Step)\n \nSince April 2015 our group has studied the Allergic Rhinitis of a subject with the goal of building a machine learned model that predicts the need for antihistamines. Approximately 30% of the world's population suffers from allergies, we aim to provide a methodology for others to identify the drivers of their own symptoms.\nThis is a \"citizen science\" project, currently focused on one individual and a year's worth of self-reported antihistamine usage, sneezing data and geolocated points. We'll discuss the available external data (including the London Air project's pollution readings, weather, diet, exercise and commute data), exploratory data analysis, our approach to feature engineering from time-series and text sources and our modeling progress.\nThe data logging iPhone app and data preparation tools are all open sourced. Python tools discussed include scikit-learn, statsmodels, glueviz, textract and t-sne. We'll also review our distributed working practices.\n \nSince April 2015 our group has studied the Allergic Rhinitis of a subject with the goal of building a machine learned model that predicts the need for antihistamines. Approximately 30% of the world's population suffers from allergies, we aim to provide a methodology for others to identify the drivers of their own symptoms.", "Scan Statistics with Spark Streaming: Distribution Based Real Time Anomaly Detection\n \nScan Statistics is a distribution based methodology for detecting anomalous data. Unlike simpler methodologies like moving average and exponential smoothing that rely on previous data, we can perform a hypothesis test regarding the distribution of the data and thus perform the analysis in real time. Spark Streaming is a framework that lends itself well to this use case. This talk will introduce a Python package built for Spark Streaming that performs real time anomaly detection using various distributions of count data.\n \nScan Statistics is a distribution based methodology for detecting anomalies. This talk will explore the use of scan statistics to perform real time analysis on streaming data using Spark Streaming.", "Pythonic Polling Analysis and Comments on 2016's Polling Surprises\n \nBetween Brexit and the US Presidential election, it has been quite a year in the English-speaking world for pollsters. In this talk I will first introduce basic concepts of political polling design methodologies and traditional analytical techniques for dealing with the necessarily skewed data that results from traditional sampling. I will then give an overview of existing Python packages for tackling survey design and demonstrate sample code applying existing packages and also roll-your-own approaches. I will discuss current industry best practice for polling and explain how these traditional methods were deployed to monitor the year's two biggest political votes: the US Presidential election and the UK Brexit referendum.\nI will explore how, and whether, the outcomes of these two votes was as much as a surprise to pollsters as the media indicated and what might have led to more accurate predictions. Finally, I will close with comments about how polling methodology is likely to change in the coming years and what, if anything, could have been done differently analytically to better predict the actual results of these important 2016 votes.\n \n2016 was a whopper of a year both for political upsets and debates about traditional polling and its relevance. In this talk I will discuss Pythonic survey analysis and will also highlight the pitfalls of polling and sampling generally. I will close with some thoughts on polling surprises from Brexit and the US Presidential election.", "Increasing the trading prediction by mining aggregated human texting messages\n \nSentiment analysis\nThis step is observed the time series of the company\u2018s event based on the financial news or the social media, which analyses problem of coreference in text mining that is sentiment classification. Fasttext is inspired method that focuses on the meaning of words, which solves to obscure by ambiguity and play on words. This way can classify polarity, subjectivity and intensity in the corpus. According to the results of the classification, then it changes to the word representation. Thus, prediction accuracy increases\nsignificantly at this layer that is attributed to appropriate noise-reduction from the feature-space.\nModel creation and prediction\nThe first layer is termed Sentiment Integration Layer by the deep neural network, which integrates sentiment analysis capability into the algorithm by proposing a sentiment weight that reflects their sentiment. Additionally, this layer reduces the dimensions by eliminating value in terms of sentiment and thereby improves prediction accuracy. The second layer encompasses a deep neural network model creation algorithm. It updates the models with the most recent information available, the deep learning algorithm is extensively evaluated using real market data and news content across multiple years, which is a good fitting capability to evaluate and select the financial news. The third layer is that it uses the recursive neural network explore patterns and of financial news through the recursive and pooling operation.\n \nA novel approach is proposed to predict intraday directional-movements of the stock in the trading market based on the text of breaking financial news or social media by event data. This work is an effort to put more emphasis on the text-mining methods and tackle some specific aspects thereof that are weak in previous works. The research is a specific market, namely, the trading and stock market,", "Introduction to Search\n \nSearch engines make archives, inventories, websites, large publishing platforms, and the internet navigable. In this talk, I will introduce the basic building blocks of a search engine.\nWordPress.com is a platform where anyone can create a blog or website, and read posts that are published there and elsewhere. More than 90 million posts and pages are published on WordPress.com per month. To make it easier for our users to find the ones they are most interested in, we built a search engine that is based on Elasticsearch.\nIn order to keep improving our search algorithm, we need to track and analyze our user\u2019s interactions with the search results. I will present an overview of the metrics we and others use to evaluate the performance of search algorithms.\n \nSearch engines make archives, inventories, websites, large publishing platforms, and the internet navigable. Using the search engine we built at WordPress.com, a platform where more than 90 million blog posts and web pages are published per month, as an example, I will explain how search engines work and how their performance can be evaluated.", "Fuzzy Search Algorithms: How and When to Use Them \n \nFuzzy Searching or approximate string matching is powerful because often times text data is messy - shorthand and/or abbreviated text are common in various data sets, voice to text conversion can also be messy sometimes. As a result, we want to be able to make the most of our data by extrapolating as much information as possible. In this talk, we will explore the various approaches used in fuzzy string matching and demonstrate how they can be used as a feature in a model or a component in your python code. We will dive deep into the approaches of different algorithms such as Soundex, Trigram/n-gram search, and Levenshtein distances and what the best use cases are. For instance, Levenshtein is great for real time analytics whereas trigram/n-gram works best on a batch data set with appropriate indexes. Finally, we will demonstrate via live coding how to implement some of these fuzzy search algorithms using python and/or PostgreSQL. \n \nSo much of data science is about understanding the context around your data. In this talk, we hope to address how to work with messy text data by leveraging fuzzy search algorithms in python or against a database such as PostgreSQL. We will talk specifically about fuzzy algorithms such as Soundex, Trigram/n-gram search, and Levenshtein distances and demonstrate use cases in an ipython notebook.", "Dynamics in Graph Analysis: Adding Time as a Structure for Visual and Statistical Insight\n \nModeling data as networks of relationships between entities can be a powerful method for both visual analytics and machine learning; people are very good at distinguishing patterns from interconnected structures, and machine learning methods get a performance improvement when applied to graph data structures. However, as these structures become more complex or embed more information over time, both visual and algorithmic methods get messy; visual analyses suffer from the \"hairball\" effect, and graph algorithms require either more traversal or increased computation at each vertex. A growing area to reduce this complexity and optimize analytics is the use of interactive and subgraph techniques that model how graph structures change over time.\nIn this talk, I demonstrate two practical techniques for embedding time into graphs, not as computational properties, but rather as structural elements. The first technique is to add time as a node to the graph, which allows the graph to remain static and complete, but minimizes traversals and allows filtering. The second is to represent a single graph as multiple subgraphs where each is a snapshot at a particular time. This allows us to use time series analytics on our graphs, but perhaps more importantly, to use animation or interactive methodologies to visually explore those changes and provide meaningful dynamics.\nWe explore these dynamics using a concept graph extracted from a natural language corpus using NLP techniques as well as a graph of GitHub commits. The creation and analysis of the graphs will be conducted via NetworkX and the visualization aspects will be conducted using Graph Tool and matplotlib. An outline is as follows: \n\nAn introduction to our graphs and possible insights\nWhere can we find the time? Data wrangling for time as a property\nStatic analyses: moving time from a property model to a structural element \nTraversals and subgraphs of time\nThe \"betweenness\" of time and new node proximities \nA small overview of network visualization \nVisualizing static layouts with timely edge properties \nAdding dynamics for interactive analysis\nAnimating the change in structure of a graph over time \nFinding insights with visual analytics: overview first, zoom and filter, details on demand. \n\nAll code will be made open source, as will the data sets that we collect! \n \nNetwork analyses are powerful methods for both visual analytics and machine learning but can suffer as their complexity increases. By embedding time as a structural element rather than a property, we will explore how time series and interactive analysis can be improved on Graph structures. Primarily we will look at decomposition in NLP-extracted concept graphs using NetworkX and Graph Tool. ", "Keynote: Developing Communities to Develop Themselves\n \nComing Soon\n \nComing Soon", "Fighting Against Chaotically Separated Values with Embulk\n \nIn this talk we will cover:\n- How we created a data collection tool that can read any chaotically formatted files called \"CSV\" by guessing its structure automatically\n- Explore the plugin-based-architecture that makes it easy to load data from external sources and publish to production systems. From files to business systems such as Salesforce & Mixpanel.\n- Review current plugins (over 100 released by the OSS community) and use cases\n- Explain how distributed execution enhances stability and scalability \n \nPython is a great tool for performing data analysis, but often time the hardest part is getting access to your data that\u00e2\u0080\u0099s located in a variety of business systems - files, database, and SaaS applications. Productionizing this process is even harder: scripts frequently fail  and require precious to to fix and re-test.  In this talk, I will review some open source tools I authored and show you how ", "What's new in Deep Learning?\n \n@karpathy's recent tweet \"BatchNorm, STN, DCGAN, DRAW, soft/hard attention, char-rnn, DeepDream, NeuralStyle, TensorFlow, ResNet, AlphaGo.. a lot happened over 1 year\" sums up the many new aspects of Deep Learning research.\nIn this talk I will review some of the highlights of deep learning in the context of Python deep learning frameworks.\n \n@karpathy's recent tweet \"BatchNorm, STN, DCGAN, DRAW, soft/hard attention, char-rnn, DeepDream, NeuralStyle, TensorFlow, ResNet, AlphaGo.. a lot happened over 1 year\" sums up the many new aspects of Deep Learning research.\nIn this talk I will review some of the highlights of deep learning in the context of Python deep learning frameworks.", " The NetworkL python package\n \nGraphs are the most popular way to represent and analyse a variety of real-world system and data sets from different domains. Today data grow fast and change rapidly over time.\nNetworkL (https://networkl.github.io/) is an experimental python package I wrote to supports the manipulation and efficient (L)ongitudinal analysis of large-scale time-varying graphs. NetworkL includes a set of optimized algorithms and data structures which create the basics to carry out network analysis of large time-varying networks even on commodity workstations.\nIn particular it implements a smart way to representation the full distance matrix of the network as a sparse matrix. This reduces the memory load up to 50%. Moreover, NetworkL include an implementation of the Ramalingam&Reps algorithm to recompute all the shortest paths length. Re-computations are performed in centiseconds regardless of the graph size. This performances makes NetworkL particularly suitable for the analysis of (L)ongitudinal network data-sets.\nBiblio: Ramalingam, G., & Reps, T. (1996). On the computational complexity of dynamic graph problems. Theoretical Computer Science, 158(1), 233-277.\n \nNetworkL.github.io is an experimental python package which supports the manipulation and efficient (L)ongitudinal analysis of large-scale time-varying graphs. NetworkL reduces the memory load of the Distance Matrix up to 50% and performs re-computation of shortest paths in centiseconds after edges updates. The package opens the possibility to perform real-time network analysis on streaming data. ", "National Geospatial-Intelligence Agency: Changing the Bureaucrat\u2019s Mind Toward Data-Driven Decisions\n \n\nData-Driven Decisions\n\n \nMr. Gary Dunow will discuss the National Geospatial-Intelligence Agency (NGA), its challenges with big data, and some of the ways that NGA is taking on these challenges.", "\u2764 Analyzing the ElectroCardioGram (ECG) and classifying what's healthy and what's not.\n \nThe ElectroCardioGram (ECG) is a periodic waveform that describes the\naction of heart as it moves through 3 electromechanical phases:\n\nDepolarization and contraction of the atria\nDepolarization and contraction of the ventricles\nRepolarization of the ventricles and atria\n\nIt is an enormous area of study and the ECG is tractable and effective\nway of detecting healthy sinus rythmn, diagnosing arrthymia and\npotentially predicting the decline of the heart from a healthy state\nto a disease state.\nPython and the scientific stack offers everything a researcher or a\nhobbyist would need to conduct sophisticated analysis and in this talk\nwe'll describe how to store and load the ECG, process the signal, classify\nfiducial markers and make interpretations about the state of the heart.\nThe talk will be presented in an ipython notebook and involve h5py\nfor reading ECG data in from disk as well as using the python-wfdb to get data from the Physionet repositories. scipy.signal for smoothing,\nprocessing and classifying parts of the ECG as well as peakutils to classify peaks. matplotlib and seaborn will be used for visualisation and statsmodels will be used to describe the data. This will ultimately generate features that can be used as the basis or an ML model.\n \nThe ElectroCardioGram (ECG) is the electrical activity of your heart. By recording it, classifying fiducial markers and analysing these features we can make assessments about the healthy state of the heart, diagnose certain diseases of the heart and predict whether a subject will go on to develop certain diseases. Python and the scientific stack provide all the tools you need.", "Prototyping Interactive Dashboards with Jupyter Notebooks: Examples from Network Engineering\n \nAs python programmers, we often use jupyter notebooks as a platform for reproducible data analysis and publication of results. The jupyter ecosystem is still under-utilized  for projects that require interactions from non-coder users, such as explorative dashboards or Business intelligence applications. There are other specialised options for these UI/UX interfaces, but integrating them into the analysis pipeline (e.g. Jupyter) is often rather hard. Preferably, we would be able to prototype these interactive applications without leaving our environment. \nJupyter-dashboards, ipywidgets, and other typical pydata libraries provide jupyter with the capabilities to create flexible, and rather powerful interactive elements. This toolset can be used in small projects to create the final facing interface for a limited number of end users. For bigger projects, a more robust UI/UX is normally required to support larger number of simultaneous users. However, the jupyter-dashboard/ipywidgets toolset can still be useful in these cases as an end-to-end prototyping platform, allowing us to get feedback from users at early stages of the project, thus helping us better steer our development.\nIn this talk, I share experiences of using this toolset for different use cases related to analysis of Internet traffic and network engineering. I\u2019ll go over examples of how to build widgets to explore the Internet traffic and other network data, while highlighting some of its characteristics. Furthermore, I\u2019ll describe some optimisations performed by network engineers on their traffic, such as load balancing, and how the tools can be used to evaluate them.\n \nIn this talk, I describe my experience in using jupyter-dashboards, ipywidgets and other libraries to build basic prototypes of data exploration dashboards for datasets related to Internet traffic and packet-switching networks. We\u2019ll go over various examples on how to build and use the widgets to navigate the data, visualize analysis, and evaluate optimization changes.", "Size Matters! A/B Testing When Not Knowing Your Number of Trials\n \nGetYourGuide is the leading online platform for touristic in-destination activities such as tours, excursions, and pub crawls. An important marketing channel for us is search engine marketing (SEM). The question of how much to pay for showing a potential customer a paid ad is highly complex. It does not only involve some estimate about the likelihood that the user will follow our ad to our website and finally book with us; the price is also influenced by our competitors since the decision about what ad is shown is made in an auction. A lot of room for improving our bidding strategies.\nIn early 2016, Google Adwords, one of the big players in SEM, rolled out a new feature called Campaign drafts and experiments. It enabled advertisers for the first time to test different bidding strategies in proper A/B tests. We thought they were proper, at least, until we figured out that Adwords wouldn't supply us with all necessary information to evaluate our tests in a decent statistical way.\nThe simple question of statistical significance became a search for the right question to ask.\nIn this talk, we will explain the problem in some more detail and report about our various approaches of how to circumvent it. On our journey, we will touch the foundations of SEM, A/B testing as well as time series.\n \nIf an A/B test's statistical evaluation is based on the number of trials and successes for each variation, what happens if you don't know the number of trials for your experiment? Although this question sounds rather theoretical, we faced it in practice. This talk is about our search for the right question to ask.", "Beyond Bag of Words: A Practitioner\u00e2\u0080\u0099s Guide to Advanced NLP Using Open Source\n \nMachine learning is an important concept in data science but it doesn't exist in isolation. We will show how algorithms such as Linear Regression, K-Nearest Neighbors, and SVD fit into a larger workflow of preprocessing, feature engineering, tuning, and testing. We\u00e2\u0080\u0099ll delve into techniques that can dramatically boost accuracy with minimal computational overhead. We'll explore how the popular NumPy, Pandas, Scikit-Learn stack handles a variety of use cases.\nThe first problem we'll explore involves anomaly detection in time series data. We'll use tools like autocorrelation, Fourier analysis, and modeling to remove seasonality trends in bikeshare data, then apply statistics to qualify data points as outliers or not.\nThe second problem we'll discuss involves building a custom recommendation engine using data about users, movies, and ratings. We try and compare different approaches using feature similarity, regression, and both content and collaborative methods.\nThis course emphasizes a practical and exploratory approach to using machine learning in Python. It highlights the flexibility of the workflow and functionality which leads Python to be widely applicable in many situations. Participants will be walk away with take-home code samples that they can apply directly to their work.\n \nWe offer a foundation in building intelligent business applications using machine learning, walking you through all the steps to prototyping and production\u00e2\u0080\u0094data cleaning, feature engineering, model building and evaluation, and deployment\u00e2\u0080\u0094and diving into an application for anomaly detection and a personalized recommendation engine. All concepts will be presented with example code in Python.", "Transforming Data to Unlock Its Latent Value\n \nAt the heart of data analysis, there lies a need to understand the real world entities being represented in the data. Every data set we encounter is an attempt to capture a slice of our complex world and communicate some information about it in a way that has potential to be informative to humans, machines, or both. Moving from basic analyses to advanced analytics requires the ability to imagine multiple ways of conceptualizing the composition of entities and the relationships present in our data. It also requires the realization that different levels of aggregation, disaggregation, and transformation can open up new pathways to understanding our data and identifying the valuable insights it contains.\nIn this talk, we\u00e2\u0080\u0099ll discuss several ways to think about the composition and representation of our data. We\u00e2\u0080\u0099ll also demonstrate a series of methods that leverage tools like networks, hierarchical aggregations, and unsupervised clustering to visually explore our data, transform it to discover new insights, help frame analytical problems and questions, and even improve machine learning model performance. In exploring these approaches, and with the help of Python libraries such as Pandas, Scikit-Learn, Seaborn, and Yellowbrick, we will provide a practical framework for thinking creatively and visually about your data and unlocking latent value and insights hidden deep beneath its surface.\n \nThis talk will be about gaining an understanding of the real world entities represented by our data, creatively conceptualizing different perspectives from which our data can be analyzed, and then bringing those conceptualizations to life with the help of Python libraries such as Pandas, Scikit-Learn, Seaborn, and Yellowbrick so that we can unlock the latent value and insights hidden in our data.", "Implementing distributed grid search for deep learning using scikit-learn and joblib\n \nWhile sophisticated machine learning approaches such as deep learning and tree ensembles often produce excellent predictive models, they also typically require performing computationally expensive searches over hyperparameter settings (e.g., the number, sizes, and types of hidden layers for a neural network).  It is desirable to parallelize this search across multiple machines, but such functionality is not readily available in scikit-learn. However, scikit-learn does provide functionality to parallelize grid search across multiple processes on a single machine, using the joblib package. Also, joblib recently added functionality that allows users to implement custom parallel backends to support parallel computation on their own infrastructure. In this talk, I will describe the implementation of a custom joblib backend for an EC2-based auto-scaling computation engine. I will also describe applying this backend to the problem of grid searching deep learning text classifiers, using TensorFlow models that follow the scikit-learn estimator API.\n \nGrid search over hyperparameters is an important but computationally expensive process in machine learning, particularly for deep learning and tree ensembles. In this talk, I will describe how one can use joblib's recently added custom backend functionality to do distributed grid search on Amazon EC2 for a TensorFlow deep text classifier that follows the scikit-learn estimator API.", "Large Scale CTR Prediction - Lessons Learned\n \nAfter briefly introducing Yelp and more specifically click-through rate (CTR) prediction at Yelp, we will start out with a basic setup for model-based predictions in a production system. From there we will point out deficiencies of said setup in various areas, some of which arise especially in large scale environments or when predicting CTRs.\nThis will give us an opportunity to dive deeper into a selection of insightful practical lessons we've learned from operating and scaling the mission-critical CTR prediction system at Yelp. Those lessons can be categorized into:\n\nInfrastructure\nFor example: How can you set yourself up for a successful integration of user feedback?\n\n\nModel Comprehension\nFor example: Do you treat your model as a black box or do you know the importance of each feature? Why might that be valuable?\n\n\nSpecifics\nFor example: When applying a threshold, wouldn't missing training data below the threshold affect the online model performance in that area? (Spoiler: Yes, it does.)\n\n\n\nAlong the way, various Python tools which we're actively using will be highlighted.\nBy touching on a multitude of diverse challenges, this presentation strives to provide valuable insights for any engineer interested in or working with ML models in a production environment.\n \nStarting with a basic setup for click-through rate (CTR) prediction, we will step by step improve on it by incorporating the lessons we've learned from operating and scaling such a mission-critical system. The presented lessons will be related to infrastructure, model comprehension, and specifics like how to deal with thresholds. They should be applicable to most ML models used in production.", "Introduction to Julia for Python programmers\n \nPython users have long benefitted from the less verbose nature of Python, when compared with C and Fortran. However, Python was originally designed for scripting tasks, using dynamic types and widescale object orientation, neither of which features are necessarily beneficial when it comes to numerical computing. Thus, we have seen the widespread use of Python libraries for numerical computation (scipy, numpy, etc.).\nJulia is a new language, developed at MIT, which attempts to learn from the experience of development of Python and similar languages. The main goals are to provide a non-verbose, performance oriented language written from the ground-up to support numerical processing and parallelisation. In its most basic syntax Julia resembles a cross between Matlab and Python, but via compilation through an intermediate level representation (llvm) it offers performance which is comparable to compiled C-code.\nI am not going to argue that Julia is ready for primetime yet. However, it is definitely worth consideration by anyone currently resorting to cython or needing distributed access to large datasets. \nI will present an outline/introduction to the language, including the main benefits and current weaknesses. Of particular interest to the audience may be the fact that Python libraries are importable and callable from within Julia, allowing a continuity of existing workflow but from a Julia-based host environment. My main focus will be for a numerically literate audience who are already contending with the technical limitations of Python and are curious about the new language in town.\n \nJulia is a performance oriented language written from the ground-up to support numerical processing and parallelisation. The basic syntax of Julia resembles a cross between Matlab and Python, but offers performance which is comparable to compiled C-code. I will present an overview of the language with particular emphasis on where Python users may benefit in using it in their daily work.", "PySpark in Practice\n \nAt Pivotal Labs we have many data science engagements on big data. Typical problems involve real-time data from sensors collected by telecom operators to GPS data produced by vehicle tracking systems. One widespread framework to solve those inherently difficult problems is Apache Spark. In this talk, we want to share our best practices with PySpark, Spark\u00e2\u0080\u0099s Python API, highlighting our experience as well as dos and don'ts. In particular, we will focus on the whole data science pipeline from data ingestion, data munging and wrangling to the actual model building. \nFinally, many businesses have started to realise that there is no return on investment from data science if the models do not go into production. At Pivotal Labs, one our core principle is API first. Therefore, we will also talk how we put our models into production, sharing our hands-on knowledge in this field and also how this fits into test-driven development.\n \nIn this talk we will share our best practices of using PySpark in numerous customer facing data science engagements. Topics covered in this talk are\n\nConfiguration\nUnit testing with PySpark\nIntegration with SQL on Hadoop engines\nData pipeline management and workflows\nData Structures (RDDs vs. Data Frames vs. Data Sets)\nWhen to use MLlib vs scikit-learn\nOperationalisation\n", "Unlocking the power of AI: A fundamentally different approach to building intelligent systems\n \nBuilding deep learning systems at present is part science, part art, and a whole lot of arcana. Rather than focusing on the concepts you want the system to learn and how those can be taught, you often find yourself dealing with low-level details like network topology and hyperparameters. \nDatabases solved this problem for data by allowing users to program at a higher level of abstraction. Databases eschew low-level implementation details and instead build a model of the information (the schema) using a high-level declarative programming language (such as SQL). The database server actualizes this model and manages its usage with real data. Similarly, for artificial intelligence, one can build a model for conceptual understanding (the mental model) using a high-level declarative programming language (such as Inkling). A runtime server can then be used to actualize this model and manage its usage with real data.\nKeen Browne explains how Bonsai\u2019s platform enables every developer to add intelligence to their software or hardware, regardless of AI expertise. Bonsai\u2019s suite of tools\u2014a new programming language, AI engine, and cloud service\u2014abstracts away the lowest-level details of programming AI, allowing developers to focus on concepts they want a system to learn and how those concepts can be taught. Keen explores the underpinnings of this technique, details the Inkling programming language, and demonstrates how to build, debug, and iteratively refine models. To make things concrete and fun, Keen demonstrates how to create a system to play the video game Breakout using deep learning (but requiring codifying only the high-level concepts relevant for intelligent play) and offers a curriculum for how to teach this system.\n \nKeen Browne explains how Bonsai\u2019s platform enables every developer to add intelligence to their software or hardware, regardless of AI expertise. Bonsai\u2019s suite of tools\u2014a new programming language, AI engine, and cloud service\u2014abstracts away the lowest-level details of programming AI, allowing developers to focus on concepts they want a system to learn and how those concepts can be taught.", "Yellowbrick: Steering Machine Learning with Visual Transformers\n \nIn machine learning, model selection is a bit more nuanced than simply picking the 'right' or 'wrong' algorithm. In practice, the workflow includes (1) selecting and/or engineering the smallest and most predictive feature set, (2) choosing a set of algorithms from a model family, and (3) tuning the algorithm hyperparameters to optimize performance. Recently, much of this workflow has been automated through grid search methods, standardized APIs, and GUI-based applications. In practice, however, human intuition and guidance can more effectively hone in on quality models than exhaustive search.\nThis talk presents a new Python library, Yellowbrick, which extends the Scikit-Learn API with a visual transfomer (visualizer) that can incorporate visualizations of the model selection process into pipelines and modeling workflow. Yellowbrick is an open source, pure Python project that extends Scikit-Learn with visual analysis and diagnostic tools. The Yellowbrick API also wraps matplotlib to create publication-ready figures and interactive data explorations while still allowing developers fine-grain control of figures. For users, Yellowbrick can help evaluate the performance, stability, and predictive value of machine learning models, and assist in diagnosing problems throughout the machine learning workflow. \nIn this talk, we'll explore not only what you can do with Yellowbrick, but how it works under the hood (since we're always looking for new contributors!). We'll illustrate how Yellowbrick extends the Scikit-Learn and Matplotlib APIs with a new core object: the Visualizer. Visualizers allow visual models to be fit and transformed as part of the Scikit-Learn Pipeline process - providing iterative visual diagnostics throughout the transformation of high dimensional data.\n \nYellowbrick is a new library that extends Scikit-Learn's API to incorporate visualizations into machine learning. While the machine learning workflow is increasingly being automated with gridsearch, APIs, and GUIs, in practice, human intuition outperforms exhaustive search. By visualizing model selection, we can not only steer towards robust models, but also avoid common pitfalls and traps.", "LympHOS2, Organizing and Sharing Biological Data of the Human Lymphocyte Proteome using Python\n \nAt the LP-CSIC/UAB we use a technology called Mass Spectrometry to study the phospho-proteome of human T-Lymphocytes; this is, the group of proteins that are phosphorilated (modified with phosphate groups) in the human T cells during their activation and differentiation as part of the immune response.\nThe experiments involved in this study generates a large amount of complex data:\n\nExperimental conditions and procedures metadata.\nThe spectra data and metadata obtained from the mass spectrometers (usually in proprietary binary formats).\nQualitative or identification data, consisting of search results of those spectra against human protein sequence databases (using multiple search engines, that use different output file-formats) linking the spectra with possible peptides (small protein sequence fragments) that may or may not contain phosphorylated amino-acids.\nSemi-quantitative data about the abundance of each possible identified peptide.\nMissing values, identification scores, phosphorylation reassignments, and a lot of relationships and inter-linked data...\n\nAnd all this data has to be processed, stored, curated, and made easily accessible to researchers in our lab and worldwide, so they can study it to obtain biomedical knowledge about the phosphorylation changes in peptides and proteins involved in the signal transduction pathways of T cells after their activation during the specific immune response.\nWe have used different Python packages to develop different tools and applications to accomplish those objectives:\n\nThe EasierMgf front-end application, to extract plain text spectra data from proprietary binary mass spectrometer files, was developed using the wxPython GUI toolkit.\nThe ORM from the django framework was used to design and interact with the LymPHOS2 MySQL database, which stores all the data and their inter-relationships.\nAlso, different modules from the Python standard library (json, xml.etree.ElementTree, csv, zipfile) were used to read and import into the database the different files containing the data; and to export database data into commonly used file formats for the researches to work with.\nThe PQuantifier group of tools was developed to do the statistical processing and analysis of the LymPHOS2 semi-quantitative data. It uses the SQLAlchemy ORM for fast database storage in local SQLite files,  the NumPy N-dimensional array package, the SciPy scientific computing library for the statistics, the uncertainties  package for calculations with error propagation, and the matplotlib 2D plotting library for some nice plots of data distributions.\nAnd the full django framework itself was used to develop the LymPHOS2 web application. Which also uses the matplotlib library to dynamically generate the mass spectra images.\n\nThe final result is the LymPHOS2 web-oriented database, that nowadays (2017) contains 131.908 mass spectra, 15.566 phosphorylation sites from 8.273 unique phospho-peptides and 4.937 proteins (which represent a 45-fold increase over the original LymPHOS database of 2009); aside from the new quantitative data for 1.975 of the identified phospho-peptides, which was not present in the previous version of LymPHOS.\n Repositories and Presentation Slides: \n\nBitbucket code repositories: https://bitbucket.org/lp-csic-uab/\nPresentation Slides (.ODP, .PDF and .PPTX): https://bitbucket.org/lp-csic-uab/lymphosdocs/downloads/\n\n Credits: \nThe exposed work has been carried out at LP-CSIC/UAB from Catalonia, part of the Spanish National Research Council (Consejo Superior de Investigaciones Cient\u00edficas - CSIC) and of ProteoRed (Proteomics National NetWork Platform).\nThe people who have participated directly in the current work are:\n\nData analysis, bioinformatics and informatics: Joaquin Abian and \u00d3scar Gallardo.\nMass Spectrometry, experimental design and implementation: Montserrat Carrascal, Nguyen Tien Dung and Oriol Vidal-Cortes.\nSample preparation: Montserrat Carrascal, Nguyen Tien Dung,  Oriol Vidal-Cortes and Vanessa Casas.\nPast collaborators: David Ovelleiro and Marina Gay.\nDirection: Joaquin Abian.\n\n \nThe study of the proteome (the set of proteins expressed by a cell, tissue, or organism at a defined time and  conditions) generates a large amount of complex data. This data should be processed, stored, curated, and made easily available to researchers so it can be studied to obtain biomedical knowledge. In this talk we expose the Python tools we have used and developed to accomplish it.", "Best Practices for Debugging\n \nDebugging is a daily activity of any programmer. Frequently, it is assumed that programmers can debug. However, programmers often have to deal with existing code that simply does not work. This tutorial attempts to change that by introducing concepts for debugging and corresponding programming techniques.\nIn this tutorial, participants will learn strategies for systematically debugging Python programs. We will work through a series of examples, each with a different kind of bug and with increasing difficulty. The training will be interactive, combining one-person and group activities, to improve your debugging skills in an entertaining way.\nContents:\n\nSyntax Error against Runtime exceptions\nDebugging with the scientific method\nInspection of variables with print and introspection functions\nUsing an interactive debugger\nLogging\nCode Review\n\n \nThis tutorial introduces concepts and techniques for systematic debugging. Participants will debug example programs with different kinds of bugs and with increasing difficulty.", "A Practical Guide to Dimensionality Reduction Techniques\n \nMany machine learning applications involve datasets with high dimensionality. In most cases, the intrinsic dimensionality is much smaller than the observed dimensionality of the data, and it becomes imperative to eliminate unavailing and redundant features before performing the core analysis. This is useful not only because it speeds up the core part of the analysis which typically involves complicated algorithms, but it may also improve the accuracy of the analysis (such as the classification accuracy of a classification model). Multiple feature reduction techniques have been proposed and utilized by statisticians and data scientists over the years. These methods can be broadly categorized into two groups: supervised and unsupervised methods. \nIn this presentation, I will focus on several dimensionality reduction techniques pertaining to the pre-processing of data for supervised learning. A large data-set will be used to demonstrate these techniques by following a dimensionality reduction work-flow. The objective of this presentation is to introduce several dimensionality reduction techniques, demonstrate how to implement them (using Python), and assess their efficacy as it relates to supervised learning problems.\nHere\u00e2\u0080\u0099s a list of twelve dimensionality reduction techniques that will be discussed in this presentation: (1) Percent missing values, (2) Amount of variance, (3) Correlation (with the target), (4) Pairwise correlation, (5) Multicollinearity, (6) Principal Component Analysis (PCA), (7) Cluster analysis, (8) Forward selection, (9) Backward elimination, (10) Stepwise selection, (11) LASSO, (12) Tree-based methods.\n \nThis talk provides a step-by-step overview and demonstration of several dimensionality (feature) reduction techniques. Attendees should have some basic level of understanding of data wrangling and supervised learning. The presentation will also include snippets of Python code, so familiarity with Python code will be useful. ", "Testing is Fun in Python!\n \nTesting is vital to the success of any software, including big data and analytics code.  Unfortunately, it is often regarded as a \u00e2\u0080\u009cnecessary evil\u00e2\u0080\u009d \u00e2\u0080\u0093 extra work that slows down progress.  In this session, I will highlight how testing in Python can be fun, easy, fast, and helpful.\nFirst, I will give a brief overview of basic best practices for testing.  We will talk about the difference between debugging and testing, different types of tests, how to write good test cases, and basic testing fixtures like assertions and results.  I will focus on unit testing, but the concepts can be applied to higher levels of testing as well.\nThen, for the majority of the session, I will introduce different Python test frameworks:\n- unittest as the standard module for unit test classes.\n- doctest as a lightweight way to write short, self-documenting assertions in docstrings.\n- py.test as a way to write very concise test cases.\n- Nose as an extension of unittest with added features.\n- Avocado as a comprehensive framework with parameters, replay, and test discovery.\nThis talk is designed to be useful to Python programmers of any skill level.  Only a basic understanding of Python is required.\n \nTesting software is just as important in Python as it is in any other programming language.  Rather than treat testing as a \u00e2\u0080\u009cnecessary evil,\u00e2\u0080\u009d Python offers a number of versatile test frameworks to make it fun and easy.  This talk will cover basic testing best practices and introduce a few of the popular frameworks, including unittest, doctest, py.test, Nose, and Avocado.", "Embracing the Monolith in Small Teams: Doubling down on python to move fast without breaking things.\n \nProducts are more than code\n\nGood architectures gives you flexibility \nChoosing right tools is a biz decision: why Django\nOvercoming the operational vs analytical data workload disconnect(age-old OLAP vs OLTP) with Postgres.\n\nData Products are more than models\n\nCoupling your Django models with pandas dataframes: a sharp knife.\nSeamlessly incorporating external sources with Segment.com\nThe right thing, to the right person, at the right time: smart notifications.\nAppearing smart with dumb safeguards around noisy data.\n\nFeeding back into the business\n\nIPython as a BI tool: how a little python can go a long way.\n\"If you have to copy-paste a dashboard, you're not serious about tracking that\"\nSlicing and dicing customer engagement data\nWhat Sales didn't know they could have: engagement data as a lead-gen tool. \n\n \nIts unlikely for small orgs to have resources to support the fanciest and shiny data tooling out there. Choosing the right tools and tightly coupling them, can can give small, cross-disciplinary teams the power of moving fast, without breaking things. This goes against a lot of common advice, but becomes powerful when applied with disciplines. I'll show you how with Django, IPython, and Postgres.", "How Soon is Now:  automatically extracting  publication dates of news articles with machine learning\n \nScraping New York Times articles for publication dates is easy, scraping 10 000 different sites is hard. Beyond page-specific scraping, how do you build a parser than can extract the publication date of (almost) any news article online, no matter what the site is?  We implemented a research paper in machine learning to solve this problem, and talk about the challenges we faced.\nWe\u00e2\u0080\u0099ll cover when to use machine learning vs. humans or heuristics for data extraction, the different steps of how to phrase the problem in terms of machine learning, including feature selection on HTML documents, and issues that arise when turning research into production code. Data scientists and developers will leave knowing how to extract information from the web using new and more sophisticated techniques than simply writing a scraper. \n \nScraping New York Times articles for publication dates is easy, scraping 10 000 different sites is hard. Beyond page-specific scraping, how do you build a parser than can extract the publication date of (almost) any news article online, no matter what the site is?  We implemented a research paper in machine learning to solve this problem, and talk about challenges we faced.", "Python based predictive analytics with GraphLab Create\n \nThe most interesting apps utilize multiple types of data (tables, graphs, text & images) in a creative way. In this talk, we will show how to quickly build and deploy a predictive app that exploits the power of combining different data types together using GraphLab Create, our open source based Python software.\n \nOne of the most exciting areas in data science is the development of new predictive applications; apps used to drive product recommendations, predict machine failures, forecast airfare etc. These applications output real-time predictions and recommendations in response to user and machine input to directly derive business value and create cool experience. ", "Survival Analysis in Python and R\n \nMany problems involve the understanding the duration of specific events; for example, predicting when a customer will churn, when a person will default on a credit, how long a machine will work, etc. These type of questions constitute the realm of Survival analysis, a branch of statistics historically developed by professionals in the actuarial and medical fields dealing with event durations as governed by probability laws.\nIn this talk I will cover the basics of Survival analysis via examples implemented via Lifelines, an open-source python library and in R (survival and KMsurv libraries), going from survival curves to regression models.  I will discuss how survival analysis can be applied to a variety of problems and in particular, I will focus on the problem of out of stock prediction for an online retailer.\n \nSurvival analysis is a set of statistical techniques that has many applications in the industry.  This talk will discuss key concepts behind survival analysis by means of examples implemented via Lifelines, an open source python library, and in R for comparison purposes. I will also describe how we have made use of these techniques in Lyst to try to predict when items go out of stock.", "Robust Automated Forecasting in Python and R\n \nThis talk will cover how to make millions of time series forecasts in an automated fashion. We will be covering helpful heuristics to inform preprocessing, tradeoffs between contextual evaluation metrics (and meta-metrics), useful libraries for employing different forecasting techniques in Python and R, and how to choose the best hardware for forecasting given cost and runtime constraints.\n \nThis talk will cover how to make millions of time series forecasts in an automated fashion. We will be covering helpful heuristics to inform preprocessing, tradeoffs between contextual evaluation metrics (and meta-metrics), useful libraries for employing different forecasting techniques in Python and R, and how to choose the best hardware for forecasting given cost and runtime constraints.", "Diversity and Data: Cases in the Music Industry\n \nI would like to present a few challenging questions that I have encountered at PPL:\n\n\nRolling up music and musicians: for instance, Beyonce, Bey, Queen Bee, Beyonce feat. Jay-Z, Beyonc\u00e9 feat jayz all should ideally come under the parent of Beyonc\u00e9. A lot has to do with normalising the strings. But an additional layer, with large potential for improving the predictive power, is introducing aliases and leveraging PPL's as well as external third-party data to identify potential aliases. \n\n\nIdentifying turning-point collaborations: which partnerships among musicians are most likely to change their careers significantly? This involves looking at: firstly, whether music collaborations (including band memberships) are changing in nature. Secondly, asking how to identify the value added of collaborators, i.e. the effect of certain collaborations on the artist's profile, recognition and success.\n\n\nHow to identify outliers at the artist level? The distribution of many variables of interest (band memberships, active years etc.) is highly heterogeneous across artists, and contain a few suspicious outliers that are hard to identify from aggregate distributions.\n\n\n \nWith lower barriers to entry and rise of streaming, the volume of new songs and artists is exponentially increasing. Given these trends in the music industry, I would like to look at a few challenging problems I encountered at PPL. Solving these problems will improve the efficiency with which we churn through millions of observations and therefore minimise any delay in allocations of royalties.", "How to map a protein structure into a network graph to understand protein folding.\n \nFrom DNA, the genetic code is translated into proteins being these macro molecular structures in charge to execute specific cellular functions. All proteins are composed by the combination of the same 20 amino acids, indeed, it is the 3D shape and detailed structure that differentiate a protein from another and so their functions. Determining protein structure is fundamental to understand their biological function; modify and alter their structures is the way in which drugs can block or activate a protein activity and doing so preventing or treating diseases. \nIn our lab we are understanding protein and DNA structures to the level of atomic resolution contributing to the biomedical research. We process with python the information of hundred thousands 3D coordinate structures already known and predict patterns that can be used to determine structure of unknown proteins. To achieve this objective, proteins are first reduced to a series of specific 3D vectors (Characteristic Vectors [1]) and then the geometrical relationships among them are extracted. We make use of biopython and python-igraph libraries to model each structure vectors in a network graph and we use supervised and unsupervised learning to classify and predict structure features. Python-igraph is a wrapper to a C code implementation of a set of graph algorithms of classical use in network analysis.\n[1] Sammito M1, Mill\u00e1n C, Rodr\u00edguez DD, de Ilarduya IM, Meindl K, De Marino I, Petrillo G, Buey RM, de Pereda JM, Zeth K, Sheldrick GM, Us\u00f3n I. Exploiting tertiary structure through local folds for crystallographic phasing. Nat Methods. 2013 Nov;10(11):1099-101. doi: 10.1038/nmeth.2644. Epub 2013 Sep 15.\n \nIn all living systems any biological function is carried out by the interactions of proteins. The specific function of a protein is determined by its own 3d shape that can be modeled in a list of single atom coordinates derived from experimental analysis. In our lab we use python to process the information of hundred thousands structures already known to predict common patterns in protein folding.", "Building smart IoT applications with Python and Spark\n \nThe Internet of Things and Industry 4.0 are here, bringing along a vast amount of connected devices and sensors producing even more data.\nIn order to build smart applications on top of IoT sensor data we need to deal with the challenges that come along time-series data from a large amount of devices.\nAt WATTx we build data application prototypes in the field of smart homes, smart buildings, and smart climate, which involves making use of data coming from many IoT sensors measuring -- amongst others -- temperature, humidity, motion, and luminance.\nThe purpose of this talk is to present how we use Python and Spark to effectively analyse and model IoT data. In particular I will introduce how we use Python to process and model data from multiple IoT sensors, build machine learning models on top of it, and use Spark to scale and deploy our models in automated data pipelines in the cloud as smart IoT applications.\nI will use the development of predictive models for smart building applications as a real-world example to demonstrate this setup.\nI hope that this talk will give valuable insights on how Python and PySpark in conjunction with AWS are powerful tools to work with time-series sensor data from the Internet of Things and build data products on top of it.\n \nIn this talk I will present how we use Python, PySpark and AWS as our preferred data science stack for the Internet of Things, which allows us to efficiently develop and deploy smart data applications on top of IoT sensor data. We use these technologies to analyse and model IoT timeseries data, as well as to build automated and scalable data pipelines for smart IoT data applications in the cloud.", "Building Serverless Machine Learning Models in the Cloud\n \nWe will describe the main challenges faced by data scientists involved in deploying machine learning models into real production environments with specific references, examples of Python libraries, and multi-model systems requiring advanced features such as A/B testing and high scalability & availability.\nWhile discussing the advantages and limitations of multiple deployment strategies in the cloud, we will focus on serverless computing (i.e. AWS Lambda) as a solution for simplifying your development & deployment workflows.\n \nYou\u00e2\u0080\u0099ll learn how to efficiently design and train machine learning models in Python and deploy them to the cloud. This process reduces the development & operational efforts required to make your prototypes production-ready.", " Introduction to Zeppelin Notebooks and PySpark 2.0\n \nApache Zeppelin is interactive, multi-purpose, data analytics environment for distributed data processing system. It provides beautiful interactive web-based interface, data visualization, collaborative work environment and many other nice features to make your data analytics more fun and enjoyable. This talk will provide a brief overview (via live demo) of some of Zeppelin's key features such as it's pluggable architecture for backend integration, drag and drop visualizations, dynamic forms, notebook persistence, Shiro and notebook authorization, and it's ability to share variables BETWEEN contexts )E.g. the results of a Flink paragraph can be passed to a Spark paragraph; the best tool can be used for the job can be used at each step in analytics pipeline and a data scientist who loves Scala Flink can easily work with a data scientist who loves pyspark.)\nLive demo will utilize Zeppelin Notebook's built in PySpark interpreter (will use the just released Spark 2.0 API). This talk will also explore where Zeppelin fits into the larger data science/big data ecosystem, discuss similarities and differences with Jupyter Notebooks,and why Zeppelin has a bright future despite being a late entrant into an already crowded notebook/interactive analytics space. \n \nApache Zeppelin  is interactive data analytics environment for distributed data processing system. This talk will give a brief overview of what Zeppelin is and where Zeppelin fits into the larger data science/big data ecosystem, discuss how it differs from Jupyter  and cover several of Zeppelin's key features via a live demo use the integrated (and just released) PySpark 2.0 interpreter . ", "Creating a Contemporary Risk Management System Using Python\n \nEngineering a sophisticated in-house risk management solution for a commercial lending platform doesn't necessarily need to involve millions of lines of low-level code, clunky desktop applications, fancy front-end development, or messy spreadsheets. That is, so long as the problem is approached objectively and the solutions are evaluated critically.\nThis talk will focus on the basics of lending risk mitigation (related to underwriting and portfolio management), a high-level overview of the architectural requirements, and the packages that were leveraged along the way; namely: \n\npandas for data preparation\nOrganizing panels and cleaning time series data\n\n\nstatsmodels and scikit-learn for regression and classification\nPredicting accounts receivable discontinuation and bankruptcy probabilities\n\n\nR/rpy2 for integrating R's advanced forecasting capabilities\nForecast comparison using the forecast package and the bsts package\n\n\nSpyre for designing a lightweight and easily maintainable web application\n\nMost importantly we'll share a demo of the web application to see how it all comes together. To be clear, this IS NOT a product offering or any kind or SaaS; it was built to be used internally and generalizes well for other use cases.\n \nLending involves risk and in order to be a successful lender at scale that risk needs to be mitigated. We'll be discussing how C2FO has built a suite of risk management tools for underwriting and portfolio management using the PyData ecosystem, rpy2 (for integrating R), and Spyre (for building a simple web application).", "Resampling techniques and other strategies for handling highly unbalanced datasets in classification\n \nI will discuss techniques to combat class imbalance in classification problems. These mainly fall into the following categories:\n\nAsymmetric loss functions: Penalize misclassifications on the class of interest higher (often the minority class)\nResampling techniques:\nUndersampling the minority class\nOversampling the majority class\nSMOTE (Synthetic minority oversampling technique)\nTomek link removal \nCondensed nearest neighbors\nEdited nearest neighbors\n\n\nEnsemble techniques\nEasyEnsemble\nBalanceCascade\n\n\n\n \nMany real world machine learning problems need to deal with imbalanced class distribution i.e. when one class has significantly higher/lower representation than the other classes. Often performance on the minority class is more crucial, e.g. fraud detection,  product classification, medical diagnosis, etc. In this talk I will discuss several techniques to handle class imbalance in classification. ", "Holy \n \nOverview\nThis talk will cover how to handle and manage working with unclean and imperfect datasets. We'll cover several issues and suggestions as well as some code examples for managing that messy data.\nOutline\n\nThe Noble Quest against Messy Data\nWorking with null data\nInsignificant data\nMessy strings\n Regex\n Fuzzy Match\nXML / HTML Data\nPDF Data\nWhere to go from here\n\n \nEver wondered what sort of sick person created the datasets you work with? Sadly, we can't answer that question directly, but we can aim to handle messy data problems. From the non-significant or null datasets, to unclean and unclear string data, to difficult formats like PDFs, we'll take a closer look at how to best work with imperfect data and what questions you can answer given your datasets.", "Iterables and Iterators: Going Loopy With Python\n \nThe talk's outline is given below. It will take approximately 25 minutes to deliver the material, the remainder being reserved for Q&A.\n\n\nIteration History (4 minutes)\n\n\nEnter the Iterable (10 minutes)\n   Recognizing Iterators\n   Iterating over Iterators\n   Why we Need Iterables\n\n\nWriting Your Own Iterators and Iterables (10 minutes)\n   The Basic Iterator Pattern\n   The Advantages of Generators\n   The Basic Iterable Pattern\n   Python Native Iterables\n\n\nSummary (1 minute)\n\n\n \nThis talk describes how the interpreter iterates over containers when any construct involving the for keyword is used. It explains both the \"old\" and the \"new\" iteration protocols, demonstrates the difference between iterables and iterators, suggests implementation techniques to allow you to define your own iterables and iterators and points out some advantages of generators.", "In-database Machine Learning with Python in SQL Server - Sponsor Talk\n \nIn this session, we will take a deeper look at the SQL Server Machine Learning Services with Python and its enterprise grade advantages. This enables you to use the latest innovations from the open source Python world on your data that is securely stored in SQL Server and provides additional scale, performance, operationalization and management capabilities. We will walk through a demo to build and deploy a Python based machine learning application in SQL Server to predict how long a patient is likely to stay in the hospital at the time of getting admitted to the hospital.\n \nSQL Server 2017 has built in AI capabilities that bring Python based intelligence to where the data lives.", "How distributed representations make chatbots work  (at least a bit)\n \nIt is hard to go anywhere on the web these days without encountering chatbots and other natural language interfaces. But how do these bots actually understand what you say? It turns out it can be boiled down to a simple recipe: you need to know which words mean similar things! This sounds straight-forward, but efficient ways of doing this, namely distributed representations, were only just discovered in the past few years of machine learning research. They have immense potential and we are only beginning to realise what we can do with them.\nIn this talk I want to outline how distributed representations are used at babylon health, and how everyone can incorporate sophisticated language understanding into their applications with just a few lines of python. Furthermore I will give a glimpse of the research we are doing, some of which we just published in a paper at ICLR.\nSimple outline of the talk:\n What are distributed representations and how do they work?\n What are they good for, what are they bad at?\n Some examples on how we use them at babylon health\n Some interesting results from our research\nI will do my best to give practical examples throughout the talk and will provide python snippets in an accompanying repository, which should make this talk accessible to a wide audience!\n \nChatbots are all the hype right now, like it or not. In this talk I want to take a look under the hood and show you how simple it is today to incorporate sophisticated language understanding in your applications. The tool of choice are distributed representations, also known as word vectors, which allow us to answer the most crucial question: which of these words mean similar things?", "Blockchain and smart contracts explained (and simplified)\n \nSmart contracts are an old and simple concept. They are just computer programs that facilitate the negotiation and settlement of contracts by several parties. Their goal is to lower the transaction latency and cost associated with human middlemen. The blockchain technology brought the possibility to sanction contracts when peers do not trust each other, which is necessary for applications like virtual currencies. However, due to the CAP theorem and the cost of the Byzantine consensus, their performance tends to be quite bad.\nCurrent smart contract platforms such as Ethereum are general (one can issue any kind of contract), distributed (an updated copy is present at all the nodes within the same network), shared (an arbitrary amount of contracts are served within the same infrastructure) and secure (there is no way to tamper with the stored data). All those properties are undeniably good, but the consequence of keeping all those promises is that the infrastructure is exceedingly complex. Such an extense architecture does not add any functionality to a smart contract, it makes it trustworthy.\nIf one understands the raw concepts behind the Blockchain consensus, and we are willing to sacrifice some of the properties mentioned earlier, one can design and implement a simple ledger for smart contracts like pyledger (pyledger.readthedocs.io). Pyledger was designed as a platform for fast prototyping of smart contracts, and for experimentation, but in some particular cases can be good enough to be used in real-world cases.\nFinally... Fun time! All the concepts will be exemplified with a Q&A contest implemented as a smart contract where all attendants can play.\n \nBlockchain is the next big thing that will revolutionize everything, but no one knows how. The same can be said about smart contracts.  Bitcoin and Ethereum are great technologies, but too complex for most cases. This talk will explain why smart contracts are hard, and why they don't need to be. Finally, a simple ledger for smart contracts in python (pyledger) will be presented.", "Is That a Duplicate Quora Question?\n \nQuora released its first ever dataset publicly on 24th Jan, 2017. This dataset consists of question pairs which are either duplicate or not. Duplicate questions mean the same thing.\nFor example, the question pairs below are duplicates (from the Quora dataset)\nHow does Quora quickly mark questions as needing improvement? Why does Quora mark my questions as needing improvement/clarification before I have time to give it details? Literally within seconds\u2026\nWhy did Trump win the Presidency? How did Donald Trump win the 2016 Presidential Election?\nWhat practical applications might evolve from the discovery of the Higgs Boson? What are some practical benefits of discovery of the Higgs Boson?\nSome examples of non-duplicate questions are as follows:\nWho should I address my cover letter to if I'm applying for a big company like Mozilla? Which car is better from safety view?\"\"swift or grand i10\"\".My first priority is safety?\nMr. Robot (TV series): Is Mr. Robot a good representation of real-life hacking and hacking culture? Is the depiction of hacker societies realistic? What mistakes are made when depicting hacking in \"\"Mr. Robot\"\" compared to real-life cybersecurity breaches or just a regular use of technologies?\nHow can I start an online shopping (e-commerce) website? Which web technology is best suitable for building a big E-Commerce website?\nIn this talk, we discuss methods which can be used to detect duplicate questions using Quora dataset. Of course, these methods can be used for other similar datasets.\nMethods discussed in this talk range from simple TF-IDF, Singular Value Decomposition, Fuzzy Features, Word2Vec features, GloVe features, LSTMs and 1D CNN. We provide a comparison of performance of these algorithms on the Quora dataset.\nUsing very simple methods and then using deep learning methods in python we achieved an accuracy similar to current state of the art on a normal machine with only one GPU.\n \nQuora released its first ever dataset publicly on 24th Jan, 2017. This dataset consists of question pairs which are either duplicate or not. Duplicate questions mean the same thing. In this talk, we discuss methods which can be used to detect duplicate questions using Quora dataset. Of course, these methods can be used for other similar datasets.", "Forecasting critical food violations at restaurants using open data\n \nDetailed Abstract\nIn 2014, data scientists at the Department of Innovation and Technology for the city of Chicago built an algorithm to predict likely health code violations for restaurants based on publicly available data in an attempt to reduce foodborne illness. They turned this into a freely available open source project, available on Github in R. However, in spite of the prevalence of foodborne illness and its associated costs (as much as $2\u00e2\u0080\u0093$4 billion annually1), so far only one other location in the country has taken advantage of Chicago's work to implement this model.2  That place is Montgomery County, MD which, with the assistance of Open Data Nation, is successfully adapting the model to the local environment.\nThis talk provides an end-to-end demonstration of how to replicate the process using Python and open data from Washington, DC. The content is targeted toward the novice data scientist and will discuss the practical aspects of planning and executing the project. Learn how you can combine Python libraries like Requests, BeautifulSoup, Sqlite, Numpy and Sckit-Learn to build your own machine learning model to predict health code violations!\nOutline\nIntroduction/Problem statement\n\nFoodborne illness outbreaks affect millions of people annually\nDC has 22 inspectors responsible for ~5500 food establishments in addition to other facilities\nPrioritization of routine inspections handled by risk category\n\nData Science Pipeline\n\nData Ingestion - Identification to build a dataset for modeling using scraped, API, and open city data\nData Munging and Wrangling- Getting the data to work together\nComputation and Analyses- Do you have what you think you have? Feature engineering\nModeling and Application- Algorithm identification, building a pipeline\nReporting and Visualization - What does the data say?\n\nLessons learned\n\nProblem identification\nWhen the data you thought was available is not\nClass imbalance in existing data\nDomain expertise\n\n\n\n\n\nHealth Department Use of Social Media to Identify Foodborne Illness \u00e2\u0080\u0094 Chicago, Illinois, 2013\u00e2\u0080\u00932014\u00a0\u21a9\n\n\nPredictive Policing Comes to Restaurants\u00a0\u21a9\n\n\n\n \nAs many as 105 million Americans suffer from foodborne illness annually. In 2014, the City of Chicago began forecasting these outbreaks targeting limited health inspection resources toward likely sites, showing a 7 day improvement in locating critical violations at food establishments. This talk provides an end-to-end walkthrough of predicting critical violations in Washington, DC using Python.", "A pythonista\u00e2\u0080\u0099s pipeline for large-scale geospatial analytics \n \nLarge-scale geospatial data is a common analytical challenge. We receive billions of location data points per day for user locations. We need to filter this data and map it to millions of points of interest. Our end use case is to provide clients with information as to the attributes of users who visit their stores. This workflow, however, has application to the broader analytical problem of mapping geospatially located entities of interest to points of interest with known locations. We also provide our approach to the frequently encountered issue of needing both standardized and flexible reporting. We have automated the standard analyses in our workflow, but created an API for analysts to quickly develop ad-hoc analyses based on customer requests. \nThere are freely available tools to the Python user that can help us complete all of the tasks in our data analysis pipeline. At a high level, the architecture of our process is as follows: HDFS storage for large-scale geospatial data, Spark for geospatial joins, Cloudera Impala accessed via Ibis to query resultant datasets, and scientific python for conducting analyses. We make the automated analyses to the end users via a web portal created using Flask and Celery. We created an API available to analysts as a Python package so that they can quickly perform custom analyses created by clients. We used Sphinx to aid in documentation for easier use of the API.\nIn this talk we describe our architecture and workflow to accomplish the following analytical tasks related to user location:\n1.) Estimation of the user\u00e2\u0080\u0099s approximate home location (within a few miles)\n2.) Estimation of approximate demographic characteristics of visitors to a location\n3.) Distribution of distances traveled to reach a specific location\n4.) Distribution of visit propensities over time\nWe also describe an analytic technique used to statistically determine the impact of treatment on user\u00e2\u0080\u0099s geographical behavior patterns using propensity-based test and control groups.  \nPython is the only language with tools readily available to efficiently complete this task end-to-end.\n \nWe created an automated framework for analysis of large-scale geospatial data using Spark, Impala, and Python. We use this framework to join billions of device locations daily from mobile phone users to millions of points of interest. We discuss our project structure and workflow. Our work has broader application to movement of populations through time and space. ", "Python Framework for Zero-shot Learning Approach: An Application on Network Intrusion Detection.\n \nNetwork intrusion detection (NID) is one of the most visible uses for Big Data analytics. One of the main problems in this application is the constant rise of new attacks. This scenario, characterized by the fact that not enough labeled examples are available for the new classes of attacks is hardly addressed by traditional machine learning approaches. New findings on the capabilities of Zero-shot learning (ZSL) approach makes it an interesting solution for this problem because it has the ability to classify instances of unseen classes. This approach has been widely applied to computer vision related tasks but from the best of our knowledge it has not been applied to other kind of problems. ZSL has inherently two stages: the attribute learning and the inference stage. \nBased on scientific Python frameworks such as: Scikit-learn, Scipy and Numpy we developed a framework for the application of ZSL on Network Intrusion Detection. We propose a new algorithm for the attribute learning stage. The idea is to learn new values for the attributes based on decision trees (DT). Our results show that based on the rules extracted from the DT a better distribution for the attribute values can be found. We also propose an experimental setup for the evaluation of ZSL on NID. Then, for the inference stage we propose an instance-based classification algorithm representing the data on the Grassmann manifold and implementing a new distance function to compute the k-Nearest Neighbours. On this stage around of 5 millions of data instances are processed on real time in different batches simulating a real network traffic. \nIn this talk we present our framework implementation details as well as its deployment on a Big Data environment. The obtained results show a competitive accuracy in both detecting the new classes of attacks as well as in its classification.\n \nBased on scientific Python frameworks we developed a framework for the application of Zero-shot Learning on Intrusion Detection. On the attribute learning stage we propose a new algorithm based on decision trees. The inference stage is based on a k-NN algorithm representing the data in a Grassmann manifold. We present our implementation details as well as the results on a Big Data environment.", "Transfer Learning and Finetuning Deep Convolution Neural Network on different domain-specific images\n \nABSTRACT:\nIn this talk, we propose prediction techniques using deep learning on different types of images datasets \u00e2\u0080\u0093 medical images and fashion images. We show how to build a generic deep learning model, which could be used with \u00e2\u0080\u0093 \n1.  A fluorescein angiographic eye image to predict Diabetic Retinopathy\n2.  A fashion image to predict the clothing type in that image\nWe propose a method to apply a pre-trained deep convolution neural network (DCNN) on images to improve prediction accuracy. We use an ImageNet pre-trained DCNN and apply fine-tuning to transfer the learned features to the prediction. We use this fine-tuned model on two very different domain specific datasets. Our approach improves prediction accuracy on both domain-specific datasets, compared to state-of-the-art Machine Learning approaches.\nTALK OUTLINE:\n1.  Brief introduction to Deep Learning\n2.  Motivation behind using Deep Learning models for Images:\nMuch work has been done in developing state-of -the-art machine learning algorithms and morphological image processing techniques, that explicitly extract features prevalent in images. The generic workflow used in a standard image classification technique is as follows:\n\u00e2\u0080\u00a2   Image preprocessing techniques for noise removal and contrast enhancement. \n\u00e2\u0080\u00a2   Feature extraction technique\n\u00e2\u0080\u00a2   Classification/Prediction\nHowever, these explicit feature extraction processes are very time and effort consuming. Further improvements in prediction accuracy require large quantities of labeled data. Image processing and feature extraction of image dataset is very complex and time-consuming. Thus, we choose to automate the image processing and feature extraction step by using DCNNs. \n3.  Transfer Learning & Fine-tuning DCNNs:\nCurrent trends in the research have demonstrated that DCNNs are very effective in automatically analyzing large collections of images and identifying features that can categorize images with minimum error. DCNNs are rarely trained from scratch, as it is relatively uncommon to have a domain-specific dataset of sufficient size. Since modern DCNNs take 2-3 weeks to train across GPUs, Berkley Vision and Learning Center (BVLC) have released some final DCNN checkpoints. In this work, we use an ImageNet pre-trained DCNN - GoogLeNet. GoogLeNet, which was developed at Google, won the ImageNet challenge in 2014, setting the record for the best contemporaneous results. Motivations for this model were a simultaneously deeper as well as computationally inexpensive architecture.\n4.  Deep Learning models for Image Classification:\nDiabetic retinopathy (DR) eye disease is a common cause of vision loss. Screening diabetic patients based on the diabetic retinopathy symptoms in fluorescein angiography (FA) images can potentially reduce the risk of blindness. Data was drawn from a dataset maintained by EyePacs, and provided via Kaggle. The dataset is composed of multiple, smaller datasets of fundus photographs drawn from various sources. We fine-tune the pre-trained generic DCNN to recognize fluorescein angiography images of eyes and improve DR prediction. Our approach is an end-to-end learning strategy, with minimum assumptions about the contents of images. We show that our approach improves DR prediction accuracy upon the results produced by the Support Vector Machine Approach.\n5.  Deep learning model for Image Tag Prediction:\nGiven the role of clothing apparel in society, fashion classification has many applications. We will focus on optimizing fashion classification for the purposes of annotating images and predicting clothing tags for the fashion images. In this work, we apply a pre-trained Convolutional Network (CNN) and Long short-term memory (LSTM) Recurrent Neural Network (RNN) on Apparel Classification with Style (ACS) images to improve Fashion Image Tag (FIT) prediction. We combine DCNN for fashion image classification with recurrent neural networks (RNN) for sequence modeling, to create a single network that generates clothing tags for images. We fine-tune the same ImageNet pre-trained GoogLeNet model to extract ACS image features. These CNN-features are in turn used as input to the LSTM RNN model to generate tags for images. The RNN is trained in the context of this single \u00e2\u0080\u009cend-to-end\u00e2\u0080\u009d network. Our approach improves FIT prediction accuracy compared to state-of-the-art approaches.\n6.  For this work, we have used all open source software \u00e2\u0080\u0093 \nI.  Theano/Lasagne:\nTheano is a software package which allows us to write symbolic code and compiles it onto different architectures (in particular, CPU and GPU). It is a Python library that allows us to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. It was developed by machine learning researchers at the University of Montreal. Theano is efficient for deep learning of data. Lasagne is a lightweight library to build and train neural networks in Theano.\nII. Pre-trained model weights:\nMany researchers and engineers have made Caffe models for different tasks with various DCNN architectures and data. These models are learned and applied for problems ranging from simple regression, to large-scale visual classification, to speech and robotics applications. Several pre-trained model weights are shared by Berkeley Vision and Learning Center (BVLC) via the model zoo framework. Lasagne has implementations for both ImageNet-trained VGG with 16-layers and ImageNet-trained GoogLeNet. The weights for both these models are stored as pickle files. We leverage these pre-trained models for DR prediction, using them as initial weights for fine-tuning the models.\nIII.    GPU, CUDA and cuDNN:\nTraditional machine learning uses handwritten feature extraction and modality-specific machine learning algorithms to label images or recognize voices. However, this method is computationally very expensive. Advanced deep neural networks use algorithms, big data, and the computational power of the graphics processing units (GPUs) to change this dynamic. CUDA (Compute Unified Device Architecture) is a parallel computing platform and programming model created by NVIDIA and implemented by the GPUs that they produce.\nThe NVIDIA CUDA Deep Neural Network library (cuDNN) is a GPU-accelerated library of primitives for deep neural networks. cuDNN provides highly tuned implementations for standard routines such as forward and backward convolution, pooling, normalization, and activation layers.\nHere, we use one NVIDIA Quadro K1200 GPU, CUDA 7.5 and cuDNN 5.\n7.  Outcome: \nWe show how deep learning out performs state-of-the art image prediction techniques. Below is our prediction accuracy chart for each model -\n1.  Medical-Image-Classification-Model\na.  Feature-based SVM Accuracy - 0.66\nb.  Our fine-tuned GoogLeNet DCNN Accuracy - 0.79\n2.  Fashion-Image-Classification-Model\na.  Feature-based SVM Accuracy - 0.72\nb.  Our fine-tuned GoogLeNet DCNN Accuracy 0.93\nAUDIENCE:\n[Advanced Talk], [Machine Learning], [Deep Learning], [Data Science], [Image Classification], [Image Tag prediction], [Healthcare], [Fashion]\n \nWe propose a method to apply a pre-trained deep convolution neural network (DCNN) on images to improve prediction accuracy. We use a pre-trained DCNN on two very different domain specific datasets, and apply fine-tuning to transfer the learned features to the prediction. Our approach improves prediction accuracy on both domain-specific datasets, compared to state-of-the-art approaches. ", "Robot uses toddler-like self exploration for the development of body representations\n \nThe project is part of an attempt to study and improve a robot learning skills using developmental approach inspired by psychology research, which has the potential to allow more flexibility in uncertain environments. We focused on the development of body representations - orientation and movement. This ability has a crucial part in the mammalian sensorimotor and self perception systems. \nIn infants,the process of acquiring body awareness involves active exploration of the sensorimotor space by babbling and self-touch. We tried using similar technics with the Nao humanoid robot. \nUsing an artificial neural network, we believe the robot can develop and maintain a body schema that can also be used for recognizing unexpected situations such as external stimuli or body changes. \nWe used different python tools, both for the activation of the robot and the analysis of the recorded data. Our main tool was Lasagne - a library used for creating feed-forward neural networks and train them using back propagation. \n \nCan artificial childhood improve robot intelligence? For some animals, including humans, childhood experiences are crucial for the healthy development of the brain. We believe that robots can also benefit from having such an exploratory stage. By adopting biologically inspired developmental methods, we tried to train a neural network to develop body representations of a humanoid robot.", "How to become a Data Scientist in 6 months: a hacker\u00e2\u0080\u0099s approach to career planning\n \nShort intro\nWho I am, my background and short summary of my story. Here I will list the steps I personally took to achieve the goal I had.\nHow did I do it?\n\nWhy I chose a \u00e2\u0080\u009chacky\u00e2\u0080\u009d way to enter this career path. First mover advantage, why getting a degree doesn\u00e2\u0080\u0099t always improve your career prospects. Possibly a rant on the signalling function of formal education and how that is rarely aligned with a relevant practical skill set. Some stats to back it up (best career success predictors). Examples of hacking bureaucracies/social hierarchies from my experience and elsewhere.\nList of things not to do and common cognitive pitfalls. \nNetworking for nerds - how to do it right.\nTime management for chronic procrastinators - how to plan a self-guided project. Some notes on psychology of time discounting and need for external reinforcement, with autobiographical examples.\n\nConclusion\nYou don\u00e2\u0080\u0099t need a PhD or even a masters to do machine learning. On taking calculated risks and especially calculated exits from one\u00e2\u0080\u0099s comfort zone. Some notes on soul searching and how to choose a career that is also a passion. Reading list. \n \nThis talk outlines my journey from complete novice to machine learning practitioner. It started in November 2015 when I left my job as a project manager, and by April 2016 I was hired as a Data Scientist by a startup developing bleeding edge deep learning algorithms for medical imagery processing.", "How to be a 10x Data Scientist\n \nKnowing the difference between your logistic and linear regression models or knowing how to train a model using a CNN won't make you a 10x data scientist, but there are other tips and tricks to becoming an even greater commodity to your employer than you already are.  Bringing ideas from the developer community, I'll cover what you can do to increase productivity and level up your career.\nUsing basic principles from the world of software development, this talk will cover ideas on how to become a more productive data scientist. This includes common principles such as not reinventing the wheel by using API's and libraries instead of writing your own code, writing tests to future-proof your code and be your own QA, how to make your models available to team members regardless of what language they use, how to write your code for production, versioning and automation.\nData scientists will take away how they can become a 10x developer and increase their value and write better code by leveraging software developers common best practices.\n \nKnowing the difference between your logistic and linear regression models or knowing how to train a model using a CNN won't make you a 10x data scientist, but there are other tips and tricks to becoming an even greater commodity to your employer than you already are.  Bringing ideas from the developer community, I'll cover what you can do to increase productivity and level up your career.", "Machine learning with ventilator data to improve reporting on critically ill newborn infants\n \nMechanical ventilators are widely used in intensive care. Even two decades ago they were be primarily mechanical devices whose \"only\" task was to inflate the patient\u2019s lung. Recently, however, they have become equipped with powerful computers that provide sophisticated ventilator modes. Data provided by the ventilators are almost never downloaded, stored or analysed. The data is complex, high frequency and requires time-intensive scrutiny to review. Doctors do not have time to analyse these traces in a neonatal unit.\nWe are providing a simple and easy-to-interpret summary of 100Hz dual-channel ventilator data to improve the quality of care of young infants by time-poor staff. This involves signal processing, visualisation, building a gold standard and machine learning to segment breaths and summarise a baby's behaviour. This builds on our talk at PyDataLondon Meetup 30 in January 2017. Our goal is to open source the research so that others can benefit from the processes that we develop. We invite feedback from the audience to help improve our methods.\nAnyone interested in time series data, automated labeling, scikit-learn, Bokeh and medical applications will find this talk of interest. Both the highs and lows of our current approaches will be discussed.\nThis is a collaboration between Dr Gusztav Belteki (Cambridge University Hopsitals NHS Foundation Turst), Ian Ozsvald (ModelInsight) and Giles Weaver (ModelInsight).\n \nMechanical ventilators are widely used in intensive care, they are sophisticated but Doctors do not have time to analyse the copious traces of data in a neonatal unit. We are providing an easy-to-interpret summary of this time-series data using visualisation and machine learning. This is an open source collaboration with the NHS, All results are open.", "Python for visualization of HPC applications\n \nThis work describes the usage of python in application that makes use of high performance application (HPC). \nSuch applications comprise examples as computational fluid dynamics, where the analysis of large data bases \nis necessary in order to extract useful information. On the other hand, a common way used in the analysis of \ndata is the visualization. It can be done by libraries as matplotlib or pandas, however  more sophisticated 3D visualization \n can also be performed though vtk and mpi4py libraries, or even though Blender of 3d Max. The two latter case enable \nthe use of high quality rendering.\n \nThis work describes the usage of python in application that makes use of high performance application (HPC). \nSuch applications comprise examples as computational fluid dynamics, where the analysis of large data bases \nis necessary in order to extract useful information.", "Fairness and transparency in machine learning: Tools and techniques\n \nWhen working with personal data, we need to make sure that our algorithms treat people fairly, are transparent and can be held accountable for their decisions. When using complex techniques like deep learning on very large datasets, it is not easy to prove that our algorithms behave they way we intend them to and e.g. do not discriminate against certain groups of people.\nIn my talk, I will discuss why ensuring transparency and fairness in machine learning is not easy, and how we can use Python tools to investigate our machine learning systems and makre sure they behave they way they should.\n\nIntroduction: Why you should care about this (->EU-Data Protection Directive)\nWhat kind of problems can occur in machine learning systems (bias in the input data, leakage of sensitive information into the training data, hidden usage of protected attributes by the algorithm)?\nHow can we measure and correct for bias in our systems (certifying and removing disparate impact)?\nHow can we understand the decisions that our algorithms make (perturbation analysis, simplified modeling, blackbox testing)?\nHow can we design our machine learning systems to make sure they're compliant and accountable (anonymization of data, monitoring of outcomes, auditing of algorithms)?\nOutlook: The future of transparency and accountability in machine learning\n\n \nThis talk will try to answer a simple question: When building machine learning systems, how can we make sure that they treat people fairly and can be held accountable? While seemingly trivial, this question is not easy to answer, especially when using complex methods like deep learning. I will discuss tools and techniques that we can use to make sure our algorithms behave as they should.", "Eat Your Vegetables - Data Security for Data Scientists\n \nThe ubiquity of data in the modern age has created an environment where data scientists can thrive, but it's also leading to a nasty situation: the very data that makes our lives so interesting is also making us a target for some people who don't have our interests at heart.  Data scientists are quickly becoming the caretakers of their organization's data if for no other reason than that we use it the most! That means data scientists must become the guardians of that data.\n\"But that's someone else's job!\" Even if you are part of an organization that's large enough to have a dedicated security team, you should still care about your data.  It's your data.  Your security team isn't working with it every day.  They aren't relying on it for their next big project.  They haven't spent hours upon hours cleaning and tagging. When it comes down to it, you are the person with the most investment in the data.  Can you really trust some far off team to give it the same attention you do?\nIf I haven't already convinced you that this is an important problem, then this talk will really drive the point home.  We'll discuss recent data breaches from Ashley Madison, the IRS and OPM, LinkedIn, Sony, and even Wendy's. Small companies aren't immune either! Hacking activity against small businesses is on the rise and even a single breach can cost a company several hundred thousand dollars in lost revenue.\nThe world is not all dark and scary. There are several relatively easy things you can do to add a great deal of protection. Oftentimes, default security is so lax that just applying a few simple tactics can vastly reduce your attack potential and make you a far less tempting target. We'll discuss access controls on accounts and data, how to do credential management the right way, and dispel the myths around SSL that put people off.  Security isn't free, but it sure can be cheap.\nAfter you get your accounts locked down, you've still got huge datasets that need protection.  Encryption can go a long way to making sure that your hard processed data doesn't get into the hands of some ne'er-do-well. We'll also briefly discuss backup options, since all the encryption in the world can't protect against deletes\u00e2\u0080\u00a6\nFinally, we'll discuss PyCrypto and Openssl for low-level encryption and explore how these libraries can be combined with provider frameworks like Boto to create some very powerful tooling.  We'll conclude with some of the hosted solutions for data storage that have begun providing out-of-the-box security. Audience members can expect to walk away with a better understanding of the hazards we face as data scientists as well as a sense of the full spectrum of security options, from simple tricks you can integrate into your regular scripts, to complete, turn-key security solutions.\n \nYou've got data: lots of it. People want to get their hands on that data. You don't want that, so let's go over a few things you can do to dissuade attackers from getting their grubby mitts on your hard processed datastore. We'll cover the obvious things (spoiler alert: encryption) and then some advanced techniques for keeping data secure while still keeping it usable (that is to say, analyzable).", "Improving delivery of safe water to African communities with Jupyter notebooks\n \nWater Mission is a non-profit organization dedicated to bringing safe water solutions where needed around the world. The IBM jStart team is helping Water Mission to analyze water consumption data from Water Mission's treatment stations in Uganda and Tanzania. Jupiter notebooks are used to combine and correlate data with socio-economic and weather data, look for behavioral patterns, predict water shortages, and suggest changes to the way the stations are operated in order to increase delivery of safe drinking water within the local communities.\n \nWater Mission is a non-profit organization dedicated to bringing safe water solutions where needed around the world. ", "A Year of Pyxley: My First Open-Source Adventure\n \nAs a Data Scientist on the Merch Algorithms team at Stitch Fix, we empower our business partners to be data-driven through dashboards and reports. In many cases, our partners simply want to look at the data under several predefined lenses. While RStudio\u00e2\u0080\u0099s Shiny provides this capability, the growing needs of our business partners led to several large and unwieldy applications. My preference for data munging in Pandas led to the creation of Pyxley. Pyxley provides a set of simple widgets in the spirit of Shiny, allowing me to focus on the data and not the UI. \nI spent a week figuring out a decent way to have Python write JavaScript, wrote a set of wrappers, called it Pyxley, and released it in July of 2015. The amount of interest was overwhelming, especially as someone new to open-source. In this talk, I will focus on what I think has and hasn\u00e2\u0080\u0099t worked for Pyxley. In addition, I\u00e2\u0080\u0099ll talk about what it\u00e2\u0080\u0099s like to maintain a project for the first time. \n \nLast year, I created my first Flask-powered web application utilizing React.js and D3. I refactored it into reusable components and released it as Pyxley. In this talk, I\u00e2\u0080\u0099ll introduce the basics of Pyxley and discuss what has and hasn\u00e2\u0080\u0099t worked. More importantly, I\u00e2\u0080\u0099ll talk about what it\u00e2\u0080\u0099s like to maintain an open-source project for the first time.", "A full Machine learning pipeline in Scikit-learn vs in scala-Spark: pros and cons\n \nThe machine learning libraries in Apache Spark are an impressive piece of software engineering, and are maturing rapidly. What advantages does Spark.ml offer over scikit-learn?\nAt Data Science Retreat we've taken a real-world dataset and worked through the stages of building a predictive model -- exploration, data cleaning, feature engineering, and model fitting -- in several different frameworks. We'll show what it's like to work with native Spark.ml, and compare it to scikit-learn along several dimensions: ease of use, productivity, feature set, and performance. \nIn some ways Spark.ml is still rather immature, but it also conveys new superpowers to those who know how to use it.\n \nThe machine learning libraries in Apache Spark are an impressive piece of software engineering, and are maturing rapidly. What advantages does Spark.ml offer over scikit-learn? At Data Science Retreat we've taken a real-world dataset and worked through the stages of building a predictive model -- exploration, data cleaning, feature engineering, and model fitting; which would you use in production?", "The Duct Tape of Heroes: Bayesian statistics.\n \nMy talk is made up of the following examples; \n\nbasic disease example: what is the value of adding an extra test to a patient\ngive an example of an inference task that is very hard to do properly without bayesian thinking\ncreating simple probibalistic models with pandas and showing how they are robust against missing data \ndemo the daft, corner and pomegrenate library\nshow how you can use bayes rule to pick models \ndemo a bayesian probablistic approach to finding overpowered characters in the Heroes of the Storm video game.\n\n \nIn this talk I will give many examples of when Bayes rule will help you in your day to day work. I'll quickly show many examples of bayesian statistical thinking in action; the pleasure of inference, probabilistic graphs, model selection, feature generation, even operations research! I'll finish with a dataset from Heroes of the Storm and I'll show why Bayesian models can outperform randomforests.", "Code First, Math Later: Learning Neural Nets Through Implementation and Examples\n \nProgramming frameworks for implementing neural networks have become easy to use, and now allow for rapid prototyping and experimentation. These frameworks can also be used as teaching tools for those who are getting started in neural networks and deep learning. However, often students and practitioners start from textbooks and research papers in order to learn about these powerful techniques, and get bogged down in mathematical notation and jargon. This talk proposes a different approach through three high-level steps: 1. learning the basics of neural network architectures and applications, 2. experimenting with these models through code examples, and 3. revisiting the math and theory behind these models with a more practical understanding of how they work. We will focus mainly on architectures for three popular types of neural networks (feed-forward, convolutional, and recurrent), setting aside the issue of optimizing these networks during training. \nThis talk assumes some familiarity with supervised machine learning and classification, but assumes no prior knowledge of neural networks or deep learning. A familiarity with Python is beneficial, since this talk presents neural nets primarily from the perspective of programming using a high-level library. However, if you are familiar with another programming language or deep learning library, the concepts will likely make sense. A detailed talk outline is given below:\n1. Overview of Popular Network Architectures and Applications\na. Feed-forward networks\nb. Convolutional networks\nc. Recurrent networks\n\n2. Brief Overview of Keras API\na. Layers, stacking, and data transformation\nb. Linking layers with activation functions\n\n3. Implementing Neural Nets\na. A convolutional network for object recognition\nb. A LSTM network for document classification\n\n4. Beyond the Black Box -Investigating Feature Extraction Layers\na. General framework for thinking of network layers: input, feature extraction, and classification layers\nb. Intermediary layers as \"data transformers\"\nc. Implementing \"partial networks\" (e.g. networks with feature extractors, but no classification layers) to\n    better understand data transformations and for debugging\n\n5. Conclusion and Questions\n \nThis talk will cover learning about neural networks through programming and experimentation. In particular, we will use the Keras library as a straightforward way to quickly implement popular neural network architectures such as feed-forward, convolutional, and recurrent networks. We'll also focus on internal data transformations within these networks, as information is passed from layer to layer.", "Julia: A Fresh Approach to Machine Learning\n \nExisting machine learning frameworks are complex \"black boxes\" which are designed to work at a certain level of abstraction. If you need higher-level (e.g. complex models) or lower-level (e.g. custom gradients or GPU kernels) control than the framework provides, you get stuck.\nJulia's mix of performance and ease of use opens a radical alternative; a single language that can work at all levels, from GPUs to data processing pipelines to clusters. Different approaches can be freely mixed and matched, with transparency and control at all levels of the stack. Come see how these ideas can accelerate your current workflow, and get a glimpse of the future of ML in Julia.\n \nJulia's well-known combination of ease-of-use, performance and powerful features make it uniquely suited to the toughest machine learning problems. We'll illustrate how Julia can accelerate your current workflow, show you the groups running intelligent Julia code in production, and discuss our plans for the future.", "Using network community clustering algorithms to aid determination of protein structures\n \nThe study of proteins is extremely relevant to many fields, ranging from medicine to industrial processes. If a cell was a factory, proteins would be the workers, performing very specific tasks in a production chain, in which a failure at any of the steps might affect the others. In order to really understand the function of proteins, there are a number of biochemical techniques that we can apply, but to get a really accurate description of their mechanisms we need to have information up to their atomic level. Proteins are composed by different combinations of 20 amino acids which form chains and result in an intricate and functional structure. We know the final shape of the protein is dependent from its sequence but we can't (currently) predict the outcome of a given sequence, so we need experimental techniques to obtain this information. We can't directly look a protein with a microscope and see its three-dimensional atomic arrangement. There are biophysical techniques that allow us to get low-resolution shapes or surfaces of them, but the only current method that provides atomic detail is x-ray diffraction. In contrast to microscopy, were after illuminating your sample you can, with the appropriate lenses, transform back your reflected rays in an image, in x-ray diffraction we only get the Fourier transform of our protein's structure, that is, its diffraction pattern, but we lose the phase information from the diffracted rays. This problem, known as the phase problem, is an issue in structure solution, because, compared to the intensities of the diffraction pattern, phases provide much more information about our protein's structure. Phases can be computed from a coordinates model, so, if there is a structure of a protein that is expected to be similar (in terms of their coordinates) to an unknown one, it can be used to provide initial estimates of the phases and then improve them until the problem structure is solved.\nProteins do not adopt random shapes. There are forces that act on them and, according to the chemical properties of the amino acids and the context they have around, they will arrange in particular forms. The first level of structure formed is what we call secondary structure, which is formed of alpha helices and beta strands. These are very general and are present in all kind of proteins. As letters in different books, which are general if looked at independently but can acquire a quite different meaning in 'El Quijote' than in '50 Shades of Grey', these structure fragments alone have a different meaning than when set in a context with other structure elements. Yet, even combinations of a few elements can still be general and frequent, and have a sense, such as the phrase 'Once upon a time' in so many tales. In the protein case,  particular small combinations of alpha helices and beta strands should also appear more frequently and can be studied on their different contexts. In a way, we could say that, protein structures, from a top down view, look quite different between them, but from a bottom up one, when we go to small pieces, they look much more similar.\nIn our group, we try to exploit these properties of protein structures for solving two types of problems. One is a search problem, consisting in understanding which is the best way in which to break down a larger model of a protein similar to an unknown one in order to refine it and get the correct phases and solve the structure. The other one is more of a prediction problem, in which we want to find structural units in the database of solved structures in order to help us interpret them and extract new information from this vast amount of data. In order to solve both problems we need a numerical description of our system than can help us describing its geometrical features in an accurate way. We have developed a description of such secondary structure fragments based on what we call characteristic vectors, that we also employ to represent the relations between such elements. This description can be implemented in a network graph that allows to use community clustering algorithms on it in order to evaluate simultaneously all the relations between the elements of the structure and find its communities.\nFrom the technical point of view, the language we use for our development is Python, and the tool in which our graphs are implemented is python-igraph. There are a number of community clustering algorithms available and deciding which is the best for our case and which metrics to use to describe our structures has been an interesting process that we want to share with you. In this talk I will explain our research and development on this topic and I will link the general descriptions to our current and successful use case, possibly leaving open questions on what can we do more!\n \nHow can you best exploit the information on the protein structures that we already know in order to solve the (many more) that we don\u2019t know? That\u2019s the question we try to solve in our group and this talk will show our use of community clustering on the use case of aiding protein structure solution.", "TNaaS - Tech Names as a Service\n \nTNaaS - Tech Names as a Service\nDeploying a Random Generator  that's Phonetically Pokemon\nThere is a striking phonetic similarity between big data technology and pokemon names. This has caused some hilarious conversations with recruiters on linkedin: https://www.linkedin.com/in/vincentwarmerdam/ but it also made me wonder. Can I create a service that generates strings that sound like potential pokemon names and what might be the simplest possible way to make that into a service? Also, would it be possible to generate pokemon names that start with three random characters and end with 'base' (KREBASE, MONBASE would be appropriate but IEYBASE would not be). \nTurns out that this is an interesting problem from a ML standpoint and that it is rediculously easy to build in the cloud. In my talk I will explain the ML behind it;\n\nmarkov chains \nprobibalistic graphs \nrnn/lstm \nbidirectional lstm \n\nDuring the talk I will also do a deep dive on the pros and cons of these methods.  \nI currently have a 101 version of the service live here: \nhttps://dyccxmwpz9.execute-api.eu-west-1.amazonaws.com/pokemon/poke-names/v2\nBy the time of the conference it will have a proper front end and I have already bought the domain name of tnaas.com. \nI've talked about a similar topic in the past: https://youtu.be/TkHT3sLwtkY?t=22m10s. The goal of this talk is to spend more time explaining how I actually built the service; both from a coding perspective as well as a deployment perspective.\nLet me know if there are any questions. I am submitting multiple talks that I think are interesting and relevant to the PyData crowd, I'll gladly leave it to the committee which (or if any) of them are relevant to the local community.\n \nIn this talk I will explain how I built a service that generates Pokemon names. You'd be surprised how hard it is to do this properly and how easy it is to do it practically.", "Agent-based modeling in Python\n \nAgent-based modeling was an unfilled niche in Python\u00e2\u0080\u0099s robust and growing scientific computing ecosystem, until Mesa was created. Mesa allows users to quickly create agent-based models using built-in core components (such as agent schedulers and networks) or customized implementations; visualize them using an innovative browser-based interface; and analyze their results using Python\u00e2\u0080\u0099s extensive data analysis tools. Mesa is being developed by a group of modeling practitioners with experience in academia, government, and the private sector. It is also completely open-source, encouraging users to contribute to what we hope will be a growing repository of model components which others can reuse and expand upon in future research.\nIn this talk, you will learn the following... \n What are agent-based models & why should I care? (An overview of various agent-based models, the existing software ecosystem, and practical applications)\n What is Mesa? (An overview of Mesa\u00e2\u0080\u0099s features / components and architecture.)\n* How do I a build a model using Mesa? (A walkthrough of how to write a model.)\n \nAgent-based modeling is a technique used to explore both complexity and emergence by simulating individual actors and their actions within a system. Think of systems such as the traffic in a city, or like those in financial markets where one actor can have an effect on the decisions of others until the system\u00e2\u0080\u0099s direction changes its course. In this talk, you will learn about ABMs in Python. ", "Stemgraphic: A Stem-and-Leaf Plot for the Age of Big Data\n \nIntro\nThe stem-and-leaf plot is one of the most powerful tools not found in a data scientist or statistician\u00e2\u0080\u0099s toolbox. If we go back in time thirty some years we find the exact opposite. What happened to the stem-and-leaf plot? Finding the answer led me to design and implement an improved graphical version of the stem-and-leaf plot, as a python package. As a companion to the talk, a printed research paper will be provided to the audience.\nBackground\nWhat were the design challenges? What about other options for visualizing distributions? These questions will be answered in this part of the talk.\nDemonstration\nFrom the standalone command line tool, to the python package, there are many ways to take advantage of stemgraphic. This section of the talk will be all about live demos, at the command line and in a Jupyter notebook.\nDesign and Implementation\nIn this section we will talk about specific design decisions (such as fonts, colors, default options, number of data points, scaling) and about a few implementation details.\nPerformance\nProbably the most important question when it comes to usability of a tool designed for exploratory data analysis, data mining etc. How fast is it? This section will include some more demonstrations.\nFuture work\nStem_graphic is an open source python module that implements a fast and easy to use, highly configurable graphical stem-and-leaf plot. The best feature? Without any data wrangling or having to think about options, it provides a visually appealing, usable plot. But there is always room for improvement. We'll wrap up the talk mentioning some of the enhancements on the horizon.\nQuestions?\n \nJohn Tukey\u00e2\u0080\u0099s stem-and-leaf plot first appeared in 1970. Although very useful back then, it cannot handle more than 300 data points and is completely text-based. Stemgraphic is a very easy to use python package providing a solution to these limitations. In this talk you will learn to use stemgraphic, see it in action with very large datasets and understand what drove the design decisions.", "Keynote: Builtin Superheroes\n \nComing Soon\n \nComing Soon", "Keynote: Using the Python Data Science Stack to Determine How Many People Have Been Killed in Syria\n \nComing Soon\n \nThe Human Rights Data Analysis Group (HRDAG) uses methods from statistics and computer science to quantify mass violence.  As part of that work, we rely on open source tools, including python and R, for data processing, management, analysis, and visualization.  This talk will highlight how we use those tools to estimate how many people have been killed in the ongoing conflict in Syria.", "The IceCube data pipeline from the South Pole to publication\n \nThe IceCube Neutrino Observatory uses a grid of more\nthan 5000 light sensors embedded in a cubic kilometer of glacial ice at the\ngeographic South Pole to detect neutrinos, some of the most elusive particles\nin the universe. These neutrinos are interesting because the most energetic of\nthem likely hold the key to one of the longest-standing problems in\nastrophysics, the origin of ultra-high-energy cosmic rays.\nCosmic rays were discovered over 100 years ago, and in the mean time we know\nthat they are electrically charged particles (mostly protons) from outer space.\nThe most energetic of them pack the kinetic energy of a rifle bullet into a\nsingle atomic nucleus, well beyond the reach of any particle accelerator that\nwe could ever build on Earth. What we don't know, however, is where or how they\nare accellerated to such energies, because their directions are scrambled by\nintervening magnetic fields. This is where neutrinos enter the picture. They\nare produced in large numbers whenever particles like protons are smashed into\neach other at high energies, something that is bound to happen in the cosmic\naccelerators that produce the cosmic rays we observe at Earth. Neutrinos are,\nhowever, electrically neutral and only interact via the weak force, which makes\nthem extremely difficult to stop. That makes high-energy neutrinos a very\ninteresting way to do astronomy: unlike photons, they can't be absorbed in dust\nclouds and radiation fields, and unlike cosmic rays, they can't be deflected by\nmagnetic fields.\nThe elusiveness of neutrinos is also what makes IceCube data analysis\ninteresting and challenging. The first challenge on the way from raw data to\nscientific result is data preparation: one needs to actually find neutrino\nevents in the data. For every neutrino event, there are millions of events\ncaused by cosmic-ray air showers that penetrate the 1.5 km of ice over the\ndetector. We separate the neutrinos from uninteresting background events using\nthe features of each event. The optimal set of features and the separation\nstrategy depend on the kind of measurement one wants to do, and there are many\ndifferent directions of interest within the 300-strong IceCube Collaboration.\nThis means that the event processing chain and data format need to be extremely\nflexible, allowing physicists with limited software skills to add new features\nto the data. Our data processing framework, called IceTray, represents each\nevent as a frame, essentially a dictionary of string-key pairs. Each frame is\npassed through a series of modules that can add new features of arbitrary\ntype based on the data already in the frame, or drop frames from further\nprocessing. While the core of IceTray and many of its modules are written in\nC++, all the configuration is done in Python, and modules themselves can be\nwritten in Python. This allows for quite rapid and accessible development. The\nserialized form of these frames, called an \"I3\" file, also serves as our\nlong-term archive format.\nOnce a data sample has been prepared, it's time for visualization and analysis.\nHere we move out of our custom world and start using standard tools and data\nformats. We support two analysis universes: one based on\nHDF5, which is convenient to read with\nPyTables, and another based on\nROOT, an analysis framework and associated data format\nspecific to high-energy physics. We write to such \"tabular\" formats using an\ninternal framework called tableio that hammers each object in a frame into a\nseries of table rows. Backends for HDF5 and ROOT then emit these rows in a\nspecific format, along with a description and unit string for each column.\nI will illustrate a typical Python-based analysis using the discovery of the\nfirst high-energy astrophysical\nneutrinos as an\nexample.\n \nTransforming data produced at the IceCube Neutrino Observatory into scientific\nresults presents several interesting challenges. My talk will give an overview\nof our software stack, written in a colorful mix of Python, C++, Java, and\nFORTRAN, present some of our custom solutions, and highlight areas where we use\nstandard libraries and design patterns.", "Lessons from 6 months of using Luigi in production\n \nIn the beginning, there was Cron. We had one job, it ran at 1AM, and it was good. Then we added another job, and to make them run one after the other, we used Luigi, which says \"This can only run when this is finished\". Then we added another ~500 jobs, long running scikitlearn computes, external API dependencies, a business reporting systems with 2000+ reports and 400+ users and a scheduling system with 5000+ users. This is when things got interesting.\nThis is the story of building the data systems at Deliveroo.  This is not a talk about Big Data, cutting edge algorithms or new open source technology. Rather, this is a talk about coping with complexity in a rapidly changing landscape. I'll start from the beginning, giving a brief overview of what Luigi is and why we decided to roll with it. The body of the talk will be about the challenges we faced as our company grew in size and complexity, the solutions that worked (and those that didn't), and what we know now that we didn't know then. I'll cover a bit of the luigi syntax itself, but mostly I'll focus on the things we did around luigi that made it work for us; how (not) to design pipelines, how to test them, how to manage issues gracefully and how to detect problems in advance. \nBy attending this session you'll learn:\n\nWhy DAG based ETL systems are fundamentally useful\nWhat to think about when designing your DAG\nWhat to implement early to save you pain later on\n\n \nAt Deliveroo we've built our data plumbing from the ground up using Luigi to manage our data workflows. In this talk I'll be walking through our experiences using Luigi scaling from a few simple jobs to a complex, production grade system. This talk is mostly about building robust data pipelines, but is also a little bit about why it's better to be woken up by your cat than by the server alarm.", "Python and IoT: From Chips and Bits to Data Science\n \nEver want to know what is behind the \"Internet of Things\" hype? I wanted to as well, so I embarked on a side project to learn more. This talk is the story of my journey, using, of course, my favorite programming language, Python.\nIn this talk, I will take you through my project, a lighting replay system. The application monitors the light levels in several rooms of a residence and then replays a similar pattern when the house is unoccupied. The goal is to make the house look occupied, with a lighting pattern that is different every day, but looks realistic. It accounts for the different patterns found in each individual room as well as seasonal factors (e.g. changing sunrise/sunset times). The full source code for the application is available on github here.\nAudience\nA basic knowledge of Python is assumed, and exposure to the PyData ecosystem helpful, but no special knowledge about hardware or data science is needed to follow this talk.\nOutline\nThe application was built in three phases: 1) data capture, 2) data analysis, and 3) the lighting player.\nData Capture\nLight sensor data is gathered via ESP8266-based microcontrollers. These chips have only 96 kilobytes of data RAM, but have built-in WiFi and can run MicroPython, a lightweight implementation of Python 3 that works without an operating system. Data is sent to a Raspberry Pi via the MQTT messaging protocol. There, it is saved into CSV files for offline processing.\nData Analysis\nThe light sensor data is next analyzed. The CSV data files are parsed, post-processed, and read into Pandas Series data structures. The raw and processed data can be visualized in a Jupyter notebook.\nThe light readings are then grouped into four levels via K-Means clustering. These four levels are mapped to on-off values, depending on the particulars of each room (e.g. how much ambient light is present). We divide each day into four \"zones\", based on the absolute time of day and sunrise/sunset times. The samples are grouped into subsequences separated by zone and by gaps in the data readings.\nThese subsequences are then used to train Hidden Markov Models using Hmmlearn. Hmmlearn can infer a state machine that will emit a similar pattern of on/off samples. A total of four models are created per room, with one for each zone.\nLighting Player\nThe light controller application runs off the Hidden Markov Models created in the analysis phase. It controls Philips Hue smart lights, which are accessible via a REST api. We use the phue library to abstract the details of the control protocol. Since both the light sensors and the lights are wireless, this application can be easily deployed without replacing light switches or stringing wires through the house.\n \nThis talk will take you through the design of a smart lighting system, including sensor hardware and software (based around MicroPython), data analysis (using NumPy, Pandas, and Jupyter), and lighting control (using Hidden Markov Models via Hmmlearn). From the talk, you should get a sense of how the hardware, software, and math fit together to create a solution.", "A laser light show on a quantum scale\n \nLaser light can be used to trap, cool and manipulate atoms and microscopic particles, with applications ranging from simulating quantum systems to real-time manipulation of single atoms or biomolecules. The development of new laser beam shaping methods is therefore imperative to the progress of research in a variety of fields within optics, atomic physics and biophotonics. \nSpatial light modulators offer a highly versatile method of time-dependent beam shaping, based on imprinting a phase profile onto an incident laser beam that determines the intensity and phase distribution of the trapping plane laser field. Any desired trapping plane field must therefore be expressed in terms of the corresponding input plane phase, a phase retrieval problem that can be solved by methods falling broadly into two categories: iterative Fourier transform algorithms, and those based on the minimisation of a cost function. I will present a versatile approach that falls into the latter category, describing how we use a conjugate gradient minimisation routine implemented in Python to directly target specific output plane features of interest. By using the Theano library to determine the cost function gradient, we have drastically streamlined the process of assessing cost function suitability, making possible our recent implementation of the simultaneous and independent control of both the intensity and phase of the trapping plane optical field. This latest development grants an extra dimension to our control of light field structure, and thus our ability to manipulate the properties of trapped particles.\n \nPrecise shaping of laser light is crucial to trapping and manipulating ultracold atoms, with critical implications for quantum computing, quantum simulation and precision measurements. I will discuss a Python-based approach to the beam shaping problem that maximises our control of trapped particles.", "Marketing Data Science\n \nDigital Marketing is changing the way corporations and brands communicates with customers. The investments in Digital Marketing are skyrocketing around the world in a multi-billion industry.\nBut consumers and customers (specially millenials) does not trust advertising. Different approaches are being made by corporations like Inbound Marketing Strategies. My presentation is about how Digital Marketing needs Data Science in order to better understand the customer needs and generate new niches of interest for companies. Companies investing in Digital Marketing should take a close look at Data Science Platforms like Python in order to better gather inisghts, create segments and personalice a customer experience.\nI will provide some short examples about how we are using python jupyter notebook environment in order to gain inisghts from customers using IBM Watson API, generating new segmentation and customer experiences.\n \nMarketing Data Science: how digital marketing needs data science to survive.", "Semi-Supervised Bootstrapping of Relationship Extractors with Distributional Semantics\n \nRelationship Extraction (RE) transforms unstructured text into relational triples, each representing a relationship between two named-entities. This relationships can then be used to populate knowledge bases, or build knowledge graphs, which can support several tasks, such as Question Answering.\nA bootstrapping system for RE starts with a collection of documents and a few seed instances.  The system scans the document collection, collecting occurrence contexts for the seed instances. Then, based on these contexts, the system generates extraction patterns. The documents are scanned again using the patterns to match new relationship instances. These newly extracted instances are then added to the seed set, and the process is repeated until a certain stop criteria is met.\nBootstrapping approaches relying on TF-IDF weighted vectors have limitations when trying to find similar instances, since the similarity between any two relationship instance vectors is only positive when the instances share at least one term. For instance, the phrases was \"founded by\" and is the \"co-founder of\" do not have any common words, but they have the same semantics.  Stemming techniques can aid in these cases, but only for variations of the same root word. By relying on word embeddings, the similarity of two phrases can be captured even if no common words exist. For instance, the word embeddings for \"co-founder\", \"founded\" and \"creator\" should be similar, since these words tend to occur in the same contexts.\nI propose to present a system which extracts relationship instances by bootstrapping and by relying on word embeddings. It was evaluated against a popular system which relies on TF-IDF weighted vectors, the paper describing the system was presented at EMNLP'15 and won an honorable mention for best short-paper award.\n \nSemi-supervised bootstrapping techniques for relationship extraction from text iteratively expand a set of initial seed relationships while limiting the semantic drift. This talk presents an approach to bootstrap relationship instances using word embeddings to find similar relationships. Results show that relying on word embeddings achieves a better performance than using TF-IDF weighted vectors.", "NoSQL doesn't mean No Schema\n \n\n\nWhat a schema is and why we need it? How does the SQL database three-schema concept apply to MongoDB? Consequences of database schema not matching the application programming schema. Especially in a more flexible NoSQL context.\n\n\nWays to represent a schema: Code v. Metadata. There are really five cases here. On one hand there's pure code, including completely ad-hoc in the code, via Python class definition. One the other hand there are more formal metadata: packaged with the code, in separate files, or in the database.\n\n\nCommon Use Cases for apps that use a schema definition. We'll look at Python code for a persistent object that serializes itself to MongoDB. This relies on a formal schema definition using the JSON-Schema structure. This includes a simple Python Metaclass to add features to Python's base class definition\n\n\nSchema version management and Schema migration. Some techniques that can help with schema and data migration. \n\n\n \nWhile databases like MongoDB don't require a formal schema, there's still a schema somewhere. It might be merely implied by validation rules in the code. Or, there might be a more formal representation. In some cases, the lack of strict schema creates a dynamic flexibility that creates value rapidly. Other times, the lack of formal structures leads to chaos. How can we find a balance?", "Scikit-build: A build system generator for CPython C extensions.\n \nAlthough the scientific packages NumPy, SciPy, SymEngine or Matplotlib can be used from Python today, since they bundle C, C++ or Fortan extensions, successfully building the associated binary wheels is complex and prone to errors. This is a significant barrier to allow use of these scientific packages on a broader set of platforms like mobile, Raspberry Pi or HPC.\nTo facilitate the build process, the scikit-build package is fundamentally just glue between the setuptools Python module and CMake. Currently, the package is available to perform builds in a setup.py file. In the future, the project aims to be a build tool option in the currently developing pyproject.toml build system specification.\n \nIn this talk we present \"scikit-build\", an improved build system generator for CPython C extensions. It provides better support for additional compilers, build systems, cross compilation, and locating dependencies and determining their build requirements. We also describe how Matplotlib and SymEngine are updated to make use of \"scikit-build\"", "When Worlds Collide: Productionalizing a Data Science Model\n \nData engineers and data scientists operate under different constraints. Engineers want stable, testable, high code-quality codebases. Data Scientists want flexibility and sandbox environments to easily test their hypotheses. These differences can exacerbate the already difficult inter-team dynamics involved in building a joint product. While working for the first time as a cross-functional team to build a data application, we had to deal with the following issues:\n\n\nHow much documentation should the data scientists provide the engineers about the model\n\n\nReconciling differing interpretations of business logic between teams\n\n\nReimplementing code from IPython notebooks into an abstracted, object-oriented application\n\n\nPredicting future usage of the application given only the current use case \n\n\nHow to allow the data team to easily experiment and directly reimplement functionality without necessarily going through the full engineering code review process \n\n\nThis talk will discuss how those problems arose and how the data science and engineering teams solved them while creating a new application. \n \nOn our first data science project at Shiftgig, the data science and engineering teams had to build software that was production-ready while maintaining the flexibility of a data science sandbox. Although these seem like irreconcilable goals, they forced us to improve inter-team communication and ultimately helped create a great product. We\u00e2\u0080\u0099ll walk through our process and the lessons we learned. ", "Anaconda Ecosystem for Open Data Science\n \nFull abstract to come.\n \nThe Anaconda Distribution has been pivotal for the adoption of Open Data Science since its creation in 2012.  Today Anaconda has grown far beyond just a cross-platform scientific Python distribution into a rich ecosystem of users, services, software, and libraries.  This talk will demonstrate some of the exciting features of the Anaconda ecosystem.", "Usable A/B testing \u00e2\u0080\u0093 A Bayesian approach\n \nWhen it comes to developing a product, most people focus on \u00e2\u0080\u009cwhy\u00e2\u0080\u009d a feature should be implemented. I would like to look at the \u00e2\u0080\u009chow\u00e2\u0080\u009d A/B testing can support such efforts in order to improve your users experience on your platform. Many companies lose a good amount of conversion and therefore money because they struggle to understand the A/B testing tools that they are using and how to interpret the test results.\nWe will take a look at why A/B testing experiment results using conventional approaches are often hard to interpret correctly and will explore an alternate, visually easy to understand approach that was implemented using Bayesian statistics. We will touch on the implementation, as well as the theory behind this approach.\nFinally I will present a new library for A/B testing experiment evaluation that will help you to effectively increase your conversion rates and help your product managers to make the right decisions.\n \nWhen it comes to developing a product, most people focus on \u00e2\u0080\u009cwhy\u00e2\u0080\u009d a feature should be implemented. I would like to look at the \u00e2\u0080\u009chow\u00e2\u0080\u009d A/B testing can support such efforts in order to improve your users experience on your platform. We will then touch on the implementation of an experiment evaluation approach using Bayesian statistics that will help your product managers to make the right decisions.", "Challenges of analysing the wheat genome\n \nTBC, but along the lines of:  Wheat has 5 times more DNA than a human, hugely repetitive regions, is a mixture of three plants and can differ significantly between varieties. It also supplies 20% of the calories consumed by humanity, and 35% of us depend on it for survival, so there is a strong motivation to understand it. The fact that wheat is not only not a 'model organism' but has some features which means assumptions generally made by the writers of bioinformatics software don't hold makes it hard to work with. In fact, often tools which document themselves as being suitable for use with data in the formats wheat researchers use fall over in this use case. Ideally, we would have time to rewrite them exactly for our use case, but this doesn't always happen. \nThis may either be a discussion or a talk. So far I've moved hardware twice, abandoned tools, rewritten parts of tools for my use case (I have a simple python example of this), my current task is to understand in detail why a particular piece of software is segfaulting so I will have a lot more insight very shortly.\nI will aim to keep the talk more about how to deal with data which differs significantly from the standards in a given field than the particulars of wheat, but there are some neat bioinformatics algorithms so I will explain one to demonstrate why we need to store so much in RAM. \n \nTBC, but along the lines of:  What do you do when your data, whilst formally meeting common requirements, is, to put it mildly, an edge case? And what if you don't have the time to write a bespoke tool for your analysis? I discuss this scenario, and will explain why my data is such difficult to start with. ", "Applying the four-step \"Embed, Encode, Attend, Predict\" framework to predict document similarity\n \nA new framework for building Natural Language Processing (NLP) models in the Deep Learning era has been proposed by Matthew Honnibal (creator of the SpaCy NLP toolkit). It is composed of the following four steps - Embed, Encode, Attend and Predict. Embed converts incoming text into dense word vectors that encode its meaning as well as its context; Encode adapts the vector to the target task; Attend forces the network to focus on the most important parts of the data; and Predict produces the network's output representation. Word Embeddings have revolutionized many NLP tasks, and today it is the most effective way of representing text as vectors. Combined with the other three steps, this framework provides a principled way to make predictions starting from unstructured text data. This presentation will demonstrate the use of this four step framework to build Deep Neural Networks that do document classification and predict similarity between sentence and document pairs, using the Keras Deep Learning Library for Python.\n \nThis presentation will demonstrate Matthew Honnibal's four-step \"Embed, Encode, Attend, Predict\" framework to build Deep Neural Networks to do document classification and predict similarity between document and sentence pairs using the Keras Deep Learning Library.", "Journeys through JuPyteR\n \nIn the last few years notebooks have become de rigeur within data science, and the ability to mix & match different languages is one of many exciting elements within them. Using as a motto \u2018find the best tool for the job\u2019 we will take the Python code from Tariq Rashid\u2019s book \u201cMake your own Neural Network\u201d and, using the MNIST data set, rewrite it in R and Julia. Of particular interest will be the idiosyncrasies that we see within each language as well as the readability and speed of each.\n \nThere have been many talks about Jupyter notebooks but rather than concentrating on the notebook this talk will look at three data science languages available within it: Jula, Python and R. This talk will be aimed at beginner coders where we will write code in all three languages and discuss the differences between them.", "Data science for lazy people... genetics will work for you!\n \nWe love Data Science, but sometimes we have to do some manual and repetitive work before starting with the interesting and fun parts, but that will change.\nTPOT is an open source tool built on top of scikit-learn for creating and optimizing machine learning pipelines. It can be considered a data science assistant. \nThe library will automate from feature selection to parameter optimization, it is also able to do preprocessing or construct new features from existing ones.\nTPOT tests a huge number of pipelines to provide you with the optimal one, this work is done with genetic algorithms.\nIt is easy to use, has a familiar syntax if you have used Pandas or scikit-learn, and it's very powerful.\nLet genetics work for you!\n \nData science is fun... right? Data cleaning, feature selection, feature preprocessing, feature construction, model selection, parameter optimization, model validation... oh wait... are you sure? What about automating 80% of the work using genetic algorithms that can make better choices than you? TPOT is a tool that automatically creates and optimizes machine learning pipelines.", "Biases are bugs: algorithm fairness and machine learning ethics\n \nAlgorithms can make decisions, and these decisions can have an impact on people's lives. By feeding data into these algorithms, they can reproduce or amplify our societal biases and take unfair decisions.\nBiases are bugs. They need to be found, fixed, and learnt from. A mix of good ethics and good engineering practices can get us a long way towards that goal.\nIn this talk, you will learn what biases are, see examples of algorithms gone wrong, and explore some software tools you can use and engineering practices you can adopt in your own work to make your algorithms more fair.\n \nBiases are bugs.  They need to be found, fixed, and learnt from. A mix of good ethics and good engineering practices can get us a long way towards that goal.\nIn this talk you'll learn what biases are, what software tools can help, and how to adopt engineering practices that can make your algorithms fairer.", "Creating a Contemporary Lending Risk Management System Using Python\n \nEngineering a sophisticated in-house risk management solution for a commercial lending platform doesn't necessarily need to involve millions of lines of low-level code, clunky desktop applications, fancy front-end development, or messy spreadsheets. That is, so long as the problem is approached objectively and the solutions are evaluated critically.\nThis talk will focus on the basics of lending risk mitigation (related to underwriting and portfolio management), a high-level overview of the architectural requirements, and the packages that were leveraged along the way; namely: \n\npandas for data preparation\nOrganizing panels and cleaning time series data\n\n\nstatsmodels and scikit-learn for regression and classification\nPredicting accounts receivable discontinuation and bankruptcy probabilities\n\n\nR/rpy2 for integrating R's advanced forecasting capabilities\nForecast comparison using the forecast package and the bsts package\n\n\nSpyre for designing a lightweight and easily maintainable web application\n\nWe will share a demo of the web application to see how it all comes together, but to be clear, this IS NOT a product pitch or any kind or SaaS; it was built to be used internally and generalizes well for other use cases.\n \nLending involves risk and in order to be a successful lender at scale that risk needs to be mitigated. We'll be discussing how C2FO has built a suite of risk management tools for underwriting and portfolio management using the PyData ecosystem, rpy2 (for integrating R), and Spyre (for building a simple web application).", "Sharing Your Side Projects Online (and Making Your Github the Best R\u00c3\u00a9sum\u00c3\u00a9 It Can Be)\t\n \nPython makes it easy to create small programs to handle all kinds of tasks, and tools like Github make it easy and free to share code with the world. However, simply adding a *.py to a Github repository (or worse: a zip file on your personal website) doesn't mean other Python programmers will be able to run and use your code.\nFor years, I've written one-off scripts and small programs to automate personal tasks and satisfy my curiosity. Until recently, I was never comfortable sharing this code online. In this talk, I will share good practices I've learned and developed for sharing my small projects online.\nThe talk will include tips on writing reusable scripts, the basics of Git and Github, the importance of READMEs and software licenses, and creation of reproducible Python environments with Conda.\nBesides making your code more usable and accessible to others, the tips in this talk will help you make your Github profile a valuable component of your online r\u00c3\u00a9sum\u00c3\u00a9 and open the door for others to improve your programs through Github pull requests. \n \nPython's design makes it easy to create small programs to handle all kinds of tasks. Tools like Github make it easy (and free!) to share code with the world. However, code that solves a problem on your local machine may not directly translate to solving the same problem for someone else. This talk will provide basic practices and guidelines for making your code usable and accessible to others. ", "AutoDocish: Automated-ish Dataset Documentation\n \nThe creation of dataset documentation is often left to an authoring researcher.  Documentation is vital for the future reusability of a dataset, and yet this time consuming process yields very little immediate impact.  Arguments can be made that well documented datasets are more reusable and can generate better citation metrics in the long term, but it still takes cognitive and workload energy away from the core short-term research mission.  Deconstructing the documentation process tells us that much of the content can be derived from data profiling tools, which can be automated.  This means that the remaining work is simply explaining the context of the data and the meaning of certain values.  AutoDocish is a prototype tool based in Python to assist anyone needing to create data documentation with a base template that includes basic profiles over each column of data and places for authors to provide meanings and explanations for each.  AutoDocish is a command line Python tool written with a framework for expansion, with areas researchers can easily add their own custom functions for specific types of data profiles and calculations.  The output is in basic markdown, meaning that it can easily be added to a website or included as a readable plain text file within a dataset deposit. \n \nAutoDocish is a command line Python tool to semi-automate the dataset documentation process.  Written with a framework for expansion and customization, it produces template files in MarkDown that contain a basic data dictionary structure. This talk explains dataset documentation practices and how this tool could fit into the data publishing workflow.", "Derivation & Presentation: How to Effectively Tell A Data Science Story In A Business Environment \n \nMain message: Generating business value through data science has two forms, Derivation (doing analysis with a business eye) and Presentation (communicating the results effectively), conquering both results in positive and actionable change!\nPart 1:\nDerivation - How do you get into the mind of your stakeholders? How do you prescribe actionable change (real business value) through data. How do you manage your external stakeholders and their trade offs? This portion is all about creating value for the business from a data perspective\nPart 2:\nPresentation - Once you have your business insights, how do you communicate these insights effectively to your stakeholders? Often product managers and exec only require a high level of data-detail when confronted with a business decision. How can you effectively abstract your data-insights for different audiences. \nMy favorite line from this section is a rhetorical question, \"Does your audience need to see a graph? Or do they only need to understand the message the graph is saying? Do you even need to show a graph?\"\n \nData Science is essential for driving business value in the 21st century. This talk will focus on 1. Doing data science in the work place through a business lens and 2. Communicating that data message to your external stakeholders to drive actionable change. The audience will leave with an alternative understanding of take home tests, interviews, data communication and ultimately ...story-telling.", "Connecting Keywords to Knowledge Base Using Search Keywords and Wikidata\n \nMany large-scale Knowledge Bases (KB), such as Yago, Wikidata, Freebase, and Google\u00e2\u0080\u0099s Knowledge Graph, have been build by extracting facts fro structural Wikipedia and/or natural language Web documents. \nThe main observation of using knowledge base is that not all facts are useful and have enough information. To tackle this problem I will introduce how we build various data sources to help facts and keywords selection. We will also discuss important questions of KB applications including, \n- architecture of a KB processing and extraction system using Wikipedia and two public available KB including Wikidata and Yago; \n- method for calculating contextual relevance between facts. \n- how to present different facts to users.\n \nThe development of  large-scale Knowledge Base (KB) has drawn lots of attentions and efforts from both academy and industries recently . In this talk I will introduce how to use keywords and public available data to build our structural KB, and build knowledge retrieval system for different languages using python .  ", "Open Work Space \n \n\n \n", "WorldRowing.com: End To End Data Analysis\n \nEver wonder if 10,000 hours is really the baseline? Did some set of events make the difference in someone getting to the olympics? WorldRowing has captured data on races and athletes for decades but little has been done to analyze the data across countries, athletes and races to the possible outcomes and where investment might be best to identify impact to rowing at a high performance level. Data for each race, for each athlete is stored on the WorldRowing website. Varying amounts of personal information, plus race information, is available for athletes spanning several decades. This talk investigates the athlete race data from WorldRowing.com and demonstrates an end to end walk through of a data analysis problem using Python.\nIntroduction/Problem Statement\n    \u2043   World Rowing Athlete Database\n    \u2043   Description of the data\nData Analysis\n    \u2043   Scraping the data -  what to get, how to get it\n    \u2043   Data Munging and Wrangling\n    \u2043   Analysis - Does the data show correlations?\nTakeaways\n    \u2043   How to identify a problem for data analysis\n    \u2043   Dealing with inconsistent data\n    \u2043   Drawing conclusions from that dataset\n \nEver wonder if 10,000 hours is really the baseline? Did some set of events make the difference in someone getting to the olympics? WorldRowing has captured data on races and athletes for decades but little has been done to analyze the data across countries, athletes and races to the possible outcomes and where investment might be best to identify impact to rowing at a high performance level.", "SaaaS - Sampling as an Algorithm Service\n \nA lot of people understand the scikit-learn models of todays world but feel uneasy about the whole MCMC method of training. Why are these algorithms different? How is it that you don't use a gradient method but a sampler instead? It can feel a bit misterious if you've not properly been introduced to this other way of thinking.\nIn this talk I will explain the idea of sampling to get to your model and I will demonstrate it with examples. The goal is to start with a for loop and to end with understanding how MCMC algorithms work. As a consequence the audience will also get a proper introduction to PyMC3. In particular I will discuss the following;\n\nwhy markov chain sampling can be equivalent to direct sampling\nhow to build your own MCMC sampler with a for loop\nhow this for loop can be run faster by using PyMC3 instead\nthe key idea of inference and how i was briefly able apply it in a santa kaggle competition\nhow to analyse timeseries with MCMC in PyMC3\n\nParts of this talk are readily available on my blog;\n\nhttps://koaning.io/switching-to-sampling-in-order-to-switch.html\nhttps://koaning.io/elimination-via-inference.html\n\nLet me know if there are any questions. I am submitting multiple talks that I think are interesting and relevant to the PyData crowd, I'll gladly leave it to the committee which (or if any) of them are relevant to the local community.\n \nIn this talk I will explain the idea of sampling to get to your model and I will demonstrate it with examples. The goal is to start with a for loop and to end with understanding how MCMC algorithms work.", "Polynomial Chaos: A technique for modeling uncertainty\n \nThere is an intricate link between orthogonal polynomial families and well-known probability distributions. Known as Polynomial Chaos, this technique is largely unknown outside of some engineering fields. Nevertheless, the method allows us to model arbitrary distributions (with finite second moment) using distributions that are more familiar, e.g. the uniform or normal distributions. The polynomial chaos technique shifts the burden of understanding random variables to one of understanding deterministic series coefficients.\nThis method is particularly good for understanding dynamical systems with parametric uncertainty. The Polynomial Chaos expansion allows us to generate Monte Carlo simulations with far fewer simulation runs. In addition, we can use the method to quantify uncertainty in observations even when faced with small sample sizes. This talk will demonstrate the derivation of the technique and include some Python examples of ways it can be used to model systems and understand data in the presence of uncertainty. This will be a highly technical talk, touching on elements of measure-theoretic probability and functional analysis.\n \nParametric uncertainty is broadly difficult to quantify. In particular, when those parameters don't fit nice distributions it can be hard to generate reasonable simulations. Polynomial chaos is a somewhat obscure technique that leverages a natural connection between probability distributions and orthogonal polynomial families. This talk will demonstrate the technique and its applications.", "Clustering: A Guide for the Perplexed\n \nFinding clusters is a powerful tool for understanding and exploring data. While the task sounds easy, it can be surprisingly difficult to it well. Most standard clustering algorithms can, and do, provide very poor clustering results in many cases. Our intuitions for what a cluster is are not as clear as we would like, and can easily be lead astray. We will attempt to find a definition of clustering that makes sense for most cases, and introduce an algorithm for finding such clusters, along with a high performance python implementation of the algorithm, building up more intuition for what clustering really means as we go.\n \nFinding clusters is a powerful tool for understanding and exploring data. While the task sounds easy, it can be surprisingly difficult to do it well. Most standard clustering algorithms can, and do, provide very poor clustering results in many cases. We discuss how to do clustering correctly.", "Keynote: Data Science Community and You\n \nNavigating open source communities, including data science, can be challenging on many levels. By way of some illustrative stories, learn how to find your role and how you will create innovations which guide us to a better future.\n \nNavigating open source communities, including data science, can be challenging on many levels. By way of some illustrative stories, learn how to find your role and how you will create innovations which guide us to a better future.", "Using Exploratory Data Analysis to Discover Patterns in Image and Document Collections\n \nUsing Exploratory Data Analysis to Discover Patterns in Image and Document Collections\nIntroduction to EDA\n\nWhy EDA?\nChallenges with non-structured datasets: text and image collections\nFeature embedding of text and image collections using machine learning\nVisualization techniques for high dimensional data: PCA, t-SNE, PyImagePlot\n\nText Case study: using EDA to investigate the gender pronoun gap in newspapers\n\nAbout the text corpus and the problem: gender pronoun gap in language\nDefine summary statistics and extract features from text collection\nVisualize summary statistics and features using PCA\n\nImage Case study: using EDA to find patterns in collection of images\n\nFinding patterns in large collections of images\nExtracting Deep Learning features from images using SkiCaffe \nVisualize image collections using PyImagePlot\n\n \nExploratory Data Analysis (EDA) is one of the key sets of procedures for summarizing a dataset. In this talk we will develop an EDA procedure for large collections of documents and images (such as photo albums, emails, articles, etc). We will show features used from NLP and Deep Neural Nets and also introduce novel visualization techniques for large image collections using PyImagePlot. ", "How to Scrape Data from any Website\n \nYou will learn: \nHow to automate data extraction from javascript-heavy websites. \nPre-reqs: \nBeginners to scraping are welcome. Knowledge of javascript, HTML, and the HTTP request-response protocol are necessary. \n \nScraping gets hard--fast--when you need deal with dynamic content (read: javascript), pagination, and changing page structures. In other words, most popular websites built in the last 10 years.\nLearn how we built ParseHub to cope with these problems. Try the same techniques in python with the open-source Scrapy framework.", "Statistically Speculating on the Source of Sneezes and Sniffles\n \nSince April 2015 our group has studied the Allergic Rhinitis of a subject with the goal of building a machine learned model that predicts the need for antihistamines. Approximately 30% of the world's population suffers from allergies, we aim to provide a methodology for others to identify the drivers of their own symptoms. This is a \"citizen science\" project, currently focused on one individual and a year's worth of self-reported antihistamine usage, sneezing data and geolocated points. We'll discuss the available external data (including the London Air project's pollution readings, weather, diet, exercise and commute data), exploratory data analysis, our approach to feature engineering from time-series and text sources and our modeling progress. The data logging iPhone app and data preparation tools are all open sourced. Python tools discussed include scikit-learn, seaborn, statsmodels and textract. We'll also review our distributed working practices. \n \nSince April 2015 our group has studied the Allergic Rhinitis of a subject with the goal of building a machine learned model that predicts the need for antihistamines. Approximately 30% of the world's population suffers from allergies, we aim to provide a methodology for others to identify the drivers of their own symptoms. ", "Estimating stock price correlations using Wikipedia\n \n According to Modern Portfolio Theory, assembling a portfolio involves forming expectations about the individual stock's future risk and return as well as future correlations between stock prices. These future correlations are typically estimated using historical stock price data. However, there are situations where this type of data is not available, such as the time preceding an IPO. \n In this talk I look at an alternative to historical correlations as proxy for future correlations: using graph analysis techniques and text similarity measures in order to estimate the correlation between stock prices. \n The focus of the analysis will be on companies listed on the London Stock Exchange which form the FTSE 100 Index. I am going to use Wikipedia articles in order to derive the textual description for each company. Additionally, I will use the Wikipedia category structure to derive a graph describing relations between companies. \n The analysis will be performed using the scikit-learn and networkX libraries and example code will be available to the audience. \n \nBuilding an equities portfolio is a challenging task for a finance professional as it requires, among others, future correlations between stock prices.  As this data is not always available, in this talk I look at an alternative to historical correlations as proxy for future correlations: using graph analysis techniques and text similarity measures based on Wikipedia data. ", "How diversity drives excellence in our data-driven tech world\n \nThis session will be organized as a panel and QA discussion around the need for diversity, how diversity drives excellence, and incentivizing a diverse community. The panel participants will represent an array of diversity in backgrounds, experiences, current job positions, and career stages. \nThe flow of the discussion will loosely follow this general outline:\nBrief panelist introductions\nWhat obstacles have you encountered to building a diverse community?\nWhat lessons can you share about overcoming these obstacles?\nWhat actionable advice do you have for someone who is trying to increase understanding of the value of building a diverse a community?\nThe goals of the session are to build a discussion such that the audience will\nLearn about how others have overcome obstacles to inclusive environments.\nHave an increased understanding of how diversity benefits more than the minority.\nConnect with others and share resources for building up communities in which the importance of diversity is a foundational principle.\n \nIn an age where decisions are increasingly powered by data, the benefits of being data-driven are undeniable. We no longer need to convince the data science community of the value of data. But can the same be said about the value of diversity? This panel discussion and QA will be framed around the benefits of inclusion efforts and how building a diverse data science community benefits everyone.", "Show me the failures! Data products for manufacturing at shop floor\n \nPirelli has a 140 year old tradition of manufacturing with 20 factories across 14 countries and headquarter office in Milan. Production flows, logistic, machinery and the whole extended value chain has morphed through decades across a broad range of needs and circumstances.\nThe creation of a Data Science and Analytics department at the beginning of  2016 has the goal of speeding up change and innovation, starting from areas that are harder to tackle. Some of the most interesting challenges include:\n\nbring data products at shop floor to increase efficiency while being aware of UX principles\nkeep 2-sided communication alive with wide number of actors, particularly with IT, quality and engineering\nencourage active participation by providing accessible analytics tools and an internal Academy training program\nactivate the virtuous circle of prototyping, feasibility check and production releases for sound product lifecycles\nintroduce Agile development methodologies in traditional waterfall environments\nshape a roadmap with principal stakeholders starting from off-line through live analysis and heading to ahead-of-time predictions\n\nopening a steady communication channel across groups is progressively eroding barriers between white and blue collars, allowing teams to better understand each other requirements and kicking off a broader conversation. At the end of the first year since releasing the first prototype, there is much more on the plate, and groups are now more familiar with concepts of User Experience, release lifecycle, data exploration and agile development.\nOutline\nIn this talk we are going to show the data science team approach to prototyping and implementation of data products for Pirelli factories, both at shop floor and quality / engineering offices. Different needs or - taking a UX approach - different personas, lead to different outcomes: from large displays mounted in wide warehouses to detailed descriptions of statistical distributions, from near real-time processing of streams of data coming out of sensors to large computations for statistical models made on millions of rows stored in sql tables. \nThe sheer variety of technologies involved in the process is probably the biggest challenge when deploying at production level: aside standard data processing and machine learning packages, such as Pandas and scikit-learn, our Flask and Django based web infrastructures interact with MsSQL servers, JBoss data virtualisers, a Hadoop cluster and Oracle data warehouse, responsively adapting their output for different contexts with Angular and React front-ends.\nThe introduction of data products has triggered a little revolution allowing to improve and widen the offer of internal training via an Academy program, in search of competences inside the factories for all people willing to open an editor and write some Python and Javascript.\n \nThe Data Science and Analytics group at Pirelli has to deal with factories' day to day that can't be further from the aseptic crunching of data from a keyboard in an office. Our group took the lift, went down at shop floor and started asking questions to try and make their life better: turns out questions flowed the other way round and results were startling.", "Conversational AI: Building clever chatbots\n \nConversational software is everywhere: messaging apps have opened up APIs to bot developers and millions of consumers now own voice controlled speakers. But the tools and frameworks for building these systems are still immature. Tom will talk about Rasa, an open source machine learning framework for building conversational software. The talk will cover the algorithms Rasa uses to build flexible and robust voice and text systems, the trade offs in using supervised versus reinforcement learning, and whether it's really such a good idea to generate text with LSTMs.\nOutline\n\nComponents : NLU , DM , integration , NLG\nOverview of available tools and frameworks\nDescribe how Rasa does NLU\nMotivation & a chatbot leading to state machine hell\nHow Rasa does dialogue management.\nHow to advance a bots capabilities - closing the loop and data collection.\nCurrent research topics and challenges.\n\n \nMost chatbots and voice skills are based on a state machine and too many if/else statements. Tom will show you how to move past that and build flexible, robust experiences using machine learning throughout the stack.", "From rocks to a hammer: when and how to change your company's analytical tools\n \nMany organizations still rely on SPSS/SAS to do most of their analytical work. These tools are not only very costly ($10k+ per year per license), but are also limited (no scripting ability, very manual). In 2015, we began transitioning into Python to build robust tools and to reduce operational costs. Along the way, we learned a lot about propagating new tools within a company, reverse engineering, and helping others adjust to a new paradigm. This talk will outline our process of evaluating new tools, initial adoption and companywide propagation. One of the main findings was that open source does not mean free, and we had to take into account each person's experience and comfort in devising our implementation strategy. Lastly, we had to develop our own internal library in order to maintain functionality.\n \nDescription in progress", "Keynote: The NumFOCUS Ecosystem\n \nThe NumFOCUS Foundation is a 501(c)3 non-profit organization which supports several open source Python packages, R tools, and the Julia language; fosters diversity within the open source data science community, and produces the internationally acclaimed PyData conference series. Find out how NumFOCUS works.\n \nThe NumFOCUS Foundation is a 501(c)3 non-profit organization which supports several open source Python packages, R tools, and the Julia language; fosters diversity within the open source data science community, and produces the internationally acclaimed PyData conference series. Find out how NumFOCUS works.", "Create Spark REST APIs with flask\n \nPython has first class web development and data science tools. This talk looks at an approach to combine the two in a unique way in order to provide powerful access to Spark compute clusters via flask routes.\nI will go over an example of creating a Spark SQL context using pyspark within a flask application, performing initial processing on startup, caching the dataset in memory across worker nodes, and then creating Flask routes to access data using the Spark SQL context. By doing this, you can create simple REST API's so that you can share your data in your spark cluster with the world. \nThere are some issues that arise with this approach that I will discuss. We will go over some approaches that we can take to get around this using gunicorn's default synchronous mode as well as using nginx as a proxy in front of multiple spark-flask applications.\n \nSpark is a great tool for exploration and batch processing, but what about on demand REST services? In this talk we will go over how to create a flask service that vends data using a Spark backend with Spark SQL/Dataframes. ", "Clustering Data Science Interviews: Seven Related but Distinct Categories\n \nFor both candidates and interviewers, the interview process doesn't lend itself to the classic scientific method. We can't iterate over an interaction, and needs change. Deciding what to ask and who to interview can be a guessing game. As an interviewer, what kind of candidate do you really need? As a candidate, what kind of role do you really want? Having done more than 30 phone screens and technical challenges, I can define seven distinct types of Data Scientist interviews. I'll give examples of questions specifically relevant to assessing each aspect of a Data Scientist's skills, as well as discussing what's not being measured by the typical interview process. \n \nMatching candidates with openings: defining features across several sets of Data Scientist selection criteria, using both qualitative and quantitative methods. ", "pypet: A Python Toolkit for Simulations and Numerical Experiments\n \npypet (python parameter exploration toolkit [1]) is a new multi-platform Python toolkit for management of simulations and storage of numerical data. Exploring or sampling the space of model parameters is one key aspect of simulations and numerical experiments. pypet was especially designed to allow easy and arbitrary sampling of trajectories through a parameter space beyond simple grid searches. \nSimulation parameters as well as the obtained results are collected by pypet and stored in the widely used HDF5 file format [2]. This allows fast and convenient loading of data for further analyses. Furthermore, pypet provides an environment with various features. For example, among these are multiprocessing for fast parallel simulations, dynamic loading of data, integration of Git version control, and supervision of experiments via the electronic lab notebook Sumatra [3]. A rich set of data formats is supported encompassing native Python types, Numpy and Scipy data, and pandas DataFrames [4]. Moreover, the toolkit is easily extendable to allow the user to add customized data formats. pypet is a very flexible tool and suited for short Python scripts as well as large scale projects that involve simulations and numerical experiments.\n[1] https://pypet.readthedocs.org/\n[2] https://www.hdfgroup.org/HDF5/\n[3] https://pythonhosted.org/Sumatra/\n[4] https://pandas.pydata.org/\n \npypet manages exploration of the parameter space of any numerical simulation in Python, thereby storing your data into HDF5 files for you.  The toolkit offers a new data container which lets you access all your parameters and results from a single source. Data I/O of your simulations and analyses become a piece of cake!", "10 things I learned about writing data pipelines in Python and Spark. \n \n\n\nI am a Data Engineer at Duedil - a fintech enabling access to public data about private companies. \n\n\nStarting in the Q4, 2015, I wrote the financials data pipeline that collates ~200 data points and calculates ~300 metrics for ~80M account filings from ~11M private companies. \n\n\nAs I write, this is in production: https://bit.ly/1T3CzDG, https://bit.ly/1Q8iBBq.\n\n\nI used Python, Spark and loads of good fortune to make this. I would like to share my journey with the PyData community - purely to give something back, as I have learned so much out of the meetups. \n\n\nMy talk would include takeaways, patterns, anti-patterns, mistakes and big mistakes that I made and learned from. I think this will be very useful for beginner-intermediate data wranglers.\n\n\n \nStarting in the Q4, 2015, I wrote the financials data pipeline that collates ~200 data points and calculates ~300 metrics for ~80M account filings from ~11M private companies. In this talk, I would share what I learned.", "Big data processing with Apache Beam\n \nCurrently some popular data processing frameworks such as Apache Spark consider batch and stream processing jobs independently. The APIs across different processing systems such as Apache Spark or Apache Flink are also different. This forces the end user to learn a potentially new system every time. Apache Beam [1] addresses this problem by providing a unified programming model that can be used for both batch and streaming pipelines. The Beam SDK allows the user to execute these pipelines against different execution engines. Currently Apache Beam provides a Java and Python SDK.\nIn the talk, we start off by providing an overview of Apache Beam using the Python SDK and the problems it tries to address from an end user\u2019s perspective. We cover the core programming constructs in the Beam model such as PCollections, ParDo, GroupByKey, windowing and triggers. We describe how these constructs make it possible for pipelines to be executed in a unified fashion in both batch and streaming. Then we use examples to demonstrate these capabilities. The examples showcase using Beam for stream processing and real time data analysis, and how Beam can be used for feature engineering in some Machine Learning applications using Tensorflow. Finally, we end with Beam's vision of creating runner and execution independent graphs using the Beam FnApi [2].\nApache Beam [1] is a top level Apache project and is completely open source. The code for Beam can be found on Github [3].\n[1] https://beam.apache.org/\n[2] https://s.apache.org/beam-fn-api\n[3] https://github.com/apache/beam\n \nIn this talk, we present the new Python SDK for Apache Beam - a parallel programming model that allows one to implement batch and streaming data processing jobs that can run on a variety of execution engines like Apache Spark and Google Cloud Dataflow. We will use examples to discuss some of the interesting challenges in providing a Pythonic API and execution environment for distributed processing", "To explain or to predict?\n \nThe goal of statistical modelling is to build a relationship between the observational data and the phenomenon under investigation. Predictive and explanatory modelling are sub-branches of statistical modelling that optimise for prediction or explaination respectively.\nThe data science and machine learning literature is filled with examples of predictive modelling approaches while other disciples like economists are almost solely reliant on explanatory methods.\nIn theory a model that explains your dataset well should come with high predictive capabilities but in reality it has been shown that this is not the case. This leads to a different set of optimisations and tradeoffs during modelling.\nThe goal of this talk is to discuss these different tradeoffs and showcase the similarities and differences between these approaches as well as to discuss when is best to use one over the other.\n \nData science and machine learning in particular has a preference towards the kind of statistical modelling that optimises for prediction rather than explanation. Understanding which variables correlate with the phenomenon you are investigating is an inherent part of both approaches but models with high explanatory power are not necessarily the ones that yield the best predictions.", "Data Analytics and the new European Privacy Legislation\n \nThe EU General Data Protection Regulation (GDPR) is a stringent privacy regulation coming into effect in May 2018, along with the new planned ePrivacy Regulation. The GDPR provides for strong sanctions, with fines up to 20M Euro, or 4% of the yearly global turnover (whichever is higher) for companies in breach. It applies to any EU company, and any company processing the data of EU residents. It also broadens \"personal data\" to include anything generated by people behavior or referring to an identified person. Pseudonymised data will remain personal data under the GDPR.  \nThe GDPR turns data privacy from a simple legal problem to a core business issue about data collection, management, processing, and analysis. Doing any kind of data analytics, especially in B2C contexts, will become challenging. For example, marketing analytics based on people tracking may become technically difficult and methodologically almost unfeasible. Automated scoring and decision-making will become strongly regulated. Simply collecting \"big data\", figuring out later what to do with it will be a thing of the past - many companies will, therefore have to radically change their way of working with data: either delete a lot of their data or genuinely anonymize it. Once data is anonymized, it is no longer subject to the GDPR, but achieving high-utility anonymization is a difficult task. \nPrivacy by design and by default will become a pre-requisite for compliant personal data analytics. There has been a lot of progress over the last decade on data privacy and anonymization techniques. It is now possible to build recommender systems, classification models and almost everything imaginable with anonymized data, but maintaining data utility requires careful planning and optimization. This is especially tough for web/mobile/IoT data streams: high-dimensional data from multiple sources with spatial and temporal attributes. Most use cases are doable in compliant ways - state-of-the-art solutions will be reviewed.\n \nThey upcoming privacy legislation of the EU will radically change the way we do data analytics, restricting the processing of personally identifiable data. We will go through common data processing scenarios and learn how the new legislation will affect them, offering practical solutions.", "Python useRs\n \nIt may be better to extend a simple analysis in R than passing around CSV files.\nEven worse, there might be a new analysis package that only exists in R,\nor certain bits infrastructure are already implemented in R and it needs to\nincorporate a bit of Python analysis.\npythonistas can be a #rstats user, they just need examples of the similarities\ndifferences between container objects, functions, dataframes, and how to use Python in an R environment and vice versa.\nThis allows data scientists to work in their preferred language and still use each other's analysis in a larger analysis project.\n \nAccording to the most recent IEEE language rankings, R is now the 5th most popular language.\nIt is the only domain specific language in the top 5, behind the general-purpose languages C, Java, Python, and C++, respectively.\nIt is common for data science teams to work in multiple languages and for the Pythonista,\na working knowledge in R is useful for collaboration, analysis, and performance.", "Keynote: How to Engage with the South Big Data Innovation Hub\n \nIn recognition of the substantial and growing impact of big data to the U.S., across sectors, in 2012 the White House launched a multi-agency research initiative to foster and coordinate big data innovation across the US. Under this initiative, the National Foundation launched four Big Data Regional Innovation Hubs, new organizations intended to develop the Big Data innovation ecosystem and facilitate thematic communities\u00e2\u0080\u0099 use of data sciences for societal benefit. Specifically, the Big Data Regional Innovation Hubs accelerate partnerships among people in business, academia, and government who\u00c2\u00a0apply\u00c2\u00a0data science and analytics to help solve regional and national challenges. The Big Data Hubs cover all 50 states and currently include several hundred universities, corporations, federal agencies, and non-governmental organizations. Dr. Shanley will introduce the Big Data Hubs and report on some of the significant activities of the South Big Data Innovation Hub. Finally, she will discuss opportunities to engage with the Big Data Hubs and our growing networks of Public/Private partnerships. https://southbigdatahub.org \n \nBig Data is a prominent, rapidly emerging discipline with far-reaching scientific and economic potential, yet there remains a gap in the translation of Big Data research findings into economic growth and end-user impact. To exploit the full potential of Big Data, the Big Data Innovation Hubs endeavor to foster innovation through collaboration, diversity, education, and workforce development.", "Genotype-Phenotype Modelling with Python and Machine Learning\n \nWorking with genomic datasets poses a number of challenges such as high-dimensional vector spaces and extreme amounts of noise. All genotype-phenotype studies ask \"Given this textual genome, will a some trait be observed?\" and can be viewed from the lens of a text classification problem. Predicted traits can be anything observable in the organism such as their hair color or whether they will be likely to have cancer. In this talk, I demonstrate the process for constructing a genotype-phenotype machine learning pipeline which predicts resistance to certain pharmaceuticals in Neisseria. We will discuss how data libraries in Python are used throughout each step in this pipeline.\nWe will first get an overview of the necessary biological background including the fundamental dogma of molecular biology and gene mutation types. We will then look at popular of feature vector representations for genomes including k-mers and single-nucleotide polymorphism presence. I will demonstrate training classifiers for predicting whether the given organism will exhibit antimicrobial resistance and inspect our trained classifiers for making biological conclusions. Lastly, I discuss how recurrent neural nets, which are useful for non-fixed length input, and naturally work well with genomes.\n \nGenotype-phenotype studies are done for predicting traits such as whether someone will go bald or have a particular disease given their only genome. We look at how Python libraries such as scikit-learn and keras have made it easier to develop these statistical models. We describe a pipeline to predict antimicrobial resistance in bacteria and elaborate on challenges when working with genomic data.", "Prescriptive Analytics with DOcplex and pandas \n \nPrescriptive analytics technology recommends actions based on desired outcomes, taking into account specific scenarios, resources, and knowledge of past and current events. This insight can help organizations make better decisions and have greater control of business outcomes. Prescriptive analytics used to rely on dedicated modelling languages like AMPL, GAMS, OPL\u00e2\u0080\u00a6 DOcplex is an open source Python package that intends to provide the same flavor as those dedicated languages, both in terms of expressiveness and performance, thanks to pandas. It can actually run with community edition (free) of IBM CPLEX, a free for academics edition or a commercial edition. This talk provides an overview of Prescriptive Analytics and illustrates with examples how DOcplex, combined with tools available in the python ecosystem, can smoothly support the full workflow from data import and preparation, to visualization of optimization results.\n \nThis talk provides an overview of prescriptive analytics technology and illustrates how DOcplex, an open source Python package, supports the development of optimization models. Relying on pandas and the Python ecosystem, a complete prescriptive analytics workflow, from data import and pre-processing to visualization of optimization results, can be implemented in a single language", "ExpAn - A Python library for advanced statistical analysis of A/B tests\n \nA/B tests, or randomized controlled experiments, have been widely applied in\ndifferent industries to optimize the business process and the user experience.\nHere we'll introduce a Python library, ExpAn, intended for the statistical\nanalysis of A/B tests. \nThe input data to ExpAn has a standard format, which is defined \nto interface with different data sources. The main statistical functions in\nExpAn are all standalone and work with either the library-specific input \ndata structure or some\nPython built-in data types. Among others, the functions can be used to\nassess whether the randomization is appropriate, and measure the expectation\nand error margin of the uplift due to the treatment. We also implemented a \nrobust discretization algorithm to handle typical heavy-tailed distributions\nin the real world. Finally, a generic result\nstructure is designed to incorporate results from different types of analyses.\nOne can easily feed data from other domain-specific data fetching modules\ninto ExpAn.\nOther advanced algorithms for the analysis of \nA/B test data can be implemented and plugged into ExpAn, eg. a Bayesian \nhypothesis testing scheme instead of the \nfrequentist approach. The generality of the result structure also makes it\nhandy to apply different kinds of visualization on top of the data.\n \nA/B tests have been adopted by various companies in different industries to \ndrive the data-driven decision making process. Therefore, a statistically solid \nanalytic framework is of common interest to a large community. We'll introduce \nthe ExpAn library developed for the statistical evaluation of A/B tests, it has \na generic data structure and all functions are standalone. ", "Python vs Orangutan\n \nBorneo and Sumatra in South East Asia are home to a number of orangutan rescue and rehabilitation centres. A common problem each of these have is keeping track of the animals they release back in the wild. Traditionally this is done with ground teams using a radio antenna to search for the weak radio pings each animal sends out. A true needle in the haystack problem.\nThis talk will discuss a drone-based tracking system that has been developed to alleviate this problem. More specifically the data analysis problem of searching and locating the signal of each animal against a noisy background.\n \nSomewhere in 2000km2 worth of Bornean Jungle there are 6 orangutans that need to be found and tracked. All you have is a short radio ping against a noisy background resulting in a unique and interesting anomaly detection problem. ", "Make it Work, Make it Right, Make it Fast - Debugging and Profiling in Dask\n \nDask is a pure python library for parallel and distributed computing. It's designed with simplicity and flexibility in mind, making it easy to parallelize the complicated workflows often found in science. However, once you get something working, how do you debug or profile it? Debugging and profiling parallel code is notoriously hard! In this talk we'll cover the various tools Dask provides for diagnosing bugs and performance bottlenecks, as well as tips and techniques for resolving these issues.\nStarting with an example single-threaded probram, we'll walk through adding Dask to parallelize it, and then iterate on this example to gradually improve performance throughout the talk. Attendees should leave having a better understanding of:\n\nWhat tools Dask provides for debugging on a single machine\nHow Dask uses IPython to make debugging distributed computations easy\nHow to profile Dask code both on a single machine and on a cluster\nHow to interpret the graphs presented in the Dask Dashboard\nPerformance techniques for attacking bottlenecks once they've been identified.\n\n \nDask is a pure python library for parallel and distributed computing. It's designed with flexibility in mind, making it easy to parallelize the complicated workflows often found in science. However, once you get something working, how do you debug or profile it? In this talk we'll cover the various tools Dask provides for diagnosing bugs and bottlenecks, as well as tips for resolving these issues.", "KEYNOTE: Data for good: Lessons from the frontline\n \nMost of us will know data for business as the art and science of using proprietary data to solve high value commercial problems. But how can those same techniques be used to solve societal problems as well? From tackling money laundering through network statistics to establishing the true scale of youth homelessness through Bayesian inference, DataKind UK will share examples of how they\u2019ve partnered with charities to use data for good. Reflecting on what has and hasn\u2019t worked in three years, they will share the critical success factors they\u2019ve identified along the way.\n \nDataKind UK share their experiences and critical success factors from 3 years of learning-by-doing in using data in the service of humanity.", "Fast Multidimensional Signal Processing using Julia with Shearlab.jl\n \nThe Shearlet Transform was proposed by the Professor Gitta Kutyniok (https://www.tu-berlin.de/?108957) and her colleagues as a multidimensional generalization of the Wavelet Transform, and since then it has been adopted by a lot of Companies and Institutes by its stable and optimal representation of multidimensional signals. Shearlab.jl is a already registered Julia package (https://github.com/arsenal9971/Shearlab.jl) based in the most used implementation of Shearlet Transform programmed in Matlab by the Research Group of Prof. Kutyniok (https://www.shearlab.org/software), it was developed as a project apart of my PhD studies but ended up being the main computational tool of them, used mainly to reconstruct the Light Field of a 3D Scene from Sparse Photographic Samples of Different Perspectives with Stereo Vision purposes.\nWhy I think this will be an interesting thing to present at JuliaCon 2017?\n\n\nA lot of research institutes and companies have already adopted the Shearlet Transform in their work (e.g. Fraunhofer Institute in Berlin and Charit\u00e9 Hospital in Berlin, Mathematical Institute of TU Berlin) by its directional sensitivity, reconstruction stability and sparse representation; with applications that goes from MRI Imaging in Hospitals to Video Compression Decoding. \n\n\nI am convinced Shearlab.jl is the best implementation so far of Shearlet Transform, basing my arguments on the benchmarks already runned against the last Matlab version which is the most used at the moment (here benchmarks https://github.com/arsenal9971/Shearlab.jl/blob/master/benchmarks/README.md) beating it by at least double the speed on different experiments. \n\n\nNot everything is about performance and technical mathematics, so I also have cool usage examples to show with some algorithms that have been implemented lately, like: Image Decomposition and Recovery (https://github.com/arsenal9971/Shearlab.jl/blob/master/examples/Shearlets/Examples%20of%20Image%20Decoding%20and%20Recovery.ipynb), Image Denosing (https://github.com/arsenal9971/Shearlab.jl/blob/master/examples/Shearlets/Image%20Denoising.ipynb) Image Inpainting (the coolest so far) (https://github.com/arsenal9971/Shearlab.jl/blob/master/examples/Shearlets/Image%20Inpainting.ipynb) which I am using at the moment for the Light Field Recovery of a sparsely sampled 3D scene. - The Package was also already presented in the Berlin Julia Users MeetUp with a very good response and interest from the community (https://www.meetup.com/es-ES/Julia-Users-Group/events/236791753/).\n\n\n \nShearlab is a Julia Library with toolbox for two- and threedimensional data processing using the Shearlet system as basis functions which generates a sparse representation of cartoon-like functions with applications on Signal Processing, Compressed Sensing, 3D Imaging, MRI Imaging and a lot more, with visible improvements with respect of the Wavelet Transform in representing multidimensional data.", "Promoting a data-driven culture in a world of microservices\n \nWhat you'll learn\nTips for:\n\nExporting from production databases\nBuilding reliable data pipelines\nChoosing which tools to use\nSpreading data literacy at your company\n\nSummary\nA few years ago, Hudl switched from a monolithic app to a microservices architecture. This switch was great for developer productivity and site reliability, but it made it more difficult to get insights on user behaviors. Data was siloed across many databases, so querying across services was difficult or impossible.\nIn July 2015, we created the Data Engineering team in order to make our data more accessible. The Data Engineering team built Fulla, an internal data warehouse containing data from 20+ production databases and billions of logs. Today, squads from all areas of the company use Fulla to track key metrics and trends, make decisions on priorities, and engage with users. \nIn this talk, I discuss all aspects of Fulla. I talk about some of our challenges, including dealing with large scale MongoDB databases, as well as lessons learned on ensuring data quality and promoting a data-driven culture. Finally, I'll provide suggestions on tools to use, both paid and open-source. Tools discussed include Spark, Luigi, Redshift, re:dash, AWS Lambda, Sqoop, Hive, and more. \n \nAt Hudl, we give every employee full access to our data warehouse, and over 50% of our employees have personally written a query against it. In this talk, I discuss our journey to democratize our data. I touch on technical and non-technical challenges, including the tools we use and the structure of our teams.", "Dynamic Object-Gaze Tracking with OpenCV\n \nMost eye tracking solutions track gaze with respect to a static object like a computer screen, making it easy to know exactly where a person is looking with respect to an image on the screen. This makes analysis easier, but doesn\u00e2\u0080\u0099t accurately reflect real-life. What happens when we move eye tracking into a more realistic, dynamic setting, using eye tracking glasses that allow people to move around? People can interact with objects in a much more natural manner, but a new challenge is introduced: We only have gaze data with respect to the glasses frame of reference. In order to apply conventional analysis methods to these data, we need to map dynamic gaze back onto a static reference image, compensating for distance, head movement, and perspective.\nUsing the OpenCV package and its efficient implementations of common computer vision algorithms, we developed a method to find objects of interest in video from eye tracking glasses and return gaze coordinates over those objects, enabling experimenters to apply conventional data analysis methods to eye tracking behavior obtained in dynamic, real-world situations.\n \nUsing computer vision techniques, we extended eye tracking technology to allow for data normalization across dynamic environments. We applied these techniques to subjects viewing artwork at Duke\u00e2\u0080\u0099s Nasher Museum of Art.", "Using Twitter's Breakout Detection Package\n \nIn this talk we will explore how to use the Breakout Detection to detect level shifts in univariate data. Roughly, a level shift is a significant, sustained change in the mean of the data. Level shifts can be a signal that some fundamental change in environment the data is living in has occurred. For instance, the number of Tweets about the show Breaking Bad as a function of time as new episodes came out, or as evidenced in price of the S&P500 index during the economic downturn of 2008. \nWe will first go through some of the math used to develop the Breakout Detection algorithm, which makes use of some techniques from the areas of statistics known as Robust Statistics and Energy Statistics. Next we will see how to install and implement the package, and finally look at some examples of this package applied to some time series data.\n \nIn this talk, we will explore and learn how to use the package Breakout Detection, developed by some folks at Twitter. This package detects level shifts in univariate (often time series) data. We will go through some of the math behind the algorithm, and then look at some examples of this algorithm in action.", "Carousel - A Python Model Simulation Framework\n \nIntroduction\nMathematical models consist of algorithms glued together with generic routines. While the algorithms may sometimes be unique and complex, the rest of the code is often simple and routine. Sometimes mathematical models developed by teams of developers over time become difficult to update because there is no framework for how new data, calculations and outputs are integrated into the existing models. Carousel allows developers to focus on creating complex mathematical models that are robust and easy to maintain by abstracting generic routines and establishing a simple but extensible framework.\nThe Framework\nA Carousel basic model consists of 5 built in layers:\n\nData\nFormulas\nCalculations\nSimulations\nOutputs\n\nLayers\nCarousel is extensible by creating more advanced models and layers. A Carousel model is a collection of layers. Carousel layers share a common base class. Each layer also has a corresponding object and a registry where objects are stored. All layers have a load method that loads all of the layer objects specified into the model. When a model is loaded it loads the objects specified for each layer.\nExample\nConsider a load shifting algorithm for residential or commercial rooftop solar power. The model might have a performance calculation, a load calculation, a cost calculation and an optimization algorithm that determines how home or business appliances are operated to minimize overall yearly cost of the system.\nThe performance calculation contains several formulas which require input data from an internal database of solar panel parameters and an online API of weather conditions, so the user creates a data source and reader for each of these. There are some data readers already included in Carousel and once a data reader is created it can be reused in many different projects. Maybe the user submits a pull request to Carousel to add the new API and database reader. The load calculation contains formulas for how the appliances are used. The input data for the appliances are entered into a generic worksheet so the user creates a data source for appliances and uses the XLRDReader to collect the data for each appliance from their worksheets.\nThe user organizes the formulas into 4 modules, that correspond to each of the calculations, but some formulas are reused since they are generic. For example, data frame summation formulas are used with different time series to create daily, monthly and annual outputs. The user maps out the calculations and specifies their interdependence to other formulas. For example, the cost calculation depends on the load and performance calculations and the optimization algorithm depends on the cost.\nThe user specifies each output name, initial value and other attributes. Specifications for each layer can be in a JSON parameter file or directly in the code as class attributes; Carousel will interpret either at runtime when it creates the model. Finally the user creates a simulation which in this case is unique because instead of marching through time or space, the simulation iterates over potential load shifting solutions from the algorithm. The user decides which data or outputs to log during the simulation and which to save in reports. Now that the model is created, the user loads the model and sends it the \"start\" command. After the simulation is complete, the user can examine the outputs and their variances. The outputs will have been automatically converted to the units specified in the model.\nData Layer\nThe data layer handles all inputs to the models. The data layer object is a data source. Each data source has a data reader. A data source is a document, API, database or other place from which input data for the model can be obtained. The data source and reader provide a framework for specifying how data is acquired. For example a data source for stock market prices might be a public API. An implementation of the stock market API data source specifies the names and attributes of each input data that will be read from the API and how the data reader should read them. The data source is similar to a deserializer because it describes how the data from the source should be interpreted by the model and creates an object in the data registry.\nFormula Layer\nThe formula layer handles operations on input data that generate new outputs. It differs from the calculation layer which handles how formulas are combined together. The formula layer object is a formula, and each formula has a formula importer. For example the Python formula importer can import formulas that are written as Python functions.\nCalculation Layer\nCalculations are combinations of formulas. Each calculation also has a map of what data and output are used as arguments and what outputs the return values will be. Calculations also implement calculators. Currently there is a static calculator and a dynamic calculator, but new calculators can be implemented that can be reused in other models. The calculation also implements indexing into data and output registries in order retrieve items by index or at a specific time.\nOutput Layer\nOutputs are just like data except they don't need a reader because they are only generated from calculations. Each output is like a serializer because it determines how output objects will be reported or displayed to the user.\nSimulation Layer\nThe simulation layer determines the order of calculations based on the calculation dependencies. It first executes all of the static calculations and then loops over dynamic calculations, displaying logs and saving periodic reports as specified.\nModel\nThe model is a low level class that can be extended to add new layers or implement new simulation commands. Currently only the basic model is implemented. A new model might contain a post processing layer that generates plots and reports from outputs.\nRegistry\nEvery layer has a dictionary called a registry that contains all of the layer objects and metadata corresponding to the layer attributes. The registry implements a register method that doesn't allow an item to registered more than once. Each layer registry is subclassed from the base registry so that specific layer attributes can be associated to each key. For example, data sources and outputs have a variance attribute while formulas have an args attribute.\nRunning Model Simulations\nAfter a model has been described using the framework, it can be loaded. Then any or all of the model simulations can be executed from the model. The simulation specifies commands to the model that user sends using the models command method. Currently the basic model can execute the simulation start command.\nUnits and Uncertainty\nCarousel uses the Pint units wrapper to convert units as specified. Uncertainty is propagated using the UncertaintyWrapper package which was developed for Carousel. It can wrap Python extensions and non-linear algorithms without changing any code. It propagates covariance and sensitivity across all formulas. \nFuture Work\nA basic version of Carousel is ready now. There is an example of a photovoltaic module performance model in the documentation online at GitHub. Some ideas for new features are listed in the Carousel wiki on GitHub.\n\nData validation\nReuse 3rd party serializer/deserializer for data layer\nModel integrity check\nDatabase data reader\nREST API data reader\nOnline repository to share data readers, simualtions, formulas and layers\nAutomatic solver selection\nPost processing layer\nTesting tools\nConcurrency and speedups\nRemote process and Carousel client\n\nSource, Docs, Issues and Wiki\n\nCarousel\nUncertaintyWrapper\n\nPrevious Presentations\nCarousel was presented at the 5th Sandia PVPMC Workshop hosted by EPRI in Santa Clara in May 2016\nAcknowledgement\nCarousel and UncertaintyWrapper were developed with the support of SunPower Corp. They are distributed with a BSD 3-clause license.\n \nCarousel is an extensible framework for mathematical models that handles generic routines such as loading and saving data, generating reports, converting units, propagating uncertainty and running simulations so developers can focus on creating complex algorithms that are easy to share and maintain.", "Which city is the cultural capital of Europe? An introduction to Apache PySpark for GeoAnalytics\n \nWhich city is the cultural capital of Europe? An introduction to Apache PySpark for Big Data GeoAnalytics\nIn this workshop we will very quickly introduce you to the Apache Spark stack and then get into the meat of performing a full featured geospatial analysis. Using OpenStreetMap data as our base, our end goal will be to find the most cultural city in Western Europe!\nThat's right! We will develop our own Cultural Weight Algorithm (TM) ;) and apply it to a set of major cities in Europe. The data will be analyzed using Apache Spark and in the process we will learn the following phases of Big Data projects:\n\nConsuming: Retrieving raw data from REST API's (OpenStreetMap).\nPreparation: Data exploration and schema creation of geospatial data\nSummarize: We will query data by Location. Perform Spatial Operations such as finding Overlapping geospatial features, do joins by location, also known as Spatial Joins and finally obtain location based summary statistics to arrive at our answer regarding the cultural capital of Europe.\n\nHere's a summary of the workshop as a sketch.\n\nI hope you will join us on this journey of exploring one of the most exciting technology stacks to come out of the good folks at the UCBerkeley\nWhy Spark?\nSpark has quickly overtaken Hadoop as the front runner in big data analysis technologies. There are a number of reasons for this such as its support for developer friendly interactive mode, it's polyglot interface in Scala, Java, Python, and R, and the full stack of Algorithmic libraries that such language ecosystems offer.\nOut of the box, Spark includes a powerful set of tools: such as the ability to write SQL queries, perform streaming analytics, run machine learning algorithms, and even tackle graph-parallel computations but what really stands out is its usability.\nWith it's interactive shells (in both Scala and Python) it makes prototyping big data applications a breeze.\nWhy PySpark?\nPySpark provides integrated API bindings around Spark and enables full usage of the Python ecosystem within all the nodes of the Spark cluster with the pickle Python serialization and, more importantly, supplies access to the rich ecosystem of Python\u00e2\u0080\u0099s machine learning libraries such as Scikit-Learn or data processing such as Pandas.\nDuring the workshop we are going to use a Docker Container with the relevant libaries. Please try to have the latest docker running on your machine for hands-on work!.\n \nIn this workshop we will very quickly introduce you to the Apache Spark stack and then get into the meat of performing a full featured geospatial analysis. Using OpenStreetMap data as our base our end goal will be to find the most cultural city in Western Europe!", "Python for .NET or .NET for Python\n \nFor long time .NET developers wanted to tap into the rich libraries from SciPy and PyData communities. Bridging .NET and CPython runtimes is the easiest approach to solving this problem. Python for .NET (pythonnet, Python.NET) makes this possible by wrapping CPython C-API from C# in .NET and Mono runtimes. This also allows two-way interoperability between both runtimes on Windows, Linux, and OSX (MacOS), and even Linux subsystem on Windows!  This presentation is going to show how to use Python code from .NET and .NET assemblies from Python.\nParticular importance in Python.NET is given to installation options: Python wheels, conda and nuget binaries, docker images, and even distribution with WinPython. The deployment is also simplified with tools such as PyInstaller and cx_freeze. This presentation will show how to install and deploy Python.NET apps using these tools.\nIn this talk we are going to show how to call numpy, scipy, pandas, matplotlib, sympy, and pyomo from .NET without much boilerplate code. The second part will show how to use .NET from Python, particularly C# magic cells (clrmagic) in ipython kernel with Jupyter Notebooks. The Python.NET tutorial was converted to Jupyter Notebook and C# code cells that are embedded within the same notebook. \nThis presentation will show few demos with REPL experience both from C# and Python using IPython, scriptcs, and Visual Studio.\nAll libraries used in this presentation are open-source and available on all major platforms. Python for .NET is a library developed since 2003, which \"graduated\" from Zope, moved to SourceForge and eventually to GitHub, where it became widely used and adopted. This talk will also demonstrate clrmagic - Jupyter extension, built on top of pythonnet and developed by authors of this presentation.\nPython.NET is built with a number of open-source technologies. It uses pycparser, ply, and clang (gcc and MSVC also work) for parsing internal CPython structures. .NET types are exposed to Python with Unmanaged Exports (DllExport) open-source \"compiler\" on Windows. On other platforms, C-API of Mono and CPython are used to \"bootstrap\" both runtimes.\nPython.NET is used by financial algorithmic trading platforms, engineering companies, and few open-source projects: QuantConnect Lean, pywebview, Pybee Beeware Toga cross-platform UI toolkit. Python.NET enables large .NET applications to embed numerical Python libraries without boilerplate code and without sacrificing for performance.\n \nFor long time .NET developers wanted to tap into the rich libraries from SciPy and PyData communities. Bridging .NET and CPython runtimes is the easiest approach to solving this problem! Python for .NET (pythonnet, Python.NET) makes this possible by wrapping CPython C-API from C# in .NET and Mono runtimes. This also allows two-way inter-operability between both runtimes on Windows, Linux, and OSX.", "Recommender systems with Tensorflow\n \nRecommender systems are used across the digital industry to model users' preferences and increase engagement. Popularised by the seminal Netflix prize, collaborative filtering techniques such as matrix factorisation are still widely used, with modern variants using a mix of meta-data and interaction data in order to deal with new users and items. We will demonstrate how to implement a variety of models using Tensorflow, from simple bi-linear models expressed as shallow neural nets to the latest deep incarnations of Amazon DSSTNE and Youtube neural networks. We will also use TensorBoard and particularly the embedding projector to visualise the latent space for items and metadata.\n \nThis talk will demonstrate how to harness a deep-learning framework such as Tensorflow, together with the usual suspects such as Pandas and Numpy, to implement recommendation models for news and classified ads.", "What does it all mean? - Compositional distributional semantics for modelling natural language\n \nRepresenting words as vectors in a high-dimensional space has a long history in natural language processing. Recently, neural network based approaches such as word2vec and GloVe have gained a substantial amount of popularity and have become an ubiquituous part in many NLP pipelines for a variety tasks, ranging from sentiment analysis and text classification, to machine translation, recognising textual entailment or parsing.\nAn important research problem is how to best leverage these word representations to form longer units of text such as phrases and full sentences. Proposals range from simple pointwise vector operations, to approaches inspired by formal semantics, deep learning based approaches that learn composition as part of an end-to-end system, and more structured approaches such as anchored packed dependency trees.\nIn this talk I will introduce a variety of compositional distributional models and outline different approaches of how effective meaning representations beyond the word level can successfully be built. I will furthermore provide an overview of the advantages of using compositional distributional approaches, as well as their limitations. Lastly, I will discuss their merit for applications such as aspect oriented sentiment analysis and question answering.\n \nDistributional semantic word representations have become an integral part in numerous natural language processing pipelines in academia and industry. An open question is how these elementary representations can be composed to capture the meaning of longer units of text. In this talk, I will give an overview of compositional distributional models, their applications and current research directions.", "Data Sciencing While Female\n \n\nIntroduction\nData Sciencing while Female\nInspiration for the Women Data Scientists DC Meetup (WDSDC)\nThe First Year of WDSDC in Review\nGrowth for WDSDC as Compared to Other DC Data Science Meetups\nLessons Learned\n\n \nHow can we increase the number of female data scientists in the workplace? By building a community. Dr. Amanda Traud was the only woman on her data science team when she started the group Women Data Scientists DC. In one year, the group grew to over 1,000 members. Dr. Traud will discuss the ups and downs of being a woman in data science and how to encourage and include more women in the field.", "Identifying Racial Bias in Policing Practices: Open Data Policing\n \nNorth Carolina developers and civil rights advocates used demographic data from nearly 20,000,000 unique NC traffic stops in the state to create a digital tool for identifying race-based policing practices: https://opendatapolicingnc.com/\nThe talk will cover:\n Background on NC data collection law\n Impediments to access\n Open source project, the API architecture, and the visualizations used to present and highlight key data points\n Expansion to Maryland\n How the website helps:\n Community-based campaigns for policy reforms organized largely around the data\n Criminal defense work\n Civil litigation\n* Improved police management practices\nCome learn more about the process and how they using it to challenge these practices in the courts.\n \nNorth Carolina developers and civil rights advocates used demographic data from nearly 20,000,000 unique NC traffic stops in the state to create a digital tool for identifying race-based policing practices: https://opendatapolicingnc.com/. Come learn more about the technology used (Python/Django) and how they're expanding into the state of Maryland.", "Evolutionary Algorithms: Perfecting the Art of \"Good Enough\"\n \nScience and data analytics are full of optimization problems: we may\nwant to minimize cost, maximize model fit, or find the best clustering for a\ndataset. Many problems are combinatorially explosive, making it\nhopeless to find the global optimum with an exhaustive search. But\nmany algorithms can find excellent solutions. As long as you can\nmeasure the \"fitness\" of a given solution, you can use evolution to\nfind increasingly better solutions. This talk will cover:\n\nwhat an evolutionary algorithm is, and how to use them to solve\n  scientific problems\nfitness landscapes, mutation functions, and fitness functions\nan overview of some useful heuristic optimizers (hill climbers,\n  simulated annealing, MCMC, genetic algorithms), and their strengths\n  and weaknesses\n\n \nEvolutionary algorithms let us tackle all kinds of impossible problems. Want to design a short delivery route, but there are more possible solutions than atoms in the universe? Well, evolutionary algorithms can't promise to find the optimal solution, but can guarantee finding a pretty great one. I'll give an overview of these algorithms, and how you can use them for your own impossible problems.", "Virtual Laboratories: Vision for an Earth Sciences Collaborative And Distributed Environment\n \nVirtual Laboratories: Vision for an Earth Sciences Collaborative And Distributed Environment\nAbstract\nWith the advent of IPCC reports, Earth Sciences in general, and Climate Sciences in particular, have become an invaluable tool for policy makers. With advances in technologies, both in computation power and in storage capabilities, the amount of data available to scientists and policy makers is ever increasing. As a result, it has become nearly impossible for a scientific group (institution), to hold all the necessary data for a study at a single computer (facility). On one hand the Earth Science Grid Federation (ESGF) project solves the data distribution, on another hand the Climate Data Analysis Tools software offers advanced tools for earth science data analysis. \u00e2\u0080\u009cVirtual Laboratories\u00e2\u0080\u009d aim at combining both projects (non-exclusively) to allow scientists and policy makers to work in a remote and distributed environment.\nOutline\nHistorical Notes: PCMDI\nMIPs\nAMIP and the lost data\nAMIP2\nMany MIPs\nCDAT\nOverview\nNumpy, cdms2, vcs, and utils...\nOthers Python Packages? Distribution? Conda!\nEarth System Grid Federation\nDistributed Data\nAuthentication\nCompute Team\nAPIs\nBackends\nFusing the Projects: Virtual Laboratory\n \nEarth Sciences and Climate Sciences in particular have become invaluable for policy makers. With advances in technologies the amount of data available  is ever increasing making in situ analysis impossible. We envision to merge the Eart System Grid Federation and Climate Data Analysis Tools projects  into Virtual Laboratories offering a remote distributed environment for Earth Sciences.", "Designing spaCy: A high-performance natural language processing (NLP) library written in Cython\n \nThe spaCy natural language processing (NLP) library features state-of-the-art performance, and a high-level Python API. Efficiency is crucial for NLP, because job sizes are constantly increasing. The key algorithms are also relatively complicated, and frequently subject to change, as new research is published. This talk describes how we\u00e2\u0080\u0099ve met these challenges in spaCy, by implementing the library in Cython. Unlike many Cython users, we did not write the library in Python first, and then optimize it. Instead, we designed the library as a C extension from the start, and added the Python  API on top. This allows us to build the library on top of efficient, memory-managed data structures, without having to maintain a separate C or C++ codebase. The result is the fastest NLP library in the world, support for GIL-free multithreading, in a concise readable codebase, and with no compromise on user friendliness.\n \nThe spaCy natural language processing (NLP) library features state-of-the-art performance, and a high-level Python API. Efficiency is crucial for NLP, because job sizes are constantly increasing. This talk describes how we\u00e2\u0080\u0099ve met these challenges in spaCy, by implementing the library in Cython.", "Real-time association mining in large social networks\n \nThere is a growing realisation that to combat the waning effectiveness\nof traditional marketing, social media platform\nowners need to find new ways to monetise their data. Social\nmedia data contains rich information describing how real\nworld entities relate to each other. Understanding the allegiances,\ncommunities and structure of key entities is of vital\nimportance for decision support in a swathe of industries that\nhave hitherto relied on expensive, small scale survey data. We present a real-time method to query and visualise regions of networks that are closely related to a set\nof input vertices. The input vertices can define an industry,\npolitical party, sport etc. The key idea is that in large digital\nsocial networks measuring similarity via direct connections\nbetween nodes is not robust, but that robust similarities\nbetween nodes can be attained through the similarity of\ntheir neighbourhood graphs. We are able to achieve real-time\nperformance by compressing the neighbourhood graphs using\nminhash signatures and facilitate rapid queries through\nLocality Sensitive Hashing. These techniques reduce query\ntimes from hours using industrial desktop machines to milliseconds\non standard laptops. Our method allows analysts to\ninteractively explore strongly associated regions of large networks\nin real time. Our work has been deployed in Python based software and uses the scipy stack (specifically numpy, pandas, scikit-learn and matplotlib) as well as the python igraph implementation. \n \nSocial media can be used to perceive the relationships between individuals, companies and brands. Understanding  the relationships between key entities is of vital importance for decision support in a swathe of industries. We present a real-time method to query and visualise regions of networks that  could represent an industries, sports or political parties etc. ", "Moving notebooks into the cloud: challenges and lessons learned\n \nIn late 2016, we decided to make notebooks a core component of our data-science platform, giving each user the ability to run multiple notebooks concurrently in the cloud and share them.  To do this, we had to tackle the problem from both product and engineering perspectives.  Along the way, we learned about how notebooks fit into data-science work and how we could best leverage them to provide value to our users.\nWe present major findings from user research that we conducted, including user surveys, a design sprint, and analysis of Civis data scientists' notebooks.  For instance, we learned that in addition to providing an exploratory workspace, notebooks can be used as deliverables in two very different ways: to document a particular analysis, and to build reports or dashboards.  The former requires that running the notebook generate the same results every time.  The latter requires updated results every time.  Our users often get their data by querying a live system, so we built an always-on data store of past queries that can be used when analyses must be reproducible.  We also educated our users about various tools for building reports from notebooks.\nAnother significant engineering challenge is collecting the dependencies for a notebook.  Notebooks are typically used on local machines that accumulate state over time.  This is in contrast to cloud instances, which are dynamically provisioned.  Bundling dependencies with a notebook is a major value-add, as it allows dependencies to be changed and shared easily without affecting other notebooks.  We were able to take advantage of Docker on Kubernetes to manage dependencies and bring provisioning delays down to acceptable levels, though this remains a significant technical challenge.\nFinally, we present ways that notebook products could improve to better integrate with enterprise cloud platforms.  For example, better support for non-local filesystems or UI for observing the status of long-running operations (querying a remote data warehouse, for example) could help move cloud notebooks further into the enterprise.\n \nThe product and engineering teams of Civis Analytics integrated Jupyter notebooks into our cloud-based platform, providing the ability to run multiple notebooks concurrently and share them.  We'll discuss the technical challenges we encountered and how we solved them, and what we learned about notebook users and their user stories.", "Efficient and portable DataFrame storage with Apache Parquet\n \nSince its creation in 2013, Apache Parquet has risen to be the most widely used binary columnar storage format in the big data processing space. While supporting basic attributes of a columnar format like reading a subset of columns, it also leverages techniques to store the data efficiently while providing fast access. In addition the format is structured in such a fashion that when supplied to a query engine, Parquet provides indexing hints and statistics to quickly skip over chunks of irrelevant data.\nIn recent months, efficient implementations to load and store Parquet files in Python became available, bringing the efficiency of the format to Pandas DataFrames. While this provides a new option to store DataFrames, it especially allows us to share data between Pandas and a lot of other popular systems like Apache Spark or Apache Impala. In this talk we will show the improvements that Parquet bring performance-wise but also will highlight important aspects of the format that make it portable and efficient for queries on large amount of data. As not all features are yet available in Python, an overview of the upcoming Python-specific improvements and how the Parquet format will be extended in general is given at the end of the talk.\n \nApache Parquet is the most used columnar data format in the big data processing space and recently gained Pandas support. It leverages various techniques to store data in a CPU and I/O efficient way and provides capabilities to push-down queries to the I/O layer. In this talk, it is shown how to use it in Python, detail its structure and present the portable usage with other tools.", "Spying on my Network for a Day: Data Analysis for Networks.\n \nFor every button on the webpage that is clicked , for every picture uploaded; \nThere is a tremendous amount of data that goes over every network from users data via phones , tablets or even computers accessing a youtube or netflix Data center, to sending larger traffic between different Virtual Machines. But how does one actually analyze this volume of data sent. \nNetwork data analysis can go a long way in helping users find out the inefficiencies in their network. Understanding Networks makes for a very strong foundation on what everything else is built upon. \nIn this talk I will cover the basics of :\n\nAn IP Packet data analysis: the basic and most fundamental thing everything on a network builds upon. A packet can carry a lot of information and interesting details about behavioural habits of a user or even attributes of a device. Understanding the nature of the IP packet data leads to a larger understanding of Networks when analysed on a scale. And like the data rule of thumb the more data over a period of time you\u2019re able to learn from the better, and more accurate your intelligence become.\nEvaluating your local network using visualisation tools like IPython Notebook and Bokeh. \nGathering insights from the data to answer questions like How much data on an average do I generate on a daily basis? Why is my network slow ? Am I really getting what was promised from my Service Provider ?, what applications do I spend the most of my bandwidth on ? Why is my network experiencing down time? How can I automate the process of analysing my network data and much more.\n\n \nIn this talk I would show how I used open source tools like Moloch, Wireshark, Bokeh and Jupyter to analyse my home network data for the day. Not just the volume of data generated daily,  but how interesting it is to leverage data tools to discover when your network is experiencing downtime which could be as a result of packet loss, poorly placed Access points or just proximity away from your rout", "H2O PySparkling Water \n \n\n \nSparkling Water allows users to combine the fast, scalable machine learning algorithms of H2O with the capabilities of Spark. With Sparkling Water, users can drive computation from Python/R/Scala and utilize the H2O Flow UI, providing an ideal machine learning platform for application developers. PySparkling Water is an integration of Python with Sparkling water. It allows user to start H2O services on a spark cluster from Python API. The speed, quality and ease of use and model-deployment, for the various cutting edge Supervised and Unsupervised algorithms like Deeplearning, Tree Ensembles and GLRM, makes H2O a highly sought after API for big data data science.", "Visual diagnostics for more informed machine learning\n \nDetailed Abstract\nVisual diagnostics are a powerful but frequently underestimated tool in data science. By tapping into one of our most essential resources \u00e2\u0080\u0094 the human visual cortex \u00e2\u0080\u0094 they can enable us to see patterns rendered opaque by numeric outputs and tabular data, and lead us toward more robust programs and better data products. For Python programmers who dabble in machine learning, visual diagnostics can mean the difference between a model that crashes and burns, and one that predicts the future.\nPython and high level libraries like Scikit-Learn, NLTK, TensorFlow, PyBrain, Theano, and MLPY have made machine learning accessible to a broad programming community that might never have found it otherwise. With the democratization of these tools, there are now a great many machine learning practitioners who are primarily self-taught. At the same time, the stakes of machine learning have never been higher; predictive tools are driving decision-making in every sector, from business, art, and engineering to education, law, and defense. In an age where any Python programmer can harness the power of predictive analytics, how do we ensure our models are valid and robust? How can we identify problems such as local minima and overfit? How can we build intuition around model selection? How can we isolate and combine the most informative features? Whether you have an academic background in predictive methods or not, visual diagnostics are the key to augmenting the algorithms already implemented in Python.\nIn this talk, I present a suite of visualization tools within and outside the standard Scikit-learn library that Python programmers can use to evaluate their machine learning models' performance, stability, and predictive value. I then identify some of the key gaps in the current visual diagnostics arsenal, and propose some novel possibilities for the future.\nOUTLINE\n\n\nIntroduction/Problem statement\n\nMachine learning made accessible via the Scikit-learn API\nBut what kinds of things can go wrong?      \nAnscombe's quartet: An argument for using visual diagnostics\n\n\n\nVisual tools for feature analysis and selection\n\nThe model selection triple: What it is and how it can support the ML workflow\nEffective feature analysis is key to informed machine learning\nVisualizations can facilitate feature selection (boxplots/violinplots, histograms, sploms, radviz, parallel coordinates and more)\n\n\n\nDemystifying model selection\n\nTree diagrams and graph traversal for model selection: The Scikit-learn algorithm cheatsheet, Sayed's data mining map\nCluster and classifier comparison plots\nModel evaluation to support selection (Confusion matrices, ROC curves, prediction error plots, residual plots)  \n\n\n\nTaking hyperparameter tuning out of the black box\n\nVisualizing parameters with validation curves    \nDeveloping intuition through visual grid search\n\n\n\nOpen source Python packages for more informed machine learning\n\nYellowbrick: Aggregating the tools available in Scikit-Learn, Matplotlib, Seaborn, Pandas and Bokeh\nTrinket: Wrapping the entire machine learning workflow\n\n\n\n \nVisualization has a critical role to play throughout the analytic process. Where static outputs and tabular data can obscure patterns, human visual analysis can open up insights that lead to more robust data products. For Python programmers who dabble in machine learning, visual diagnostics are a must-have for effective feature analysis, model selection, and parameter tuning. ", "Machine learning techniques for data cleaning\n \n\nHow does messiness arise & why is it challenging?\nInferring structure in unstructured strings\nNLP parsers for names, organizations, addresses\nhow to make your own probabilistic string parser\nInferring relationships in datasets\nclustering similar rows in a dataset\nlinking similar rows across datasets\nclustering & linking without writing any code, using the dedupe.io interfacce\n\n \nOften, the most interesting datasets - data about people and organizations - are the messiest and most difficult to analyze. When data comes from multiple sources, or when data is entered manually, variation & ambiguity are inevitable. Learn about ways to infer structure and relationships in messy data, using open source Python libraries.", "Why is Windows so broken? (and what to do about it)\n \nTBD\n \nPython on Windows has a reputation for being, well, pretty broken. But the good news is that things have been getting better. Join Microsoft employee and CPython core developer Steve Dower as he walks through the recent improvements to the Python ecosystem on Windows, and how things will get even better in the future.", "Zero-Administration Data Pipelines using AWS Simple Workflow\n \nThere are quite a few great tools for building effective and robust distributed data processing pipelines, especially Luigi from Spotify and Airflow from AirBnB.\nFor scaling out, they all require a queue or master server, though. And those need maintenance.\nWe wrote floto (github.com/babbel/floto), an open source tool to programmatically author, schedule and run scalable data pipelines on AWS - without the maintenance overhead.\nIt uses AWS Simple Workflow, but I'll talk most about some general topics regarding data workflow orchestration:\n\nseparation of concerns\nmanaging complexity through dependency reduction\nidempotent (or re-runnable) jobs\ntransactional jobs (either completely fail, or completely succeed)\nfailures and reruns\nevolving changes\norganizational scaling\nheterogenous systems\n\n \nFloto is an open source tool to programmatically author, schedule and run scalable data pipelines using AWS Simple Workflow - without the need to maintain a master server or queue or the state of workers.", "Cat modelling with python\n \nA brief history of cat modelling\nA personal journey modelling natural catastrophes following\ndevelopments in the python community.\nUsing python in this environment was only possible due to some magical\ntools created within the community.\nReinsurance and natural catastrophes\nA quick introduction to the subject.\nEvent driven development\nThe insurance industry is full of inertia.  The main driver of change\nare the large events that impact the industry:\n\nhurricanes\nfloods\nearthquakes\nterror\n\nWhenever there is an event, such as an earthquake, hurrricane or\nterror the industry gains new perspective.\nEvents have driven new developments in models and technology.\nThe year of the four hurricanes.\n2004: Charley, Frances, Jeanne and Ivan.\nFour hurricanes in one year hitting Florida.\nPlotting for fun and profit\nMatplotlib and the tale of the stacked bar chart.\nData, data everywhere.\nNumpy, pytables, pandas, Scipy, scikit-learn.\nEarthquakes in Christchurch\nThree earthquakes in quick succession in the same place.\nBlack box models\nNo separation of hazard, vulnerability and financial models.\nThe present\nPython helped detect gravitational waves from a billion years away\nbut is it ready for the enterprise?\nCyber\nCybernetics:\n\nthe science of communications and automatic control systems in both\n  machines and living things.\n\nSo cyber is not so much about technology as feedback loops and\ncontrol. \nCyber risk is receiving a lot of attention in the insurance industry.\nInfomation insecurity is resulting in leakage of information, often\nwith significant financial impact to the organisations affected.\nCyber has the potential to be a systematic driver of losses across\nmany lines of business.\nIt is a changing peril, with some similaritie to terror: it is\nprecisely the attacks we have not considered that succeed.\nOasis\nOpen loss model framework: https://www.oasislmf.org/\nSeparation of hazard, vulnerability and financial models.\nSupport for niche models, allowing new science to be integrated with\nmodels in a more timely manner.\nUses outside of the insurance industry.  Strategic planning and\nemergency management.  Understanding implications of climate change.\nThe future\nClimate change.\nData analysis challenges.\n\nfinding the data\nnew tools, full time job keeping up\n\nExpert teams:\n\na team working well together can be many times more effective than\nany individual in that team.  \n\nThe world is full of specialists with incredible skills and\nknowledge.  When specialists from different disciplines come together\nthe creative sparks fly.\n \nA brief history of use of python within the Bermuda based reinsurance industry.\nA tale of matplotlib, reportlab, numpy, ipython, jupyter and more.\nCurrent challenges and opportunities and an oasis on the horizon.", "Chatbots - Past, Present and Future\n \nIn the past few months, chatbots have made headlines and have become a much-required interface for communication. In this talk, we will discuss three aspects of chatbots: \n1) How chatbots have evolved from Natural Language Processing and Artificial Intelligence\n     In this segment we will answer the following questions: \n      -- What research-areas have led to the evolution of chatbots? \n      -- How important is it for a chatbot to appear human, and to pass the Turing test?\n2) Internals and Technologies involved in Chatbots\n      In this segment we will answer the following questions: \n      -- What are the tools and technologies do you need to know about in order to make computers understand human language? \n      -- Which python libraries should one be familiar with to build Language Understanding interfaces?\n3) How to build chatbots yourself and the questions to ask yourself before building one\n      In this segment we will answer the following questions: \n      -- Which API's are available for building chatbots\n      -- Pro/Con's for using API's vs building chatbots from scratch\n      -- Where can one host their chatbots?\nFinally we will discuss where chatbots are heading to from here\n \nIn the past few months, chatbots have made headlines and have become a much-required interface for communication. In this talk, we will discuss three aspects of chatbots: \n1) How chatbots have evolved from Natural Language Processing and Artificial Intelligence\n2) Internals and Technologies involved\n3) How to build chatbots yourself and the questions to ask yourself before building one", "Is having dementia linked to where you live?\n \nOverview\nAlzheimer\u2019s is a neurodegenerative disease that currently affects over 55 million people worldwide and this is set to increase with trends in population growth and demographics.  Meanwhile, particulate emission levels in large cities across Europe and the world have been under scrutiny, as diesel emissions related to a huge uptick in diesel vehicle purchases (due to governments' push to reduce CO2 emission levels) are found to have caused many medical problems in the area of breathing difficulties.\nBreathing function and the lungs may not be the only parts affected; there is now more than one scientific study (see link [1] for an example below) that has found a link between higher incidence of dementia (in particular Alzheimer\u2019s) in those who have lived in highly polluted urban areas for large periods of their lives. \nThis talk is about a short, collaborative \u2018hack\u2019 project with a focus on using Python machine learning tools and open data to test this hypothesis and look for evidence either confirming or denying the link between increased diesel engine and tyre particulate emissions (PM10, PM2.5 and NO2) in a given residential area with a greater likelihood of diagnosis of dementia amongst the residents of that area. \nThe idea behind this endeavour is that this \u2018pilot\u2019 study might enable a funded venture to take root that, for example, increases awareness of the true impact of high diesel car pollution in dense conurbations.\nQuestions asked\n\nCan we visualize the likelihood of incidence of dementia per person on a heat map for a city or for the UK? And similarly visualize annual particulate exposure per resident? Do they look similar?\nDo various sources of data, when placed under the magnifying glass of data science corroborate the evidence gathered by recent studies?\nCan a classifier reasonably determine the likelihood of your contracting dementia based on where you have lived mostly during your life?\nWhere should you live and work that gives you a lower chance of contracting Alzheimer\u2019s later in life?\nHow can the findings be used to push city officials to improve city air quality? How can we raise awareness?\n\nChallenges encountered on the way\n\nHow to interpolate, in order to cover 'gaps' in-between emission monitoring stations, incorporating additional data sources (for example road network structure and traffic intensity levels)?\nSensitive medical data; how to merge open medical datasets securely without revealing personal information?\nWhich types of machine learning models are best suited to this problem?  \n\nApproach\nStarting at a high level, it is possible to determine if there is a higher incidence of dementia cases linked with an increase in airborne pollutants (diesel source):\n\nAt a country level (e.g. for the Netherlands or the UK)\nAt a city level (e.g. for Bristol, Eindhoven and / or London)\nAt a borough level (e.g. for Camden, London)\nAt a street level (by postcode)\nAt individual sufferer level (with de-personalized datasets, based on 1,000 blog posters)\n\nOther research inputs (articles and papers)\n[1] https://www.sciencemag.org/news/2017/01/brain-pollution-evidence-builds-dirty-air-causes-alzheimer-s-dementia\n \nIn this talk we will explore the results from a short, collaborative \u2018hack\u2019 project with a focus on using Python machine learning tools and open data to find evidence linking increased diesel engine and tyre particulate emissions (PM10, PM2.5 and NO2) in certain residential areas with greater likelihood of diagnosis of dementia amongst the residents of that area.", "Analysing user comments on news articels with Doc2Vec and Machine Learning classification\n \nDoc2Vec is a nice neural network framework for text analysis.  The machine learning technique computes so called document and word embeddings, i.e. vector representations of documents and words. These representations can be used to uncover semantic relations.  For instance, Doc2Vec may learn that the word \"King\" is similar to \"Queen\" but less so to \"Database\".\nI used the Doc2Vec framework to analyze user comments on German online news articles and uncovered some interesting relations among the data. Furthermore, I fed the resulting Doc2Vec document embeddings as inputs to a supervised machine learning classifier. Accordingly, given a particular comment, can we determine from which news site it originated?  Are there patterns among user comments? Can we identify stereotypical comments for different news sites?\nBesides presenting the results of my experiments, I will give a short introduction to Doc2Vec.\n \nI used the Doc2Vec framework to analyze user comments on German online news articles and uncovered some interesting relations among the data. Furthermore, I fed the resulting Doc2Vec document embeddings as inputs to a supervised machine learning classifier. Can we determine for a particular user comment from which news site it originated?", "Building Continuous Learning Systems\n \nProblem Statment\nIn many Machine Learning (ML) problems, there is an implicit assumption - training and test data come from the same stationary distribution. Any ML model developed and thereafter deployed in a production environment might make mistakes either because (1) It did not learn the concept correctly, or (2) the data distribution is non-stationary (concept drift; today more data is organized in the form of data streams rather than static databases, and it is unrealistic to expect that data distributions stay stationary over a long period of time).\nWon't it be great to have models that can learn \u00e2\u0080\u009ccontinuously\u00e2\u0080\u009d from their mistakes (feedback loop) and adapt to an evolving data distribution? While there is a plethora of literature on building models with higher accuracies on test data, this isnt the case when it comes to building ML systems that learn from their mistakes and adapt on the fly. Such system are often called \u00e2\u0080\u009cLearning Machines\u00e2\u0080\u009d. Currently, the standard way to handle feedback and drift is: monitor the performance of the model; If the performance goes below acceptable levels, replace the current model in production with a newly trainined model on the collected feedback & recent data.\nIn this talk we describe our efforts on building ML system that can learn continuously and can adapt on-the-fly to an evolving data distribution. The focus of this talk is on the last leg of the loop, as highlighted in the figure below:\n\nApplied domains\n\nIntroduce non-stationary distributions with examples (we'll observe the results of misclassification rate on our data over a long period of time.) \nIdentify shift in data distribution & labels (For example, we will see how customer preferences (labels) changes over time and how the fallacy of believing that our training sets represent the population, when in reality it is usally a random sample of the current population.)\nIdenitify noise in data\nTemporal nature of data\nRelevant domains that can be addressed with similar learning techniques: Social media, Monitoring and Anamoly detection, Predictions and recommendations\n\nTowards Solution:\nModeling the problem:\nThe problem of instantaneous incorporation of feedback can be modeled as Online Learning: Model sees a data point (x). Makes a prediction y'. The environment reveals the true label y. Model is correct if (y = y') and incorrect if (y != y'). In case the model is wrong, the data point (x,y) is sent to the model as \u00e2\u0080\u009cfeedback\u00e2\u0080\u009d.\nIdeally you would want the model:\n\nTo incorporate this in its learnings i.e. model updates its parameters/weights such that if this data point (x,y) is (ever) again presented to the model, model will output correct label y\nDoes not impact the accuracy of the model on other data points i.e. if prior to incorporating this feedback for all the points where model was predicting correctly, model must continue to do so after the incorporation. In short, in trying to acquire new learning, your model should not forget old (correct) learnings.\n\nDivide and Conquer\nWe borrow the ideas from algorithms (Perceptron, Crammer's PA-II variants) suitable for online settings to build fast learning per-user statistical model with dearth of data leading to temporal retention - we call this Local model. In addition, we perform online passive aggressive updates to combat high degree of noise in the training set. Each message is used to update the model. We use the aggressive & passive parameters to tweak the learning rates on the local model.\nFurther, we use Deep Neural Nets to build model where a glut of data is available leading to persistent retention - we call this Global model.\nWe use cassandra to store-serve models which acts as the distributed file system with a database.\nWe build on the work of Drift Detection (Gama et.al. 2004, Garc\u00c2\u00b4ia et.al. 2006 and Minku et.al. 2012) to refine the training window.\nEnsemble Manager\nWhile both cost-sensitive learning and online learning have been studied extensively, the effort in simultaneously dealing with these two issues is limited. Our key idea is based on the fusion of online ensemble algorithms and the state of the art batch mode cost-sensitive bagging/boosting algorithms. Within this framework, two separately developed research areas - cost-sensitive learning and online learning algorithms are bridged together.\nSummary\nWhile building models with better accuracies is critical (and thats what Machine Learning addresses); from the view point of a practitioner in industry its important to realise that Machine Learning is only a part of End-to-End cycle of building ML systems. An important piece of this endeavour is to build machines that can learn continuously - i.e. to learn from mistakes and adapt to evolving data. In this talk we elaborate our attempts and results towards building such machines. To the best of our knowledge though such systems have been built (google, fb etc), there isn't much literature/resources around them. We hope our talk will help and inspire the community to make serious attempts to bring better science to this art.\n \nIn this talk we explore how to build Machine Learning Systems that can that can learn \"continuously\" from their mistakes (feedback loop) and adapt to an evolving data distribution. ", "You Belong with Me: Scraping Taylor Swift Lyrics with Python and Celery\n \nCelery is an open source, Python-based, asynchronous task framework which is well-suited for extracting data from webpages, APIs, and text files. With its power comes a bit of a learning curve. This talk covers some of the first questions a new Celery user might have. It will also point out some common pitfalls for beginning users.\nOutline:\n- What is Celery?\n    * Project Goals\n    * Project History\n    * Major Features\n- Initial Setup\n    * Message broker\n    * Result storage\n    * First tasks\n- Running Celery\n- Webscraping Example\n- Common Pitfalls\n\n \nThis talk will demonstrate an example application of using Celery to extract all of the lyrics of the inspiring and influential Taylor Swift from the Internet. Using a light-hearted approach and practical Python examples, we aim to teach people the basics of using Celery for data extraction.", "A hybrid approach to model randomness and fuzziness using Dempster-Shaffer method.\n \nCurrent main stream data science work is mainly focused on prediction , segmentation  and data analysis.  This mainly involves supervised learning where we learn from historic data. The predicted observations usually gives a probability score which measures randomness. We believe there are situations where along with randomness we need some fuzziness to give some confidence to the observation. \nFuzziness and randomness are two very different concepts. We say that fuzziness occurs when there is no boundary between outcomes. And we commonly refer to randomness as the uncertainty associated with effective variability from alternative outputs. \nAlthough different, it is simple to provide real world scenarios where randomness and fuzziness work together. One example is in the stock market where there are no historical similarities to use in modeling such as new high patterns. Here, we say that there are no defined boundaries that forecast event probabilities in the next few days. Another example is in the emergency room where the early detection algorithm gives a risk score, but the environmental factors impacting the patient are not predictable. In both of these examples, we can fairly easily model the randomness, but there is a fuzziness that needs to be dealt with also. We will discuss a model we are researching that improves our predictability of that fuzziness and randomness.\n    The heart of this improvement lies in how we approach variability. Variability of alternative outputs can describe randomness and is easily modeled using regression or other predictive analytic methods. Using fuzziness, a second source of variability can be individualized (per observation) in the vagueness with which the attributes are selected. This type of uncertainty is due to variety of environmental factors depending on the use case. For the stock market, uncertainty is due to a set of economic conditions and policy reforms that have not been looked at before, while for an emergency room scenario it might be due to variety of scenarios before the patient was hospitalized.\n    Research recommends several different approaches to simulate the choice behavior in different choice contexts. Our focus is on designing choice models that can take into account one of two sources of uncertainty.\n    In this talk we will discuss the following:\n\nMotivation of combining fuzziness and predictability.\nMath behind Dempster Shaffer method.\nOur Analysis combining Logistic Regression with Dempster Shaffer.\nReview of our results and interpretation.\n\n \nCurrent main stream data science work is mainly focused on prediction , segmentation  and data analysis.  This mainly involves supervised learning where we learn from historic data. The predicted observations usually gives a probability score which measures randomness. We believe there are situations where along with randomness we need some fuzziness to give some confidence to the observation. ", "Predicting political views from text\n \nEvery day media generate large amounts of text. Getting an unbiased view on what media report on requires an unbiased sample of media content. In many cases it is obvious which political bias an author has. In other cases some expertise is required to judge the political bias of a text. Assistive technology for estimating the political bias of texts can be helpful in this context, especially for scaling things up. We investigated to what extent political party affiliation can be predicted from textual content with basic machine learning tools. We used the text of speeches and discussions in the German parliament as well as texts from party manifestos to train classifiers that predict political party affiliation or political views based on standard text features. Results indicate that automatic classification of political affiliation and political views is possible with well above chance accuracy. We hope that this work will eventually be helpful for unbiased political education in the presence of massive amounts of media content. We show some web applications of how the models can be used in combination with classical topic models to analyse texts for which the party affiliation is not clear, such as news articles. \n \nAn unbiased view on media reports requires understanding the political bias of a text. This talk shows how basic tools from machine learning and natural language processing combined with publicly available data can be turned into assistive technology for automatically determining the political bias of a text. Some common pitfalls and example applications will be discussed.", "Mining smartphone sensor data with python\n \nOur smartphones are increasingly being built with sensors, that can measure everything from where we are (GPS, Wi-Fi) to how we move (accelerometers) and other aspects of our environments (e.g., temperature, humidity). Many apps are now being designed to collect and leverage this data, in order to provide interesting context-aware services and quantify our daily routines.\nIn this talk, I'll give an overview of collecting sensor data from an Android app and processing the data with Python. I'll focus on accelerometers - a triaxial sensor that measures the device's motion - which is now being used in apps that detect what you are doing (cycling, running, riding a train); if we have enough time I'll also briefly cover a similar example with Wi-Fi/location data. Using an open-sourced Android app and iPython notebook, I'll discuss the following questions:\n\nWhat does the raw data look like? There are a number of trade-offs when collecting sensor data: most notably, data collection needs to be balanced against battery consumption. Plotting the raw data gives a view of how the data was sampled and how it changes across activities.\nHow can I pre-process and extract features from this data? Three kinds of features can be extracted from acceleromter data: statistical, time-series, and signal-based. Most of these are readily available in well-known Python libraries (scipy, numpy, statsmodels).\nHow can these features be used to analyse behaviours? I'll show an example of using accelerometer data to cluster users into groups, based on how active they are.\nHow can these features be used to detect behaviours? I'll show an example of training a supervised learning algorithm (using scikit-learn) to detect walking vs. running vs. standing.\n\nI'll close by discussing how these techniques are being applied in novel smartphone apps for health monitoring.\n \nData from smartphone sensors can be used to learn from and analyse our daily behaviours. In this talk, I'll discuss processing and learning from sensor data with Python. I'll focus on accelerometers - a triaxial sensor that measures motion - starting with an overview pre-processing the data and ending with supervised and unsupervised learning applications and visualisations.", "Of Mice & Python: Building a Brain Observatory for Visual Behavior\n \nThe Allen Brain Observatory, the newest publicly available resource from the Allen Institute for Brain Science, presents the first standardized and freely downloadable survey of neuronal activity in the mouse visual cortex, featuring representations of the visual world among 27,154 neurons at the latest release. We are developing the second generation of this dataset, in which we will release neuronal activity across multiple cell types and brain regions while mice are making decisions based on the images they see. In this talk, we provide an overview of our Python-based infrastructure built to integrate automated training of mice into our existing open neuroscience data generation pipeline. This infrastructure includes:\n\nAutomated training algorithms built on PsychoPy & PyDAQmx\nPyQT & Flask-based GUIs that allow technicians to manage training sessions & maintain mouse health records\nReal time monitoring & web-based data visualization with VEGA & pyzmq\nPosthoc analysis and visualization with pandas, matplotlib, & seaborn \nAutomated progression through training stages using the transitions state machine framework\n\n \nWe are developing the next generation of the Allen Brain Observatory, in which we will release neuronal activity across multiple cell types and brain regions while mice are making decisions based on the images they see. In this talk, we provide an overview of our Python-based infrastructure built to integrate automated training of mice into our existing open neuroscience data generation pipeline.", "Extracting Insight From A Muslim Marriage App\n \nDating apps have become the norm in modern society, with companies like Tinder, Bumble, Happn etc. boasting millions of users. OKCupid, an early entry into the online world of dating, were one of the leading companies to take advantage of the data at their fingertips and conducted several interesting experiments and subsequent data analysis which were published on their blog and subsequently in the book 'Cataclysm' by their lead data scientist Christian Rudder. \nI was curious to see how people interact when searching for love online when they are part of a (1) organized religion and (2) members of a minority. I teamed up with Shahzad Younus, founder of the muslim marriage app 'MuzMatch' to gather highly anonymized data about their users and extract insight into how people behave and what is important to them. For example, as an app targeted at members of a specific religious community, does levels of religiosity really matter? Does religion trump ethnicity? \nI hope to answer these questions and more.\n \nI use various Python libraries and methods from Graph theory and Information Retrieval to garner insight into the social dynamics of people searching for love and marriage on the Muslim marriage app 'MuzMatch'.", "AI assisted creativity\n \nA new wave of creative applications of AI has arrived, making science fiction authors struggle to keep up with reality. Recent advances in Deep Learning, especially generative models, make it possible to generate text, audio, speech, and images. There's a wonderfully trippy world of neural nets \"going wild\" out there, with AI choreographed dancing moves, freestyle raps, impressionist paintings, and Trump impersonating bots.\nSuch \"bots\" and experiments are but one novel use of this kind of \"Creative AI\". Taking a more human-centered approach, allowing for control and agency, has the potential to turn these content-generating neural nets, into tools for creative use and explorations of human-machine interaction, where the main theorem is \"augmentation, not automation\".\nThe talk will particularly focus on \"generative\" models, and show the python fanatic how to make your move with these particular forms of Deep Neural Nets.\n \nA new wave of creative applications of AI has arrived, making science fiction authors struggle to keep up with reality. Recent advances in Deep Learning, especially generative models, make it possible to generate text, audio, speech, and images. There's a wonderfully trippy world of neural nets \"going wild\" out there, which you, the python enthusiastic, can be part of...", "Predicting Usage for Capital Bikeshare stations based upon Spatial Characteristics\n \n\nWe start ground up, by collecting and cleaning data from multiple data sources (Capital Bikeshare, Openstreetmap).\nWe then proceed towards developing a Linear regression model to predict the usage based upon spatial attributes of the stations, and critique its performance. \nUsing Biogeme, we then develop Logistic Models to estimate the same and compare their performance to its regression counter-parts. \n\nThe aim of this talk is to provide a primer to Logistic models and explore the effects of Spatial Characteristics on a Utility usage.\n \nA step after trying to model Continuous variables from Regression models, lies a very rewarding problem of estimating decisions which more lie in the discrete domain. In this talk, we work towards developing Logistic models to predict traffic for Capital Bikeshare, and work towards finding optimal station locations for Network expansion.", "Julia for Modern Data Analysis\n \nJulia is a dynamic, high-performance language for technical computing.  The Julia package ecosystem has a growing number of tools for data analysis, graphics, statistics, and machine learning.  This talk will cover the areas where Julia excels, where it needs improvement, and how it can be used alongside existing tools in Python and R.\n \nJulia is a dynamic, high-performance language for technical computing.  This talk will describe where the newcomer Julia may fit into a data science ecosystem already full of rich libraries in R and Python.  ", "Keynote: Working Efficiently with Big Data in Text Formats\n \nIn an ideal world, all our large datasets would live in well optimized storage formats, such as RDBMS's, key-value NoSQL stores, HDF5 hierarchical datasets, or other formats that are well typed and fast to access.  In our actual world, a great deal of our data lives in CSV, flat-file, or JSON formats, roughly stored on file systems, with little typing of data values.  Moreover, data in these formats often have variably sized records making seeking data a linear scan operation.\nContinuum Analytics has produced a custom optimized library called IOPro that includes a component called TextAdapter.  TextAdapter provides abstractions to data access into these textual formats that adds much better data typing, minimizes memory use, uses indexing for seeking, and other facilities for better, faster data access without requiring conversion of exploratory datasets into permanent optimized formats.  We will be releasing this code as an Open Source project, and plan on enhancing the library to allow further performance optimizations and integration with the Dask project.\nAs well as looking at technical and performance details of TextAdapter, this talk will discuss the economic and social concerns of company developed and supported Open Source projects. Continuum continues to explore some of these issues through our release of TextAdapter, following on company trajectory of moving projects from proprietary to open source status whenever reasonable.\n \n", "Data Integration in the World of Microservices\n \nSince its launch in 2008, Zalando has grown with tremendous speed. The road from startup to multinational corporation has been full of challenges, especially for Zalando's technology team. Distributed across Berlin, Helsinki, Dublin, Hamburg and Dortmund \u00e2\u0080\u0094 with nearly 1000 professionals strong \u00e2\u0080\u0094 Zalando Technology still plans to expand by adding 1,000 more developers through the end of 2016. \nThis rapid growth has shown us that we need to remain flexible about developing processes and organizational structures, to allow us to continue scaling and experimenting. In March 2015, our team adopted Radical Agility: a tech management approach that emphasizes Autonomy, Purpose, and Mastery, with trust as the glue holding it all together. \nTo make autonomy possible, teams can now choose their own technology stacks for the products they own. Microservices, speaking with each other using RESTful APIs, promise to minimize the costs of integration between autonomous teams. In addition, Isolated AWS accounts run on top of our own open-source Platform as a Service (called STUPS.io), give each autonomous team enough hardware to experiment and introduce new features without breaking our entire system.\nOne small issue with having microservices isolated in their individual AWS accounts: Our teams keep local data for themselves. In this environment, building an ETL process for data analyses, or integrating data from different services becomes quite challenging. \nPostgreSQL's new logical replication features, however, now make it possible to stream all the data changes from the isolated databases to the data integration system so that it can collect this data, represent it in different forms, and prepare it for analysis. \nIn this talk, I will discuss Zalando's open-source data collection prototype, which uses PostgreSQL's logical replication streaming capabilities to collect data from various PostgreSQL databases, and recreate it for different formats and systems (Data Lake, Operational Data Store, KPI calculation systems, automatic process monitoring). The audience will come away with new ideas for how to use Postgres streaming replication in a microservices environment.\n \nTO BE ADDED", "Batch and Streaming Processing in the World of Data Engineering and Data Science\n \nThe large-scale adoption of \u201cBig Data\u201d has created a multitude of exciting new job roles and technologies. In line with this, data scientists and data engineers have both become key members of many technology teams, a coexistence which has often motivated the debate: streaming or batch? In this talk, we discuss the pros & cons of batch vs. streaming processing, especially with respect to finding common ground between the data engineer and data scientist\u2019s workflows. We address the types of cases that are applicable to batch processing, streaming processing, or both. Finally, we present a demo using PySpark Logistic Regression for offline or online decisioning and model updating,  to illustrate different ways to utilize batch and/or streaming processing to apply machine learning to real-time data.\n \nStreaming or batch is an ongoing debate with the large-scale adoption of \u201cBig Data\u201d. In this talk, we discuss the pros & cons of batch vs. streaming processing, especially with respect to the workflow of data engineers and data scientists. We also present a a demo using PySpark Logistic Regression for offline and online model training to illustrate the difference between the two.", "Bayesian Deep Learning with Edward (and a trick using Dropout)\n \nDeep learning methods represent the state-of-the-art for many applications such as speech recognition, computer vision and natural language processing.  Conventional approaches generate point estimates of deep neural network weights and hence make predictions that can be overconfident since they do not account well for uncertainty in model parameters.  However, having some means of quantifying the uncertainty of our predictions is often a critical requirement in fields such as medicine, engineering and finance.  One natural response is to consider Bayesian methods, which offer a principled way of estimating predictive uncertainty while also showing robustness to overfitting.  \nBayesian neural networks have a long history.  Exact Bayesian inference on network weights is generally intractable and much work in the 1990s focused on variational and Monte Carlo based approximations [1-3].  However, these suffered from a lack of scalability for modern applications.  Recently the field has seen a resurgence of interest, with the aim of constructing practical, scalable techniques for approximate Bayesian inference on more complex models, deep architectures and larger data sets [4-10].\nEdward is a new, Turing-complete probabilistic programming language built on Python [11].  Probabilistic programming frameworks typically face a trade-off between the range of models that can be expressed and the efficiency of inference engines.  Edward can leverage graph frameworks such as TensorFlow to enable fast distributed training, parallelism, vectorisation, and GPU support, while also allowing composition of both models and inference methods for a greater degree of flexibility. \nIn this talk I will give a brief overview of developments in Bayesian deep learning and demonstrate results of Bayesian inference on deep architectures implemented in Edward for a range of publicly available data sets.  Dropout is an empirical technique which has been very successfully applied to reduce overfitting in deep learning models [12].  Recent work by Gal and Ghahramani [13] has demonstrated a surprising formal equivalence between dropout and approximate Bayesian inference in neural networks.  I will compare some results of inference via the machinery of Edward with model averaging over neural nets with dropout training. \n[1] D JC MacKay. A practical Bayesian framework for backpropagation networks. Neural computation, 4(3): 448\u2013472, 1992.\n[2] Neal, R M. Bayesian learning for neural networks. PhD thesis, University of Toronto, 1995.\n[3] Hinton, G E and Van Camp, D. Keeping the neural networks simple by minimizing the description length of the weights. In Proceedings of the sixth annual conference on Computational learning theory, 1993.\n[4] A Graves, Practical variational inference for neural networks. NIPS, 2011.\n[5] D P. Kingma, T Salimans, M Welling, Variational Dropout and the Local Reparameterization Trick https://arxiv.org/pdf/1506.02557 (2015)\n[6] A Mnih, K Gregor, Neural Variational Inference and Learning in Belief Networks, ICML, 2014 \n[7] D P. Kingma, M Welling, Auto-Encoding Variational Bayes. CoRR abs/1312.6114 (2013)\n[8] D Rezende, S Mohamed, and D Wierstra. Stochastic backpropagation and approximate inference in deep generative models. ICML, 2014.\n[9] Blundell, C, Cornebise, J, Kavukcuoglu, K, and Wierstra, D, Weight uncertainty in neural networks. ICML, 2015.\n[10] Hernandez-Lobato, J M and Adams, R P. Probabilistic backpropagation for scalable learning of Bayesian neural networks. ICML, 2015\n[11] D Tran, A Kucukelbir, A B Dieng, M Rudolph, D Liang, and D M Blei, Edward: A library for probabilistic modeling, inference, and criticism. arXiv:1610.09787, 2016\n[12] Srivastava, N, Hinton, G, Krizhevsky, A, Sutskever, I, and Salakhutdinov, R. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1), 2014.\n[13] Gal, Y and Ghahramani, Z, Dropout as a Bayesian approximation: Representing model uncertainty in deep learning.  ICML, 2016\n \nBayesian neural networks have seen a resurgence of interest as a way of generating model uncertainty estimates. I use Edward, a new probabilistic programming framework extending Python and TensorFlow, for inference on deep neural nets for several benchmark data sets.  This is compared with dropout training, which has recently been shown to be formally equivalent to approximate Bayesian inference.", "When the grassroots grow stronger - 2017 through the eyes of German open data activists\n \nOpen Data is the availability of (administrative) data to the public while there are no restrictions in analyzing or editing the data. The current stage of data availability depends largely on the the German states. In 2017, Open Data will regain importance and attention as the German Open Data Law has been passed in the German parliament in spring. Recently, Germany has also joint the Open Government Partnership. As many Open Data activists have been demanding better access to data in the past, how satisfactory are those changes on the government side? Those changes will be briefly discussed regarding their mean for our society and for those who have made use of Open Data already. \nIn the second part, I will explain how Open Data can be quite awesome already and what obstacles we have been dealing with in the past by showing 3 different, exciting and very active projects: Measuring air quality (Luftdaten), making election data more accessible (Wahlsalons) and collaboratively collecting and sharing data in a huge database (Wikidata). Open Data does not only need coders. It needs those that lobby for it, those that manage the projects, those that spread the word and most importantly: those who use the applications built upon Open Data. Will it be you, too?\n \nThe talk will give an overview about  the current state of Open Data in Germany including recent legislative changes that might lead to more Open Data. Leaving behind the basics, the talk will show the implications to the existing Open Data community. Three very different examples of current projects from the Code for Germany community will be presented showing you the bright side of Open Data.", "Variational Inference in Python\n \nBayesian inference has proven to be a valuable approach to many machine learning problems.  Unfortunately, many interesting Bayesian models do not have closed-form posterior distributions.  Simulation via the family Markov chain Monte Carlo (MCMC) algorithms is the most popular method of approximating the posterior distribution for these analytically intractible models.  While these algorithms (appropriately used) are guaranteed to converge to the posterior given sufficient time, they are often difficult to scale to large data sets and hard to parallelize.  Variational inference is an alternative approach that approximates intractible posteriors through optimization instead of simulation.  By restricting the class of approximating distributions, variational inference allows control of the tradeoff between computational complexity and accuracy of the approximation.  Variational inference can also take advantage of stochastic and parallel optimization algorithms to scale to large data sets.  One drawback of variational inference is that in its most basic form, it can require a lot of model-specific manual calculations.  Recent mathematical advances in black box variational inference (BBVI) and automatic differentiation variational inference (ADVI) along with advances in open source computational frameworks such as Theano and TensorFlow have made variational inference more accessible to non-specialists.  This talk will begin with an introduction to variational inference, BBVI, and ADVI, then illustrate some of the software packages (PyMC3 and Edward) that make these variational inference algorithms available to Python developers.\n \nInference in Bayesian models often requires simulation to approximate posterior distributions. Variational inference (VI) instead approximates posteriors through optimization.  Recent theoretical and computational advances in automatic variational inference have made VI more accessible.  This talk will give an introduction to VI and show software packages for performing VI in Python.", "Principles of Reporting Systems: It's About Time we Talked About Bitemporality\n \n\n \n", "Creating Knowledgebases from unstructured text.\n \nA major part of Big Data collected in most industries is in the form of unstructured text. Some examples are log files in IT sector, analysts reports in the finance sector, patents, laboratory notes and papers, etc. Some of the challenges of gaining insights from unstructred text is converting it into structured information and generating training sets for machine learning. Typically training sets for supervised learning are generated through the process of human annotation. In case of text this involves reading several thousands to million lines of texts by subject matter experts. This is very expensive and may not always be available, hence it is important to solve the problem of generating training sets before attempting to build machine learning models. Our approach is to combine rule based techniques with small amounts of SME time to by pass time consuming manual creation of training data. Once we have a good set of rules mimicking the training data we will use them to create knowledgebases out of the structured data. This knowledgebase can be further queried to gain insight on the domain. I have applied this technique to several domains, such as data from drug labels and medical journals, log data generated through customer interaction, generation of market research reports, etc. I will talk about the results in some of these domains and the advantage of using this approach.\n \nNLP and Machine Learning without training data.", "Making Recommendations without Data\n \nIntroduction\nThe problem we were trying to solve:\n- 6tribes: interest based social networking\n- Matching of user to users\n- Matching user to user groupings\nMaking recommendations is easy.\n- People that did X also did Y\n- We didn\u00e2\u0080\u0099t have any people!\nMaking good recommendations is hard.\n- User expectations are high! \n- Getting lost in data exploration\n- Ensuring fresh recommendations\n- Scaling recommendation engine\nMaking recommendations without data is impossible. Or is it?\n\nEngineering\n- Integrating and delivering continuously with CircleCI and OpBeat\n- Using data from Facebook and iPhone music and photo library\n- Enriching our data using external API\u00e2\u0080\u0099s like: HereAPI, FourSquare, Prismatic, Alchemy, iTunes\n- Using Flask to design a Rest API: Integration of data science and scala backend - increase development turn around time\n- Using Elasticsearch to deliver recommendations at scale\n- Using AWS for our data pipeline\nUser feedback\n- Internal testing of recommendations - being careful not to overfit (Anthony\u00e2\u0080\u0099s complaints and use of music)\n- Ensuring we act on feedback from our test users\nIssues we had\n- Tech debt and lifecycle\n- Facebook signin: matured and out-dated data: not really representative of the user: show example and restrictions of data\n- No users!\n- Cost of getting data - API usage, specific data sets needed -> limited funding\nKey Takeaways\n- How to enrich data\n- Building fast\n- Getting feedback\n- Ensuring scalability\n- Understanding of complexities of recommendation systems\n \nAs Data Scientists, we often assume we have data! It\u00e2\u0080\u0099s crazy not to. What should you recommend to a new user when you know nothing about them? In this talk we will discuss the challenges we faced, the assumption we took and the solutions we came up with while building a recommendations system for an interest based social network with limited data. ", "Forecasting Time Series Data at scale with the TICK stack\n \nForecasting time series data can require a significant amount of attention in order to get reliable results.\nAs the number and variety of time series increases it becomes too expensive to manage each forecast individually.\nUsing the TICK stack we demonstrate a workflow that helps to reduce the amount of attention each forecast needs.\nThis is accomplished by using the procedure called Prophet, which was recently open sourced by Facebook.\nThe basic idea of this procedure is two fold:\n\nReduce the amount of effort to train and maintain a single forecast.\nAutomatically surface the forecasts that are performing poorly.\n\nTo reduce the amount of effort per forecast, the Facebook Prophet algorithm is a simple model with a few well understood parameters.\nBy automatically surfacing forecasts that perform poorly, effort need only be spent when specific forecasts need attention.\nThe last piece needed to make this process scale is a single set of tools that follow the workflow.\nWe demonstrate how the TICK stack can be used to manage forecasting time series at scale, using InfluxDB to store the data and Kapacitor to manage and surface forecasts.\n \nForecasting time series data across a variety of different time series comes with many challenges. Using the TICK stack we demonstrate a workflow that helps to overcome those challenges. Specifically we take a look at the Facebook Prophet procedure for forecasting business time series.", "Segmenting Channel 4 Viewers using LDA Topic Modelling\n \nWe wanted to tailor the All 4 experience to the different types of people we know use our service. If, as we suspect, viewing preferences are a reflections of personality, then perhaps the different types of people we wish to cater for can be identified by looking at their viewing behaviour on All 4?\nTraditionally topic models expect a corpus of documents as an input, here we substitute documents for viewers and words for views. The patterns (topics) identified are consequently patterns in viewing.\nMllibs LDA topic model is used to recognize these patterns in our viewers\u2019 behaviour on All 4 and allows us to segment them based on how closely aligned their viewing is with each of the topics found.\nThe model is updated periodically using an online variational Bayes algorithm, which processes a subset of the corpus on each iteration, and updates the term-topic distribution adaptively, allowing us to incorporate new shows (n\u00e9e: words).\nDemographic information on our viewers is overlaid to understand gender and age splits of the segments whilst survey data from a subset of each enriches our idea of the flavours of viewer in each group.\nThis sums to a detailed picture of the distinct groups of viewers we serve and affords us the opportunity to tailor the service we provide, hopefully, for the better.\n \nIn this talk I will walk through how we used LDA Topic Modelling to segment Channel 4 viewers based on their online viewing behaviour. Combining segment viewing with demographic information and interests/hobbies/lifestyles from survey data provides a wonderfully nuanced understanding of our viewers and allows us to tailor comms, All4, commissioned content, and recommendations to specific tastes.", "Find the text similiarity you need with the next generation of word embeddings in Gensim\n \nThere are many ways to find similar words/docs with  an open-source Natural Language processing library Gensim that I maintain.  I will give an overview of modern word embeddings like Google's Word2vec, Facebook's FastText, GloVe, WordRank, VarEmbed and discuss what business tasks fit them best.\nWhat is the most similar word to \"king\"? It depends on what you mean by similar. \"King\" can be interchanged with \"Canute\", but it's attribute is \"crown\". We will discuss how to achieve these two kinds of similarity from word embeddings.\n \nWhat is the closest word to \"king\"? Is it \"Canute\" or is it \"crowned\"? There are many ways to define \"similar words\" and \"similar texts\". Depending on your definition you should choose a word embedding to use. There is a new generation of word embeddings added to Gensim open source NLP package using morphological information and  learning-to-rank: Facebook's FastText, VarEmbed and WordRank.", "Building a (semi) Autonomous Drone with Python\n \n\n \nThey might not be delivering our mail (or our burritos--tacocopter.com) yet but drones are now simple, small, and affordable enough that they can be considered a toy. You can even customize and program some of them! The Parrot AR Drone has an API that let's you control not only the drone's movement but also stream video and images from both of its cameras. I'll show you how you can use Python and node.js to build a drone that moves all by itself.", "HistomicsTK: An open-source python toolkit for web-based analysis of digital histopathology data\n \nHistopathology, which involves the examination of thin-slices of diseased tissue at a cellular resolution using a microscope, is regarded as the gold standard in clinical diagnosis, staging, and prognosis of several diseases including most types of cancer. The recent emergence and increased clinical adoption of whole-slide imaging systems that capture large digital images of an entire tissue section at a high magnification, has resulted in an explosion of data. Compared to the related areas of radiology and genomics, there is a dearth of mature open-source tools for the management, visualization and quantitative analysis of the massive and rapidly growing collections of data in the domain of digital pathology. In this talk, we will present our ongoing effort to address this gap through the development of HistomicsTK, an open-source python toolkit for web-based analysis of large collections of whole-slide histopathology images in association with clinical and genetic variables. \nHistomicsTK is being developed with the goal of serving the needs of both pathologists/biologists interested in using state-of-the-art solutions to analyze their data, and algorithm researchers interested in disseminating their solutions for wider use by the community. It currently contains algorithms for common image analysis tasks (ex: color normalization, color deconvolution, segmentation, feature extraction, and classification) and provides an infrastructure for parallel execution of image analysis pipelines. It uses containerization technology to simplify the packaging and portable dissemination of these pipelines for wider use by the community with a standardized interface. It includes an innovative framework that automatically generates REST interfaces and web UI to access these image analysis solutions over the web. \n \nIn this talk, we will present our ongoing effort behind the development of HistomicsTK, an open-source python toolkit for integrated web-based analysis of large collections of digital histopathology data (2D images of a thin-slice of diseased tissue at a microscopic resolution) used for the clinical diagnosis, staging, and prognosis of several diseases including cancer. ", "Mental Models to Use and Avoid as a Data Scientist\n \n\nThe first principle is that you must not fool yourself and you are the easiest person to fool. -- Richard Feynman\n\nA scientific mindset is a powerful force for producing knowledge: from academic fields, to business and public policy. The combination of statistical rigor, an experimental mindset and the drive to ask the right questions produces crucial input to decision making.\nWe will illustrate several principles of scientific thinking that should be more widespread. Each principle will be demonstrated with a specific example that will help it stick with the audience after they leave. \nA few key principles, attitudes, and techniques for Data Science:\n\n\"I am easy to fool\" -- acknowledge confirmation and experimenter bias\n\"Everyone knows what it takes to change my mind\" -- culture of critical thinking, avoid congruence bias (test alternative hypotheses)\n\"A good explanation can be proven wrong\" -- falsifiability\n\"The strength of my belief is proportional to the strength of the evidence\" -- degrees of plausibility\nBlinding analyses -- not letting expectation of result drive model revision\nAnswering the question: what would it look like if your hypothesis were wrong for a hypothetical case?\n\nMindsets to avoid:\n\nJust-so storytelling is a tempting anti-pattern -- creating story to explain the data after knowing the answer.\n\"The correct explanation must be easy for me to understand\" -- availability or belief bias.\n\n \nUsing Jupyter Notebooks and Python code, we will present several data-driven examples of some simple, powerful, yet relatively uncommon, ways of thinking as a good Data Scientist. We will also warn about a few dangerous ways of thinking to avoid. Our Jupyter Notebooks and slides will be made freely available after the talk.", "The CV: A Data Scientist's View\n \nDespite the proliferation of professional networks such as LinkedIn, and the widespread use of online forms in which job seekers can fill in their details when applying for a job, the CV still remains relevant today. From online job boards to recruitment firms, personal websites to companies who remain old-fashioned in their hiring, the CV finds its way in every facet of job hunting. For the HR professional however, the CV is cumbersome to work with. For this reason, it has consumed countless man-hours of work usually spent on mundane tasks. For example, candidate details from CVs are often manually copied into forms to create a structured profile in a database.\nFrom a data scientist's perspective, the CV provides us with a wealth of interesting applications and opportunities for investigation. In this talk, the author will present learnings from over three years of working in this domain as well as a range of applications that can arise.\nConcretely, the talk will cover:\n\nIntroducing the CV Parsing task and its components\nAutomatically extracting text from documents with Apache Tika\nUnderstanding and mitigating text extraction errors\nAnalyzing CV text with an NLP pipeline\nUsing open source tools for core NLP tasks\nHigh-level CV parsing with section identification and sentence classification\nIdentifying important entities with Named Entity Recognition (NER)\nUnderstanding the limitations of existing NER systems\nBuilding your own NER model\nReproducibility in experiments\nBuilding a corpus of annotated data\nRecognizing entity relationships\nEntity normalization\nDiscovering and extracting skills from CV's\nClustering CV's by word vectors\nIdentifying duplicate profiles\n\nThough each of these topics could be an entire talk on their own, the objective here is to present the main idea of each and the role that they play within the broader context of CV parsing and interpreting job seeker data. The talk will also feature code snippets in Python and Apache Spark to give a practical foundation for some of the concepts discussed.\n \nThis talk will focus on two main goals. The first of these is to present the CV, and a job seeker's career trajectory more generally, as a highly interesting and fertile application domain for data science. Secondly, the talk will highlight a number of practical lessons and tips gleaned though years of hands-on experience in this area.", "PyData \"Pub\" Quiz! (Kodiak)\n \n.\n \n.", "\"Lights, camera, AI!\" -  Automated sports videography\n \nInspired by the shaky camera work of friends and parents  alike in school sports events; I've build a Raspberry Pi controlled robot that automatically points a camera at where the action is happening in a rugby match. \nThe talk will discuss the stages of developing an embedded machine vision model. Starting with a simple heuristic approach in order to collect the training data. Then how you can compress the feature space in order to reduce the computational load on the Raspberry Pi. Along with that, how using python multi-processing can help in sharing the load of the machine vision work and servo control.\nAfter the talk people should know a little about embedding machine learning using Raspberry Pi's as well as some insight in to SimpleCV (machine vision library for python) and multi-processing with python.\n \nAmateur sports video quality usually depends on how good your friends are with their camera-phone. In this talk I walk through the stages of how I designed and developed a machine vision model for tracking where the action is happening in a rugby match, and how I made it lightweight enough to run on a Raspberry Pi in real-time .", "Data Science for Digital Humanities: Extracting meaning from Images and Text\n \nThe focus of this talk is extracting meaning from data and making powerful methods usable by everybody. With the advent of big data, new approaches and technologies are needed to tackle the increase in volume, variety, and velocity of data. This talk illustrates how analysts, journalists, and scientists can benefit from exploratory data analysis and data science. \nImagine a journalist who wants to cross-reference the names on the guest list of a parliament with online information about lobbyists to identify which party meets which company. A business analyst might want to quantify what topics certain customers are discussing on Twitter or how their sentiment towards a particular product is. \nExploratory data analysis and data science techniques enable researchers, journalists and businesses to ask bigger and more ambitious questions than anybody before them and to leverage the abundance of information that is available today. \nThe Digital Humanities are located at the intersection of computing and the disciplines of the humanities. They can benefit from the massive-scale automated analysis of content like images and text. Researchers, analysts, and journalists can quantify the state of society from publicly available data like tweets. It is now possible to construct an almost complete map of our civilization just by looking at the tags and GPS coordinates of Flickr photos.\nA vast Python ecosystem is supporting this including machine learning frameworks like scikit-learn, dedicated deep learning frameworks like Keras, and topic modeling tools like gensim. All these tools are open source and can be integrated into powerful data science pipelines. Rather than training neural networks from scratch, pretrained features for text and images can be adapted for fast results.\n \nAnalyzing millions of images and enormous text sources using machine learning and deep learning techniques is simple and straightforward in the Python ecosystem. Powerful machine learning algorithms and interactive visualization frameworks make it easy to conduct and communicate large scale experiments. Exploring this data can yield new insights for researchers, journalists, and businesses.", "Keynote: The Culture of Data Transformation\n \nComing soon.\n \nComing soon.", "PyData in Production: Lesson Learned from Various PyData Deployment Strategies\n \nThe PyData stack offers a remarkably powerful toolkit for building complex machine learning and analytical components quickly.  However, machine learning doesn't happen in a vacuum.  It is part a large system of enterprise software responsible for data processing and must play-well with other tools in the ecosystem.  In order to get the benefits of rapid development while not sacrificing the non-functional requirements, MaxPoint as implemented and tested multiple deployment models for software relying on the PyData stack.  This talk we walk through these various deployment models and discuss the trade-offs of the approach.\n \nWe all love Pandas, Sklearn and the rest of the PyData stack.  They allow us to conduct complex analysis and implement cutting-edge machine learning models simply and easily.  However after the initial model fitting a common challenge often arises - how do we put these models in production ensuring that it fits into a larger organizational architecture?  In this talk we outline the various strateg", "Bridging the gap: from Data Science to service\n \nProviding consulting services or custom machine learning based tools involves many elements beyond choosing the right models, analysis methods and tools. While working at Machinalis, we learned a set of ideas that help us to make our projects successful, which have more to do with the human and engineering side of the service than the science and programming.\nThe goal of the talk is to share that experience with people that are providing or planning to provide services based on data science. It presents some tips and explains how they work and the kind of problems that they avoid. The content of the talk is divided into two main areas:\n\n\nTips related to team collaboration. These include aspects to consider regarding how and what to communicate between different roles in the team (data scientists, developers, project managers, people doing labeling or entry, etc.), and how to make certain decisions that the team faces (choosing tools, knowing when to do a flimsy proof of concept vs when to create robust products using best practices, etc.).\n\n\nTips related to collaboration with clients and users. There are different kinds of clients with varied levels of knowledge regarding the possibilities and limits of data science, and each kind requires an appropriate way of communicating these issues. It is also important to establish clarity about mutual expectations, about what is guaranteed and what is experimental, about how your work will be evaluated, and about how you will integrate with different stakeholders in the client's organization. This alignment with your client is a critical element of a successful service.\n\n\n \nRecent years have brought an explosion of algorithms, models and software libraries for doing data science that allow unprecedented possibilities for solving problems. But providing a data science service as a consultant or a company involves more than just tools. In this talk, I will share the most useful lessons that I learned while working at a company providing these services. ", "Python and Johnny Cash\n \n\n \nI was given this title by the Executive Director of NumFOCUS as punishment for being late in putting together a proposal.\nThis talk will be a huge stretch to link Python to Johnny Cash. I suppose this means I'll have to wear all black. \nI'll probably show off some nifty (C)Python hacks, like adding AST-literals or resurrecting the print statement or implementing lambda-fusion.", "Sirbarksalot: Bark Detection in Python\n \nMy wife and I have two dachshunds, Cody and Caylee. Cody in particular loves to bark and we do a lot to mitigate his barking when we are home. However, when we are at work, we have no idea how much barking he is inflicting on our neighbors. Out of curiosity for his daytime barking habits, I built an app that listens for barks and then sends a notification on Facebook. In this talk, I'll go over the methodology for bark detection from sample collection to creating a bark classifier. Then I will go over the basic structure of the app and the Facebook API component. Lastly, I'll discuss some interesting things I've learned about my dogs,\n \nDachshunds are notorious barkers and my dogs are no exception. Using a spare webcam, the Facebook Messenger API, and some python code, I built a simple app for sending notifications when my dogs are barking. In this talk, I'll cover the methodology and some surprising things I learned about my dogs.", "Ranking hotel images using deep learning\n \nAttractive hotel images can influence the hotels that customers book online. However,selecting the best images is not a trivial problem, as there are tens of million images,and each customer can have their own image preferences (e.g. family vs. business customer). Displaying the best images for each hotel would have a significant impact on conversion. In this talk we will cover how Expedia has been ranking hotel images using deep learning techniques in Python.\n \nIn this talk we will cover how Expedia has been ranking hotel images using deep learning techniques in Python.", "Where are we looking? Prediciting human gaze using deep networks.\n \nVisual saliency models aim at describing human eye fixations and finding the most relevant features in a visual scene. From experience one can justify two important processes that drive saliency: First, low level features like color, intensity or orientation contrast and second, high-level features like objects, faces or signs. \nEye fixations serve as an experimental setup to study saliency phenomena and give insight into the ways we attend scenes. Hereby, computational modelling is used to explain what information processing might be responsible for saliency. Evaluating how well models explain observed fixations give a framework of identifying what features contribute to saliency by designing models with different, e.g. high- or low-level feature extractors. \nMany saliency models have used low-level features but were faced with drawbacks in explaining pronounced saliency caused by high-level contributions. Simply adding face or object detectors has been a plausible follow-up but revealed little about the underlying mechanisms.\nRecent advances in object classification by training convolutional neural networks (CNN) have revealed rich filter representations in a wide range of high-level features and are therefore a promising candidate in building models of visual processing (e.g. VGG-19 trained on ImageNet).\n \nWhich features in an image draw our focus to a specific area while neglecting others entirely? This fascinating question has been motivating researchers for decades but also sparked interest in design and marketing. Thus, saliency models aim at identifying locations that stand out from their visual neighbourhood. Using tensorflow and matplotlib this talk will shed some light on these features..", "Making your code faster: Cython and parallel processing in the Jupyter Notebook\n \nTo improve the speed performance of an application, a requirement that becomes critical as the size of computations and/or databases reach sizes not uncommon nowadays, both code design and programming language issues can be addressed. In Python, scripts are usually dynamically typed and perform one computation at a time, representing two aspects that can be optimized to improve performance. Static typing can be achieved with Cython, a static compiler from Python to C. Parallel processing can be used to take advantage of all the cores present in a machine or a cluster, through the ipyparallel module. While both Cython and ipyparallel can be used in any Python environment, the Jupyter Notebook makes their implementation very simple and is the ideal environment in which to learn the basics about both of these tools.\nThe session will cover the following topics:\n- Installing the ipyparallel module\n- Using the Client class to assign tasks to individual cores\n- Synchronous and asynchronous execution of tasks\n- Use of the %px magic command\n- Statically defining variables and functions using Cython\n- Making modules available to Cython through the use of cimport\n- Numba as an alternative to Cython\n \nAs the complexity and scope of applications grow, it is very common to run into slow performance issues. In Python, it is possible to improve the speed of execution with the use of parallel processing and the Cython compiler. The Jupyter Notebook makes the implementation of both of them a relatively simple task, which will be the focus of this session.", "Online customer targeting as a classification problem\n \nHow much does exposure to online advertising influence a consumer to purchase a product? Predicting the behaviors of potential consumers is a challenging task in the online advertising business. These valuable user actions can be quite rare, and there may be a dearth of information available for modeling each user. Having an accurate model for prediction of user behaviors allows a business to minimize the cost of serving ads to users who would not be influenced by them while maximizing the occurrence of user actions for which the business gets paid.\nIn practice there may be a number of ways to model these behaviors and ensembling the results together may result in the best prediction.  One useful model attempts to predict whether users who have previously visited a web site will return and make a purchase, if they are subsequently served an ad. This so called retargeting model can be modeled as a supervised classification problem with both time varying and time invariant features. \nThis presentation will illustrate some of the concepts behind creating a retargeting model, using data provided by Maxpoint, from model conceptualization to deployment. Python and Jupyter notebooks provide an environment where prototypes can be rapidly implemented, evaluated, and iterated on. Powerful community libraries offer functionality to deal with problems like class skew, overfitting, and data censoring. The strong performance of the Python language and availability of a multiprocessing solution offer avenues for efficiently transitioning a prototype into a job that can be run at scale with minimal modification. Concepts and code will be presented in detail.\n \nPredicting user actions is a challenging and important part of any online advertising business.  The rarity of some user actions introduces problems of class skew, overfitting, and data censoring to modeling efforts. This presentation will delve into the business problem and walk through an example of fitting a predictive model to online advertising data provided by Maxpoint.", "Similarity Search in Patents via modern Artificial Intelligence\n \nTopics to be covered include:\n\nUse of python for data acquisition and AI modeling\nObservations on Natural Language Processing packages like NLTK, spaCy, gensim (performance, features, etc)\nPerformance results of 'similarity' search vs keyword-Boolean search\n\n \nWe will describe our new Patent Management Tool built using modern Artificial Intelligence (Deep Learning) in Python. The AI generates data-based vocabularies used in \u00e2\u0080\u0098similarity\u00e2\u0080\u0099 search - allowing a one-click lookup of semantically related words that would otherwise be missed by the user. Preliminary results show that \u00e2\u0080\u0098similarity\u00e2\u0080\u0099 search outperforms the traditional keyword-Boolean patent search.", "Brand recognition in real-life photos using deep learning \n \nProject was created during a 3-months full-time data science program called Data Science Retreat. I\u00e2\u0080\u0099ve developed a brand recognition system that detects logos in real-life photos. It uses Deep Learning and SVM on Instagram images uploaded by users. It allows for better brand monitoring in social media independently from text descriptions and hashtags. I\u00e2\u0080\u0099ve used state-of-the-art Deep Learning tools (Theano, Lasagne, Caffe). \nDeep Learning is an advanced Machine Learning technic that usually needs lot of data and powerful machines. I would like to show how to use it when you don't have huge dataset and your resources are limited in time and money. \nI would also like to talk about a new era in which developers can start to write code which understands visual context of images and is not limited to descriptions and tags.\n \nTalk will be about brand recognition system that detects logos in real-life photos. It allows for better brand monitoring in social media independently from text descriptions and hashtags. Software was implemented using state-of-the-art Python deep learning tools (Theano, Lasagne, Caffe). Project was created during a 3-months full-time data science program called Data Science Retreat.  ", "KEYNOTE: Scaling Out PyData\n \n\n \n", "Mosaicking the Earth every day\n \nGlobal mosaics are created from Planet satellite images at regular intervals (quarterly, monthly, and weekly) by selecting the best quality scenes (e.g. cloud- and haze-free), color balancing, and seamlessly compositing millions of scenes to create continuous maps of the Earth for each time slice. As our data rate increases, we are scaling up the cadence of our mosaics, and plan to build a continuously updated \"dynamic\" mosaic of the most recent cloud-free images of the Earth. Daily data at 5 meter spatial resolution will open up new analysis techniques previously limited by the temporal or spatial resolution of existing instruments.\n \nPlanet's mission is to image the surface of the Earth every day. With over 140 Earth observation satellites currently in orbit imaging over 100 million square kilometers of land area per day, we are approaching that goal and expect to achieve it later this year. With a data pipeline built with open source tools, we process terabytes of data every data and manage an archive of petabytes.", "Find the text similiarity you need with next generation of word embeddings in Gensim\n \nWhat is the most similar word to \"king\"? It depends on what you mean by similar. \"King\" can be interchanged with \"Canute\", but it's attribute is \"crown\". We will discuss how to achieve these two kinds of similarity from word embeddings.\nAlso touch on how to deal with the common issues of rare, frequent and out of vocabulary words.\n \nThere are many ways to find similar words/docs with  an open-source Natural Language processing library Gensim that I maintain.  I will give an overview of modern word embeddings like Google's Word2vec, Facebook's FastText, GloVe, WordRank, VarEmbed and discuss what business tasks fit them best.", "Social Networks and Protest Participation: Evidence from 130 Million Twitter Users\n \nPinning down the role of social ties in the decision to protest has been notoriously\nelusive, largely due to data limitations. The era of social media and its global use by\nprotesters offers an unprecedented opportunity to observe real-time social ties and\nonline behavior, though often without an attendant measure of real-world behavior. We\ncollect data on Twitter activity during the 2015 Charlie Hebdo protest in Paris which,\nunusually, record real-world protest attendance and high-resolution network structure.\nWe draw on a theory of participation in which protest decisions depend on exposure to\nothers' intentions, and network position determines exposure. Our findings are strong\nand consistent with this theory, showing that, relative to comparable Twitter users,\nprotesters are significantly more connected to one another via direct, indirect, triadic,\nand reciprocated ties. These results offer the first large-scale empirical support for the\nclaim that social network structure has consequences for protest participation.\n \nData mining social networks for evidence of political participation. A demonstration of python being used to data mine the twitter conversations around the #JeSuisCharlie hashtag, and analyzing it to learn about real world protest behavior.", "Triaging Feedback Form Data\u00c2\u00a0\n \nCompanies gain useful insights about their users from feedback form and other unstructured text data including live chat messages. Even though they are read and responded to, often such data is ignored when thinking about larger scale trend analysis and this can result in missed insight about how users react to a product or service. Sometimes analysis is being done by looking at changes in user sentiment or other heuristics, however it could be taken a step further by applying predictive modeling in attempt to recognize areas that need more attention and support. While you can use predictive modeling on network and log data, that is looking at how the hardware is handling your users requests, not how it's being perceived by users. By predicting areas where users are having difficulty whether it's with the UI or with the platform's response time you can triage these areas of concern to prevent future cases of negative perception. This talk will cover how to utilize common NLP tools used to gather and process the features in Python then will use R to perform trend analysis and predictive modeling then use the results to triage what areas should be focused on in the future.\n \nThis talk will cover how to use predictive modeling on unstructured text data including feedback form, social media or chat message data to triage issues in order to prevent future problems with a service, platform or user interface using NLP techniques in Python and R.", "Just Bring Glue - Leveraging Multiple Libraries To Quickly Build Powerful New Tools\n \nPrerequisites\n\nA basic understanding of programming with python\nHelpful if you'd like to follow along with the workflow example, but not required: ipython (shell) and pudb installed\n\nPart 0 - Introduction and Overview\nWe'll open with an introduction and a quick overview of each section of the talk.\n\n\nPart 1 - Case Studies: two brief case studies covering tools or projects that were created by composing several open source python libraries\n\n\nPart 2 - Strategies for Becoming Familiar with New Tools: covering the use of pip, the ipython shell and a few popular debuggers\n\n\nPart 3 - Potential Pitfalls: Common conceptual issues that arise when creating powerful tools quickly with \"glue code\"\n\n\nPart 4 - Closing, Q&A: Giving a list of resources and links, and hopefully learning about some new tools from the audience as well\n\n\nPart 1 - Case Studies\nFor the sake of time we'll move fairly quickly through each case, focusing on how libraries used in each project come together at a high level with a few flow charts, diagrams and key \"glue code\" listings.\nCase study 1 - An Intelligent Unit Test Discovery Tool\nThe first case study will cover a tool the author built that takes a natural language search term and suggests unit tests to run from a large suite of test cases. \nModules / Libraries Covered:\n\npyclbr\nnose\ntermcolor\npylzma\nscikitlearn\n\nCase study 2 - Mining, Processing and Visualizing Twitter Data\nNext, we'll review the tools used (and how they came together) in a popular 7-part tutorial on mining, processing and visualizing Twitter data.\nModules / Libraries Covered\n\noperator\ncollections\njson\ntweepy\nNLTK\naltair\npandas\ngeojson\n\nPart 2 - Strategies For Becoming Familiar With a New Python Tool\nFollowing the case studies, we'll segue into a brief overview of strategies the author uses to explore new tools and how they might fit together.\nTopics will include the use of pip and how to make the best use of a powerful debugger like pudb that allows interaction with an ipython shell when exploring a library's API.\nPart 3 - Potential Pitfalls\nWe'll round out the talk with some points on the potential pitfalls of quickly gluing things together.\nBeyond the technical concerns - it can be easy to imagine we understand a domain or problem better than we do with such powerful tools quickly at hand. \nWhat is a good strategy for discovering unknown unknowns and turning them into known unknowns when tackling a new problem? How can we close those remaining gaps and ensure we're using a tool correctly?\nLastly, how do we decide when to stop tumbling down the rabbit hole? Which are the gaps that don't necessarily need to be closed?\nPart 4 - Closing and Q&A\nA comprehensive list of categorized links will be given. The talk will close with a Q&A where (hopefully) audience members will contribute a few of their favorite tools not covered by the talk.\nBoth author-provided and audience-provided links will be compiled into a resource that will be shared via a blog post (similar to this post on resources for Machine Learning self study; compiled from a talk I gave at a BarCamp in Chiang Mai, Thailand). \n \nIt has never been easier for developers to create simple-yet-powerful data-driven or data-informed tools. Through case studies, we'll explore a few projects that use a number of open source libraries or modules in concert. Next, we'll cover strategies for learning these new tools. Finally, we wrap up with pitfalls to keep in mind when gluing powerful things together quickly.", "TBA\n \nComing Soon\n \nComing Soon", "A Gentle Introduction to Neural Networks (and making your own with Python)\n \nIdeas:\n- search for intelligence machines, what's easy for us not easy for computers\n- pigeon and worm brains few neutrons but impressively capable\n- biologically inspired computing\n- simple function -> classifier -> iterative refinement\n- problems that need more nodes (XOR) -> many nodes\n- how to train - gently explain backdrop\nDIY:\n- MINST dataset\n- simple network 3 layer\n- matrix multiplication to aid calculations\n- preprocessing, priming weights\n- 95% accuracy with very simple code!\n- improvements lead to 98% accuracy!\nsource code will be online Python 3 notebook on makeyourownneuralnetwork.blogspot.com and @myoneuralnet\n \nNeural networks are not only a powerful data science tool, they're at the heart of recent breakthroughs in  deep learning and artificial intelligence.\nThis talk, designed for a complete beginners and anyone non-technical, will introduce the history and ideas behind neural networks. You won't need anything more than basic school maths.  We'll gently build our own neural network in Python too.", "High Fidelity Web Crawling in Python\n \nMore to follow\n \nPython modules such as Requests make it easy for Python to pull HTML from a webpage which you can feed to your parsing function. What becomes difficult is converting that process into an autonomous process to crawl webpages to parse their HTML for data. This talk covers the lessons learns and solutions I\u2019ve found to create high fidelity autonomous web crawling scripts in Python.", "Outlier detection methods for detecting cheaters in mobile gaming\n \nOutlier and anomaly detection is an interesting and much used area of statistics. Its application ranges from fighting fraudulent transactions in finance, detecting anomalies in data systems and in data ingestion quality assurance.\nIn this talk I\u2019ll be describing an alternative example application of detecting cheaters within mobile games. I\u2019ll be discussing the motivation around detection and what in particular we are trying to detect - detecting every facet of cheating behaviour is not necessary; cheating that gives a negative experience to other users is particularly undesirable.\nPart of this talk will go into the various approaches to outlier detection:\nSingle variate heuristics\nDensity based approaches in multivariate data (e.g. minimum volume ellipsoid)\nNeural network methods (auto-encoders and replicator neural networks)\nI will also explore the actual implementation issues and solutions in the particular example of mobile games. In the Free-to-play market, the majority of our users enjoy the game for free - with a minority making up the player base that spends on in-app purchases. Because of this, our highest value customers can often look like outliers alongside cheaters. Getting those sets of users mixed up is highly undesirable so some manual intervention is needed to make sure there are very little or no false positives.\n \nIt has become increasingly difficult to detect cheaters in mobile games. As the complexity of games grow, so do the different methods of cheating. As users realise the boundaries of simple mechanisms they find more nuanced ways to cheat the system. In this talk I will be taking you through a number of detection methods and discussing implications and implementation in the real world.", "Machine Learning to moderate ads in real world classified's business\n \nIn an online classified's business, one may encounter a lot of spam and fraud once the business starts to grow. One way to inhibit this is to moderate all incoming advertisements by using static filters or having human moderators but this may not go a long way if the business deals with millions of advertisements every day. Static filters may catch good advertisements and flag them as bad and would also require humans to add, remove or improve them. On the other hand employing human moderators to moderate all incoming advertisements does not scale. Creating machine learning models is what we believe is the right way to address this kind of problem. Machine learning models identifies patterns in data and classifies ads thereby reducing the overhead of creating complex filters and reducing number of human moderators\nIn this talk we share our experiences in building machine learning models to act as human moderators. This talk will cover mainly the following topics\n\nCreating a simple platform architecture that can do predictions on millions of requests without spending too much resources on devops and machines\n-- Batching of requests so as to use CPU's optimally. \n-- Containerising code so as to have ease of deployments\nCreating models from training set containing millions of rows and thousands of features which can be trained on simple machines rather than using complex Spark Hadoop Architectures. \n-- Using SVM files as a means data format rather than huge dataframes that can not fit in memory\nOrchestrate model generation pipeline using Luigi workflow.\nControlling error rate using prediction probability thresholds\nEvaluating moderation/fraud detection models.\nManagement of hundreds of models and manage their performance across all geographical regions.\n\n \nIn todays world of online business, it is difficult to moderate all the content coming to your site.  In this talk we share our experiences on how we built machine learning models to moderate 100+ million classified ads every month. Audience will get a chance to experience a real world of content moderation and a race to beat online fraudsters and scammers.", "Machine Learning Techniques for Class Imbalances & Adversaries\n \nThe Data Innovation Lab at Capital One has explored more advanced modeling techniques for class imbalance & adversarial actors. Our use case has allowed us to survey the many related fields which deal with these issues, and attempt many of the suggested modeling techniques. Additionally, we have introduce a few novel variations of our own. \nThis talk will provide an introduction to the problem space, a brief overview of the modeling frameworks we've chosen to work with, a brief overview of our approaches, a discussion of lessons learned, and our proposed future work. \nThe approaches discussed will include ensemble models, deep learning, genetic algorithms, outlier detection via dimensionally reduction (PCA and neural network auto-encoders), time-decay weighting, and Synthetic Minority Over-sampling Technique (SMOTE sampling).\n \nThere are many areas of applied Machine Learning which require models optimized for rare occurrences (i.e. class imbalance), as well as users actively attempting to subvert the system (i.e. adversaries).\nThis talk will guide the audience through multiple published techniques which specifically attempt to address these issues. ", "Building self-evolving video game AI in Python\n \nLaserCat is a video game written by our team with Pygame. This project is to use the game as a test bed to experiment and implement various machine learning algorithms in Python to gain insight in building video game AI and AI in general.\nIn LaserCat, the player controls the cat to shoot the mouse-saucers that are flying across the screen from left to right. The player gains positive score from each kill, and gains negative score from letting the cat collide with the mouse-saucer or letting the mouse-saucer escape.\nWe start with building an imitation AI that can imitate a human player's decision-making. We recorded 2 hours of my gameplay as training data, including the coordinates of cat, lasers, and mice, as well as the action I chose to execute in each frame. Then it becomes a supervised learning problem in which the coordinates are the predictors and my action is the response. \nHowever, not all the coordinates are available at any given time. We handle the missing values with a hierarchical random forest model: We divide the data into 32 subsets according to the 32 missing value patterns, and train a random forest model with the scikit-learn library for each subset. Then, when the imitation AI plays the game, in each frame, it extracts the coordinates of all the objects on the screen, identifies the missing value pattern, and uses the corresponding random forest to predict the action that the human player would take. \nNext, we consider the more interesting and challenging problem of building a self-evolving AI that can learn on its own. It is a reinforcement learning problem in which the goal is to maximize the score gained per minute. In reinforcement learning, the AI has to learn the optimum strategy by an educated trial and error. \nAll the coordinates in each frame are too much information for the AI to learn efficiently. So we simplify the information from each frame by building a grid that moves with the leading mouse to track the cat's approximate relative position. Then we implement Bayesian Q-learning to let the AI learn the best action to take given the cat's position in the grid. The AI cat gets smarter and smarter as it plays the game and explores different actions. After 7 hours of training, the AI is able to play on a super-human level.\n \nIn this talk we will look at a few machine learning algorithms and their Python implementations to build AI's that can play LaserCat, a video game written with Pygame. We start with building an imitation AI that can imitate a human player's decision-making. Then we build a self-evolving AI that can explore and learn on its own to eventually achieve super-human level. ", "Visualizing Geographic Data With Python\n \nMaps have been such a mainstay of our lives for so long now that it's hard to imagine just how complex it is to create one. Keep in mind though, the earth is a 3-dimensional spherical object, so we're stuck with the problem of \"projecting\" the world onto a 2-dimensional surface. What this means is that every map you've ever looked at was wrong in some way.\nIn this talk, we\u00e2\u0080\u0099ll discuss what a map projection is, and why the Mercator projection, the map you use everyday, is both incorrect and unfair, but useful nonetheless. We\u00e2\u0080\u0099ll also see some ways that we can create maps using Python. We\u00e2\u0080\u0099ll first see how to create data-rich maps using the matplotlib library with the basemap toolkit. We\u00e2\u0080\u0099ll then see how to create maps for the web using libraries like Folium, the python interface to Leaflet.js.\nBy the end of this talk, you should have a general understanding of the problems surrounding the creation of effective maps. You should also feel comfortable picking out a proper map projection and plotting data on it using a multitude of techniques and the Python language.\n \nThe statistician George Box once wrote that \u00e2\u0080\u009call models are wrong, but some are useful\u00e2\u0080\u009d; the same could be said for maps. In this talk, we\u00e2\u0080\u0099ll discuss the problems that arise when creating 2-dimensional representations of our world. We'll then see how to create data-rich maps using Python, matplotlib, and the basemap toolkit. we'll also see how to create maps for the web using the Folium library.", "Smart Banking - Real Time Driven\n \nBanking has been the one of the most ancient professions for humans. Yet, when it comes to advanced or smart banking solutions we haven\u00e2\u0080\u0099t experienced a radical shift even in the 21st century. We, at Number26, aim to enrich that experience and provide an advanced, mobile banking platform for our customers. Each of our features, revolves around  users - their interests and personal preferences. We achieve such flexibility by the use of state-of-the-art machine learning and statistical learning methodologies. In particular, we would like to present a deeper insight into two major features we use in our product: (a) automatic real time categorisation of transactions: how we combine the power of the cloud computing service (AWS), with the intelligence of machine learning based models to achieve this (b) predictive modelling of users future financial status: how artificial neural networks do the smart thinking for our users to come up his predicted personal finances. The tech stack ranges from Python (Keras, scikit-learn and more), Dockers, AWS lambda, EBS, NLP (stanford nltk) and machine learning classifiers and more. In this presentation, we would like to make the message clear, how machine learning can make even the age old traditional ways of banking a fun.\n \nHarnessing the power of machine intelligence to empower users with advanced and smarter banking solutions.", "Assessing the quality of a clustering\n \nThere are many different methods for finding groups in data (cluster analysis), and on many datasets they will deliver different results. How good a clustering is for given data depends on the aim of clustering and on the user's concept of what makes objects \"belong together\". I will present some approaches to assess the quality of a clustering and to compare different clusterings. Particularly, I will present some indexes that measure various desirable aspects of a clustering such as stability, separateness of clusters etc.  Different aims of clustering can be taken into account by specifying which aspects are particularly relevant in the situation at hand. \n \nThere are many different methods for finding groups in data (cluster analysis), and on many datasets they will deliver different results. How good a clustering is for given data depends on the aim of clustering. I will present a number of methods that can be used to assess the quality of a clustering and to compare different clusterings, taking into account different aims of clustering. ", "Medical image processing using Microsoft Deep Learning framework (CNTK)\n \nThe field of medicine is underserved by technology and Microsoft Health is a research-focused incubator group leveraging AI to transform healthcare.  Among areas of investigation is computer vision of medical images with Dicom format, including X-rays, CT scans, and photographs.  Computer vision is an actively growing field with tremendous early impact on quality and cost of medical care.  In this presentation we will show standard preprocessing techniques for computer vision of medical images using Python and CNTK (Cognitive Toolkit by Microsoft).  Python is a first-class citizen of CNTK and a primary language for running deep-learning models.\nThrough examples we demonstrate how to utilize python and libraries (skimage, OpenCV, CNTK), including image preprocessing to normalize and segment images; and deep learning model evaluation and performance.  These examples will be reproducible with publicly shared code on open data sets.\nhttps://github.com/usuyama/pydata-medical-image\n \nIn Microsoft Health we are continuously evaluating the possibilities of machine learning methods,  including image processing techniques and deep learning in medical computer vision. In this talk, we explain typical medical image analysis problems and present how we developed and evaluated deep learning methods using Python and CNTK (Cognitive Toolkit by Microsoft).", "WTF am I doing?  An introduction to NLP and ANN's\n \nCollecting data is more fun when it's about myself: I'm an expert! So I program my computer to spy on me.  And then I play with the data, the better to understand both ML techniques and what I've been up to.  In the limit, I try not only to understand how I spend my time but also to predict how I am about to spend my time.\nConsider the active window on my computer: the window with which I am currently interacting.  Watching the window name creates a sequence of window titles and a great NLP playground.  (\"Natural language\" is here a discovered irony.)  Snapshotting the contents of the window in turn creates long sequences of images.  To save space, I reduce the images to postage stamp size, and yet it turns out I can learn some very interesting things nonetheless.  Some reasonably simple neural networks are enough to discern how I spend my time, when I work on which projects.  Indeed, well beyond providing a great playground for learning about ML techniques, the project also provides a good reminder (warning?) about how much we can learn from what appear to be relatively scant data.  Of course, the links in the final slides will share all the code to help you get started with projects of your own.\n \nThis talk will be a playful but serious introduction to natural language processing and image processing with (artificial) neural networks.", "Engage the Hyper-Python - a rattle-through many of the ways you can make a Python program faster\n \nA fast paced high-level overview of speed optimisation in Python.\nWe will start by looking systematically at the most common causes of poor speed, highlighting which resources are being the bottleneck in each case and giving practical advice on how to find out.\nWe will then discuss parallelism with threads and processes, both in the standard library and using celery.\nWe will discuss Pypy and Cython as alternatives to regular Python for CPU intensive tasks.\nWe finish our tour with asynchronous processing in Python 3 using async.io.\n \nA fast paced high-level overview of speed optimisation in Python.\nWhat makes a program \"slow\"? How to tell what is making your program slow.\nCommon speed-up paradigms: parallelization, alternatives to the regular Python interpreter and asynchronous processing.", "ElasticSearch and Redis: How and When to Use Them\n \nElasticSearch\n\nOverview of indexes & types\nPOST example\nGET example\nQuery example\nKibana visualizations example\n\nRedis\n\nOverview of Redis\nSET example\nGET example\nHMSET example\nFlask-SocketIO example \n\nAdditional Time\nif I get additional time, I'll show more examples of each technology and maybe Keen IO.\n \nWhen working with data, you have some viable options for keeping them in short and long-term storage.  I will be going over why ElasticSearch and Redis are great for data storage.  This talk will explain the purpose of each datastore when visualizing on Kibana or websockets.  ", "JupyterHub: A \"things explainer overview\" [Carol Willing]\n \nThree main actors make up JupyterHub:\nmulti-user Hub (tornado process)\nconfigurable http proxy (node-http-proxy)\nmultiple single-user Jupyter notebook servers (Python/IPython/tornado)\n\nJupyterHub's basic principles for operation are:\nHub spawns a proxy\nProxy forwards all requests to Hub by default\nHub handles login, and spawns single-user servers on demand\nHub configures proxy to forward url prefixes to the single-user servers\n\n \nWith JupyterHub you can create a multi-user Hub which spawns, manages, and proxies multiple instances of the single-user Jupyter notebook (IPython notebook) server.\nJupyterHub provides single-user notebook servers to many users. For example, JupyterHub could serve notebooks to a class of students, a corporate workgroup, or a science research group.", "Applying machine learning to software development to reduce bugs\n \nSoftware development is in a historical transition from a stage of organized craftsmanship to a stage of industrial production of software.  As a part of this transition machines are doing more and more of the repetitive work freeing humans to do the creative work and decision making.  However we are behind in the ability to extract risk signals from development artifacts of large projects.  We are not getting better at reducing project failures.\nWe can use machine learning applied to the development process to improve outcomes.\nThis talk shows how change history and issue tracking data can be correlated to identify areas of risk concentration in the project.  Simultaneously some statistical results show us areas where conventional methods may be wasting effort and areas where these may be ignoring risk.  \nDemo uses example code and artifacts from Apache projects, and scikit-learn with Jupyter notebook.\n \nThis talk shows how we can reduce risk of failure in software development by using machine learning.  Using example code and artifacts from Apache projects, and scikit-learn with Jupyter notebook,  we show how to identify areas of risk in large codelines. Some surprising statistical results on distribution of risk in code are also shown., which suggest we may be \"doing it wrong\".", "Building a polyglot Data Science Platform on Big Data systems\n \nTDB\n \nTBD", "DyND: Enabling complex analytics across the language barrier\n \nDyND is a dynamic array library for structured and semi-structured data, written with C++ as a first-class target and extended to Python with a lightweight binding. It aims to be a cross-language platform for data analysis, by bringing the popularity and flexibility of the Python data science stack to other languages, such as C++, R, and Javascript. It is inspired by NumPy, the Python array programming library at the core of the scientific Python stack, but tries to address a number of obstacles encountered by some of NumPy\u00e2\u0080\u0099s users. Examples of these are support for variable-sized strings, missing values, ragged array dimensions, and versatile tools for creating functions that apply generic patterns across arrays.\nThis talk will introduce the DyND library, motivating it with simple datasets that are hard to process with existing tools. We'll discuss its architecture, features, and a roadmap for the future.\n \nDyND is a C++ library for dynamic, multidimensional arrays. Inspired by NumPy, it aims to be a cross-language platform for data analysis, by bringing the popularity and flexibility of the Python data science stack to other languages, such as C++, R, and Javascript.", "Building a community fountain around your data stream\n \nThe increasing availability of real-time data sources and the Internet of Things movement have pushed data analysis pipelines towards stream processing.  But what does this really mean for my applications, and how do I have to change my code and workflow?  In a new era of \u201cKappa architecture,\u201d it\u2019s easier than ever to use the same programming model for both batch and stream processing. \nFor those interested in the design and operations side, I will cover high-level design considerations for architecting a modular and scalable stream processing infrastructure that can support the flexibility of different use cases and can welcome a community of users who are more familiar with batch processing.\nFor the fast-batching Pythonistas, I\u2019ll talk about some of the advantages of using streaming tech in a data processing pipeline and how to make your life easier with 1) built-in replication, scalability, and stream \u201crewind\u201d for data distribution with Kafka, 2) structured messages with strictly enforced schemas and dynamic typing for fast parsing with Avro, and 3) a stream processing interface that is similar to batch with Spark that you can even use in a Jupyter notebook.\nWhen you\u2019re ready to jump into the stream, or at least take a drink from the fountain, I\u2019ll point you to an open source, containerized (with Docker), streaming ecosystem testbed that you can deploy to mock a stream of data and take your streaming analytics on a dry run over an astronomical data stream.\n \nWith the trend towards data streams, building successful streaming analysis systems means building a community comfortable with streaming tech. But getting started with stream processing can be intimidating for anyone. In this talk, I\u2019ll talk about designing and deploying a mini-testbed system to scale down the stream and how you can practice your favorite algorithm on an astronomical data stream.", "Analyzing 3D objects with power of Deep Learning and Cython\n \nWe live in great time for development of Machine Learning algorithms. There is an abundance of ways to implement models, a lot of them are in Python or have a python API. Python is a great way to implement high level APIs and connect your code to other parts of data processing. Learning how to write in Theano or Tensorflow can be very helpful for most of the people facing problems with complex data. But for a niche area like Deep Learning for sparse 3D data there was no solutions.  Our python module enables creation of deep neural networks to process sparse data quickly using combination of Python and fast C++/CUDA code underneath. \nIn my talk I'll explain how to connect high performance code with practical user level abstractions using Cython, as it was done in our project.\nYou can see code in a github repository or checkout our paper.\n \nDeep Learning taken world by storm in recent years, and most of the time it's powered by Python language. Working with 3D data is computationally demanding even by most powerful GPUs. A lot of times 3D data is in sparse form, or can be turned without loosing too much of it's usefulness. We combined high performance of CUDA library with ease of use and power of Python by getting good with Cython.", "Dask for ad-hoc distributed computing\n \nThe Python data science stack contains efficient algorithms with intuitive interfaces for sophisticated and friendly analysis. As the data science community tackles larger problems with larger hardware we naturally ask how best to parallelize this software stack both across many cores in a single computer and across computers in a cluster. This turns out to be harder than it looks, even with traditional Big Data tools like MapReduce, Storm, and Spark. Both the complexity of the algorithms and the high expectations for interactivity raise challenges for these systems. This talk lays out the benefits and challenges of parallelizing a numeric analytic stack, and then describes Dask, a parallel framework gaining traction within the Python community for interactive performant parallel computing, and finally goes through a few domains where this work is enabling novel science today.\n \nThis talk discusses parallel and distributed computing in Python, particularly for ad-hoc and custom algorithms.  It focuses on Dask, a Python solution for flexible distributed computing.", "Balancing scale and interpretability in analytical applications with sklearn and ensembling methods\n \nModels of large-scale, high-dimensional business data are often challenging to develop because they must be simultaneously accurate (they must account for the data well) and intelligible (they must yield output that is easily interpretable by the people who use the models to inform business decisions). This is compounded by the fact that often multiple analysts are responsible for contributing to these analyses.  We demonstrate a way of addressing this problem in which we (1) divide a large set of features into smaller sets of features with easily interpretable labels, (2) fit a separate model for each feature subset in order to obtain predictors for the ensembling model, and then (3) combine these separate predictors via ensemble learning.  We also assess each predictor\u00e2\u0080\u0099s importance in the ensembled model. We implement this solution in Python, leaning heavily on sklearn, taking advantage of Python\u00e2\u0080\u0099s flexibility and ease-of-use both in software development and data analysis.\nWe describe the modeling framework in detail and present a case study demonstrating our approach by modeling the weekly sales volume of household goods across many locations of a national retail chain that yields models that leverage the predictive information of many variables without sacrificing interpretability. Moreover, the framework yields models that are easily extensible and modifiable by many contributors.  The framework is implemented in Python, and showcases Python\u00e2\u0080\u0099s broad applicability in data-intensive industrial applications.  Base models are developed using the Python analytics stack (primarily Pandas, numpy, and scikit-learn).  Moreover, Python\u00e2\u0080\u0099s native object-oriented functionality provides users with an easy way of flexibly and quickly combining and ensembling base models for new data sets.\n \nWe present a machine learning framework using ensemble learning to combine models developed by multiple analysts for distinct subsets of a large feature space.  We apply the framework to retail sales data, but its design can accommodate other types of target variables.  This is a novel application of ensemble learning since it addresses both analytical and organizational challenges.", "Chain, Loop & Group: How Celery Empowered our Data Scientists to Take Control of our Data Pipeline\n \nAs a mobile app analytics company, we work with petabytes of data. Every 1-2 weeks, we are responsible for calculating and ETLing daily download and revenue estimates for 3 million apps in 57 countries over a 1.5 year (and growing) historical time period.\nWe push frequent updates to our estimation models, which means our data pipeline has to be easily adaptable. We need to be able to prototype, test and implement changes to the pipeline in a pinch. Our dev team doesn\u00e2\u0080\u0099t have the capacity to hold our hands during this process. Thanks to a Python tool we built which submits jobs to the Google BigQuery API for Python via Celery\u00e2\u0080\u0099s distributed queuing, our data science team now has the capability to develop on the data pipeline directly. We have direct control over how and when queries should be run because we design the data flows ourselves.\nIn this talk I\u00e2\u0080\u0099ll discuss the specifics of our use case (i.e. volume of data, complexity of the data pipeline, the different views displayed in our end platform, testing and troubleshooting) and how Python Celery provides a framework for us to address these issues.\n \nIn this talk, I\u00e2\u0080\u0099ll discuss how we optimized the bi-monthly refresh of data in our online analytics product using Celery and transferred ownership of this process from the development team to the data science team. I will also introduce the custom tool we built to submit jobs to BigQuery via Google\u00e2\u0080\u0099s API for Python. ", "bandicoot:  a toolbox to analyze mobile phone metadata\n \nThe metadata generated at large scale by cellphones and collected by literally every carrier around the world have the potential to fundamentally transform the way we fight diseases, design transportation systems, and do research. Scientists have compared the recent availability of these large-scale behavioral data sets to the invention of the microscope and new fields such as Computational Social Science have recently emerged. Mobile phone metadata have, for example, already been used to study human mobility and behavior in cities, understand the propagation of viruses such as malaria and dengue fever. They have been combined with machine learning algorithms to predict people's age, gender, personality, loan repayments, and crime.\nBandicoot is a free and open-source Python toolbox to extract more than 160 features from standard mobile phone metadata. bandicoot focuses on making it easy for researchers and practitioners to load mobile phone data, analyze them, as well as compute and extract robust features from them. Emphasis is put on ease of use, consistency, and documentation. bandicoot has no dependencies and is distributed under the MIT license.\n\nIn this talk, we provide an introduction to bandicoot via real life case studies, showing you how to visualise and analyze large scale data sets, or directly metadata from your own phone.\n \nBandicoot is a free and open-source toolbox to process mobile phone metadata. It provides standardized and privacy-preserving methods to analyze such datasets, returning more than 160 behavioral indicators. Bandicoot is a complete easy-to-use environment for researchers and developers, allowing them to load their data, perform analysis, and export their results with a few lines of code.", "High-Performance Distributed Tensorflow: Request Batching and Model Post-Processing Optimizations\n \nIn this completely demo-based talk, Chris will demonstrate various techniques to post-process and optimize trained Tensorflow AI models to reduce deployment size and increase prediction performance.\nFirst, we'll use various techniques such as 8-bit quantization, weight-rounding, and batch-normalization folding, we will simplify the path of forward propagation and prediction.\nNext, we'll loadtest and compare our optimized and unoptimized models - in addition to enabling and disabling request batching.\nLast, we'll dive deep into Google's Tensorflow Graph Transform Tool to build custom model optimization functions.\n \nIn this completely demo-based talk, Chris will demonstrate various techniques to post-process and optimize trained Tensorflow AI models to reduce deployment size and increase prediction performance.", "Building robust machine learning systems\n \nAs machine learning becomes more prevalent, ever growing parts of the systems we build are changing from the deterministic to the probabilistic. The accuracy of machine learning applications can quickly deteriorate in the wild without strategies for testing models, instrumenting their behaviour and the ability to introspect and debug incorrect predictions.\nThis session will take an applied view from my experience of building production machine learning infrastructure at Ravelin. You\u2019ll learn useful practices and tips to help ensure your machine learning systems are robust. We\u2019ll go into:\n\nLabels and Data - can you trust it? Can you infer them?\nTesting - how do you ensure your model is doing the basics, up to the more complicated examples?\nAuditing and versioning - what's the provenance of your model? What data was it trained on? With which hyper parameters? Can you reproduce it?\nDebugging and introspection when deployed - when you make an awful prediction, can you figure out why that happened and prevent it happening again?\n\nAnd more, with the aim of helping you sleep a little better at night knowing your model is out there in the wild.\n \nWith the growth of AI, ever growing parts of products we build are changing from the deterministic to the probabilistic. The accuracy of machine learning applications can deteriorate in the wild without strategies for testing, monitoring and introspection. You'll leave this talk knowing how to combine the best of software engineering and machine learning to build robust machine learning products.", "You got your engineering in my Data Science: Addressing the reproducibility crisis with Software Eng\n \nData science is the backbone of modern scientific discovery and industry. It makes sense of everything from cancer trials to package delivery logistics. But all is not well with data science. Over the past decade, multiple studies have been found to be unreliable and non-reproducible when other scientists tried to recreate their results. This is due to a variety of factors, including fraud, pressure to publish, improper data handling practices, and bugs in analytic tools.\nThe problems faced by data science mirror problems that software engineering has been trying to solve. While there are no silver bullets to guarantee quality software, techniques have been developed over time that have improved quality and reliability. Some of these techniques, including open source, version control, automation, and fuzzing could be adapted to the data science domain to improve reliability and help address the reproducibility crisis.\n \nData science is the backbone of modern scientific discovery and industry. Unfortunately, multiple recent studies have been found to be unreliable and non-reproducible. Adopting techniques from software engineering might help mitigate some of these problems.", "How to get public data from public servants\n \nA healthy democracy depends on open information so that both the public and officials can participate in making good decisions. But a great deal of information is essentially hidden from public view, held in government databases that regular citizens can\u00e2\u0080\u0099t access. Even the public officials who have access are often not fully aware of what their data might show.\nSo how does a developer get access to that information, in hopes of shedding light on how government does or does not serve the needs of citizens?\n \nIn this short talk, you\u00e2\u0080\u0099ll learn the basics of making a request for public information under both federal and state law, including how to write the request, who to send it to and tactics for getting compliance.", "How you really get your data science models into production - the cool way!\n \nOver the years we have seen many non-tech companies starting to build their own data science teams because they realize that data will eat the world. In an effort to understand the space, these teams have begun to play with their data and create early prototypes. Unfortunately, those prototypes primarily end up in powerpoint and die. Of the ones that move forward, there is a gap in knowledge of their development team in how to release to production. \nFrom our experience, we\u00e2\u0080\u0099ve seen their data science and development team do not speak a common language. At Pivotal Labs, we found a good way to overcome this language problem. Our solution is to follow an API first approach which is one of our core principles.\nIn this talk, I want to to share my experiences of how to put these models into production. I will focus on the tools that we use for this and what data science has to do with microservices. My presentation will contain an end-to-end data science example.\n \nThis talk discusses one of Pivotal Labs\u00e2\u0080\u0099 core principles, API first, and how this can help to overcome the common language problem between data scientists and software engineers. Topics covered in this talk are:\n- Tools\n- Software engineering methodologies like continuous deployment and TDD\n- Microservices\n- PaaS and Cloud Native Data Science ", "Working with Fashion Models\n \nFashion is a visual medium so it makes sense for our models of fashion to include visual features. In this presentation, I'll describe how we've build a general purpose visual fashion representation using CNNs. The network is multi-task (multiple labels per image), multi-image (multiple images per label) and it runs on multiple GPUs. We used the python library Chainer to fit the network.\nI'll visually explore what is going on inside the black box of a neural network and discover how a fashion specific model sees the world differently from generic visual models. Lastly, I'll demonstrate some applications of the representation learned by the model. \nThe initial part of this presentation will be technical but the remainder will be accessible.\n \nSince the dawn of time man has harnessed the power of convolutional neural networks to understand fashion. In this presentation, I carry on the trend and discuss how we've built a general purpose visual fashion representation by simultaneously training against multiple objectives with multiple images per objective. There will be lots of pictures.", "Implementing and Training Predictive Customer Lifetime Value Models in Python\n \nCustomer lifetime value models (CLVs) are powerful predictive models that allow analysts and data scientists to forecast how much customers are worth to a business. CLV models provide crucial inputs to inform marketing acquisition decisions, retention measures, customer care queuing, demand forecasting, etc. They are used and applied in a variety of verticals, including retail, gaming, and telecom. \nThis tutorial is separated into two parts: \nIn the first part, we will provide a brief overview of the ins and outs of probabilistic models, which can be used to quantify the future value of a customer, and demonstrate how e-commerce companies are using the outputs of these models to identify, retain, and target high-value customers.\nIn the second part, we will implement, train, and validate predictive customer lifetime value models in a hands-on Python tutorial. Throughout the tutorial, we will use a real-world retail dataset and go over all the steps necessary to build a reliable customer lifetime value model: data exploration, feature engineering, model implementation, training, and validation. We will also use some of the probabilistic programming language packages available in Python (e.g. Stan, PyMC) to train these models. \nThe resulting Python notebooks will lay out the foundation for more advanced models tailored to the specifics of each business setting. Throughout the tutorial, we will give the audience additional tips on how to tweak the models to fit different business settings.\n \nCustomer lifetime value models (CLVs) are powerful predictive models that allow analysts and data scientists to forecast how much customers are worth to a business. \nIn this hands-on tutorial, you will learn how to implement, train, and validate probabilistic customer lifetime value models in python.", "Smart search using Support vector machines\n \n Smart search using support vector machines\n Search is a common, if not an essential, feature of any digital app. A simple search engine should retrieve all the information that matches a user's query. But a smart search engine is one which also ranks the retrieved information in an order of relevance that is personalised. Machine learning classifiers such a support vector machines are a handy tool for implementing smart search. Using agile, data science practices such as test-driven development and paired programming in a balanced team, we have built a grocery delivery app, with smart search, for one of UK's largest supermarkets. This talk will discuss the approaches and lessons learnt from putting data science in production.\n\n \nSearch is a common, if not an essential, feature of an app. A good search has to retrieve the information that matches a user's query but a great search should also personalize the information to improve the overall relevance. This talk will share the lessons from a recent implementation of a smart search for a grocery delivery app for one of UK's largest supermarkets.", "Data Transformation: A Framework for Exploratory Data Analysis\n \nAt the heart of data analysis, there lies a need to understand the real world entities being represented in the data. Every data set we encounter is an attempt to capture a slice of our complex world and communicate some information about it in a way that has potential to be informative to humans, machines, or both. Moving from basic analyses to advanced analytics requires the ability to imagine multiple ways of conceptualizing the composition of entities and the relationships present in our data. It also requires the realization that different levels of aggregation, disaggregation, and transformation can open up new pathways to understanding our data and identifying the valuable insights it contains.\nIn this talk, we\u00e2\u0080\u0099ll discuss several ways to think about the composition and representation of our data. We\u00e2\u0080\u0099ll also demonstrate a series of methods that leverage tools like networks, hierarchical aggregations, and unsupervised clustering to visually explore our data, transform it to discover new insights, help frame analytical problems and questions, and even improve machine learning model performance. In exploring these approaches, and with the help of Python libraries such as Pandas, Scikit-Learn, Seaborn, and NetworkX, we will provide a practical framework for thinking creatively and visually about your data and unlocking latent value and insights hidden deep beneath its surface.\n \nExploratory data analysis plays a critical role in the job of every data scientist, but very few have a structured process or framework for exploring data quickly and efficiently. This talk will introduce the exploratory framework I use in my day-to-day work and will walk attendees through a practical example of how to use the framework to unlock hidden insights with the help of Python libraries.", "Architectures for Exploring Your Personal Data\n \nAs you emerge from three days of PyData learning experiences, fortified by exposure to life-changing data science tools and inspiring stories, this interactive talk will allow you to reflect on personal data project architectures of data past, present, and yet to come. Adventurous? Look into the future and test your chops by going through a beginner-friendly, hands-on process in which you analyze your own data to inform how that might influence your objective data science practice.\n \n", "Probabilistic Programming in Python with PyMC3\n \nMachine learning is the driving force behind many recent revolutions in data science. Comprehensive libraries provide the data scientist with many turnkey algorithms that have very weak assumptions on the actual distribution of the data being modeled. While this blackbox property makes machine learning algorithms applicable to a wide range of problems, it also limits the amount of insight that can be gained by applying them.\nThe field of statistics on the other hand often approaches problems individually and hand-tailors statistical models to specific problems. To perform inference on these models, however, is often mathematically very challenging, and thus requires time-deriving equations as well as simplifying assumptions (like the normality assumption) to make inference mathematically tractable.\nProbabilistic programming is a new programming paradigm that provides the best of both worlds and revolutionizes the field of machine learning. Recent methodological advances in sampling algorithms like Markov Chain Monte Carlo (MCMC), as well as huge increases in processing power, allow for almost complete automation of the inference process. Probabilistic programming thus greatly increases the number of people who can successfully build statistical models and machine learning algorithms, and makes experts radically more effective. Data scientists can create complex generative Bayesian models tailored to the structure of the data and specific problem at hand, but without the burden of mathematical tractability or limitations due to mathematical simplifications.\nThis talk will provide an overview of PyMC3, a new probabilistic programming package for Python featuring intuitive syntax and next-generation sampling algorithms.\n \nProbabilistic programming is a new paradigm that greatly increases the number of people who can successfully build statistical models and machine learning algorithms, and makes experts radically more effective. This talk will provide an overview of PyMC3, a new probabilistic programming package for Python featuring intuitive syntax and next-generation sampling algorithms.", "Learn to be a painter using Neural Style Painting\n \nHumans have continuously mastered the art of painting images that are visually appealing. They are able mix multiple different styles to produce new styles which instantly catches our attention. Generating such high quality images using algorithms have been less explored in the past. With the advancement in computer vision and object recognition coupled by maturity of Deep Learning frameworks, recently it has become more convenient to generate high quality emotional intuitive artistic images. In 2015, Leon A. Gatys et al, published paper \"Image Style Transfer Using Convolutional Neural Network\" explaining how to generate artistic images using neural representation to separate and combine random input images using a very deep Convolutional Neural Network - VGG-VD.  In this talk, we take a fun dive into learning to be a painter by extracting relevant feature representation from high performing Neural Network using TensoFlow, SparkMagic and Livy in a scalable manner. \nTake away for the audience:\n1. Develop basic understanding of Convolutional Neural Network\n2. Realize benefits of using TensforFlow in building Deep Neural Networks\n3. Learn how to use SparkMagic and Livy to build scalable solutions\n4. Learn how to make machines paint like experts\n5. Learn how to apply this style of painting to create poster thumbnails which might have wide variety of applications\n\n \nVincent Van Gogh was noticeably one of the most influential artistic figures of the Western art. Won't it be great, if one is able to teach machines to paint in a similar manner to create visually appealing images. In this talk, we will learn how to paint images with the help of Convolutional Neural Network - VGG-19 using TensorFlow, SparkMagic and Livy to imitate renowned artists.", "Leveraging recommender systems to personalise search results\n \nThese days a lot of companies are building recommendations engines. The techniques for doing so are widely known and open source technology is accessible. We were experimenting to investigate how we can leverage our recommendations engine to extend personalisation to search results. Besides surfacing items that are relevant based on the search term our approach aims to increase relevancy for each user by considering their personal preferences. We use Pyspark to set up the data and calculate recommendation and preference elements and Elasticsearch as a search engine where we integrate our recommendations approach. We are looking into various ways of how to incorporate customer preferences such as item similarities, matrix factorisation output and preference inference.\n \nThis talk discuses an approach to personalise search results by leveraging techniques of recommender systems .We use Pyspark to set up the data and calculate recommendation and preference elements and Elasticsearch as a search engine", "Becoming a Data Scientist: Advice From My Podcast Guests\n \nThe options for learning data science online are vast and overwhelming, but it is possible to find great resources that work well for you and learn data science without going back to school if you know how to approach it.\nOn my \"Becoming a Data Scientist\" podcast, I have interviewed 17 data scientists (or those on the way to becoming data scientists) about their career paths and how they learned data science. I also interact with hundreds of data scientists regularly on Twitter. In this talk, I compile the frequent advice and the best resources, and give my answers to some common questions about how to become a data scientist.\n \nOverwhelmed by the vast resources (of varying quality) available online for learning data science? In this talk, I compile resources from data scientists on twitter, advice from guests of my podcast, and some of my own experience to help get you started on the path to Becoming a Data Scientist.", "Cross-modal Representation Learning\n \nIn the past, approaches to lexical semantics, such as distributional semantic models (DSMs) that rely on corpus-extracted vectors (e.g. Topic Models), have provided good approximations of the word meanings. Today, despite the large amount of progress in Computer Vision due to advances in Deep learning, cross-modal representation learning remains constrained by vector representation. Techniques based on vector representation have a number of limitations: first, they do not necessarily capture the intra-class variability; second, they do not express the uncertainty associated with assigning target concepts to input data, and finally, distance between the objects is commonly computed as the dot product between their corresponding vectors, a metric which does not allow asymmetric comparisons. \nIn our work, we attempt to go beyond vector representation and move towards representations based on probability distributions. In particular, we use Gaussian embedding which innately incorporates uncertainty and forms a basis upon which various divergences can be defined (e.g. f-divergence). In addition, we consider transferring representations learnt from images to text for better distributional semantic models. We draw inspiration from earlier work on kernels on distributions to propose an embedding space for words and images for efficient cross-modal representation learning and show how one can transfer from visual mode to language mode or vice-versa.\nFinally, we demonstrate examples in learning word representation, image annotation and retrieval, and zero-shot learning. \n \nIn this talk, we introduce an alternative to vector representation based on Gaussian distribution, and we show various learning algorithms using distributions for cross-modal representation learning. We show various advantages of learning based on distributions and demonstrate examples in learning word/concept representation, image annotation and retrieval and zero-shot learning.", "bqplot - Interactive Data Visualization in Jupyter\n \nbqplot is an interactive 2D plotting library for the Jupyter notebook in which every attribute of the plot is an\u00a0interactive widget. bqplot can be linked with other Jupyter widgets to create rich visualizations from just a few lines of Python code. Since bqplot is built on top of the widgets framework of the notebook it leverages the widget infrastructure to provide the first plotting library that communicates between Python and JavaScript code. The visualizations are based on D3.js and SVG, enabling fast interactions and beautiful animations. In this talk, attendees will learn how to build interactive charts, dashboards and rich GUI applications using bqplot and ipywidgets.\nIn the first part of the talk, we will walk the user through the bqplot API:\n- Grammar of Graphics based object model (axes, scales, marks etc) which lets users build custom visualizations\n- Simple API similar to Matplotlib's pyplot, which provides sensible default choices for most parameters\n- Interaction API which lets the user interact with the charts  (selecting subset of data, panzoom etc)\nWe will also review how bqplot ties into the new JupyterLab IDE, by demonstrating how the charts leverage the dashboarding, resizing and output mirroring tools of JupyterLab.\nIn the second part of the talk, drawing examples from fields like Data Science and Finance we will show examples of building interactive charts and dashboards using bqplot and ipywidgets. We will work through examples of using bqplot with popular deep learning libraries to build custom visual dashboards directly in the notebook, including network visualizations and other novel interactive ways to control your training. We will visit the use of the novel selections offered by bqplot to show how they can be used to create advanced applications for time series data. Finally, we will visit the notion of templates - reusable interactive objects that enhance not only end user workflow, but provide developers and researchers seamless ways to incorporate interactivity into their development workflows.\n \nbqplot is an interactive plotting library for the Jupyter notebook in which every attribute of the plot is an\u00a0interactive widget. bqplot can be linked with Jupyter widgets\u00a0to create rich visualizations with just a few lines of Python code.\u00a0These visualizations, which are based on D3.js and SVG, provide an unparalleled level of interactivity without a single line of JavaScript code.", "Smelly London: visualising historical smells through text-mining, geo-referencing and mapping.\n \nSmelly London: visualising historical smells through text-mining, geo-referencing and mapping.\nThe Medical Officer of Health (MOH) reports were published annually by the Medical Officers of Health (MOsH) employed by local authorities across the UK. These reports provided vital statistics and a general statement on the health of the population. MOH reports concentrated on reporting infectious diseases and resolving the problems as well as covering other areas of social responsibilities. They have been long regarded as an important source for 19th and 20th century history of Public Health and stem from reaction to infectious disease in mid-19th century. Although there were attempts at standardisation, the reports display each MOH\u2019s interest, idiosyncrasies and particular strengths. Therefore, they also provide a particular perspective on the everyday lives of Londoners over several generations. \nOver the past few years Wellcome have been developing a world-class digital library by digitising a substantial proportion of their holdings. As part of this effort, approximately 5,800 MOH reports for London spanning from 1848-1972 were digitised in 2012. Currently Wellcome holds the most comprehensive digital collection of the London MOH reports. Since September 2016 Wellcome have been digitising 70,000 more reports covering the rest of the United Kingdom (UK) as part of UK Medical Heritage Library (UKMHL) project in partnership with Jisc and the Internet Archive. No digital techniques have yet been applied successfully to add value to this very rich resource.\nAs part of the [Smelly London] (www.londonsmells.co.uk) project , the OCR-ed text of the MOH London reports has been text-mined for the first time. Through text mining we produced a geo-referenced dataset containing smell types for visualisation to explore the data.  We enrich the text-mining pipeline with NLP, including lemmatization, part-of-speech tagging and automatic identification of smell terms and concepts based on their contextual features. This allows us to identify smell categories in a data-driven fashion and to discover new categories that escaped previous classifications. This step complements the close reading analysis and enables us to scale up the amount of information extracted from the texts.\nAnalysing the MOH reports tells the intimate narratives of the everyday experiences of 19th and 20th century Londoners through the \u2018smellscape\u2019. As the data becomes more structured, they can be more readily overlaid with other maps and images such as Charles Booth\u2019s London Poverty Map and 19th century disease maps. Having multiple layers will enable us to run various comparisons and assess if there are any correlations between smells and diseases as well as links to the socio-economic identity of areas in London. Smell has a great influence over how we perceive places and contribute to the construction of a place\u2019s identify. During the 19th century the paranoia surrounding smells associated with poor hygiene heightened in many European cities. The Great Stink of 1858 resulted in the discussion of moving Parliament outside London for example. Despite the rise of germ theory (Pasteur and Koch) in the 1880s, concerns with disease-causing miasma (smells) did not disappear entirely. The MOH reports are one of the richest available sources on local public health administration and patterns of disease.\nComputer programming can be used to perform tasks thousands of times faster than humans. In the Python code written to extract the data from the MOH reports, parallel processing was employed to speed up the running time of the program. Modern central processing units (CPUs) have multiple cores which allows the calculations to be run concurrently. In our project the CPU had four cores which allowed the running time of the program to be shortened by as much as three times. The next objective for the project is to scale up the size of the text-mining from 5,800 reports to over 70,000 reports covering the entire UK. In order to process such large datasets we are investigating using distributed computing resources such as Amazon Web Service (AWS). The code written for this project has been made open source under the MIT license along with documentation so that other programmers or researchers can use the codebase for use in their own text mining projects.\nAt the end of the Smelly London project the historical smell data will be available via 21st century [Smelly Maps] (https://www.goodcitylife.org/) by Daniele Quericia . This will allow the public and other researchers to compare smells in London from the 19th century to present day. Moreover, the Smelly London dataset will be available on the [Layers of London] (https://alpha.layersoflondon.org/the-map) project platform . This has the further potential benefit of engaging with the public.\n \nSmelly London project brings together historical data with modern digitisation and visualisation to give us a unique, revealing and visceral glimpse into a London of the past and what it tells us about London today. This is a collaborative, interdisciplinary project to demonstrate the capabilities of innovative text mining tools we design to facilitate new kinds of humanities research.", "Improving PySpark Performance: Spark performance beyond the JVM\n \nThis talk covers a number of important topics for making scalable Apache Spark programs - from RDD re-use to considerations for working with Key/Value data, why avoiding groupByKey is important and more. We also include Python specific considerations, like the difference between DataFrames and traditional RDDs with Python. Looking at Spark 2.0; we examine how to mix functional transformations with relational queries for performance using the new (to PySpark) Dataset API. We also explore some tricks to intermix Python and JVM code for cases where the performance overhead is too high.\n \nThis talk assumes you have a basic understanding of Spark (if not check out one of the intro videos on youtube - https://bit.ly/hkPySpark ) and takes us beyond the standard intro to explore what makes PySpark fast and how to best scale our PySpark jobs. If you are using Python and Spark together and want to get faster jobs - this is the talk for you.", "Estimating Residential Land Prices in the UK\n \nThe goal of this project is to produce aggregate and local residential land price indices for England and Wales, and their local authorities, starting from house sales data (data on actual land sales are very sparse and not readily available). We address a current gap in UK statistics: the Valuation Office Agency discontinued its land price indices in 2011. Data on land values are important: they provide builders with information on the profitability of their investments and offer the public sector an instrument to evaluate decisions on land releases.\nWe attach to each house transaction in the England and Wales Land Registry (LR) Price Paid Dataset an estimate of the building volume and the associated land area. These additional variables are gathered from two data sources: (1) Ordnance Survey MasterMap, which includes information on the footprint and height of all buildings in Great Britain, and (2) LR maps of freehold land parcels, which can be freely downloaded online for each local authority. After putting together our dataset with QGIS and R, we can decompose each sale price in LR Price Paid into a \u00e2\u0080\u009cstructure\u00e2\u0080\u009d and a \u00e2\u0080\u009cland\u00e2\u0080\u009d component using regression methods. We double check our results on structures using data on construction costs from the Office for National Statistics.\n \nWe address a gap in UK statistics by producing aggregate and local residential land price indices for England and Wales starting from house sales data from the Land Registry (LR) Price Paid Dataset. We decompose each sale price into a \u00e2\u0080\u009cstructure\u00e2\u0080\u009d and a \u00e2\u0080\u009cland\u00e2\u0080\u009d component using data on building volumes and land areas derived from Ordnance Survey MasterMap and LR maps of freehold land parcels. ", "Making packages and packaging \"just work\"\n \nEach language and each package has some things that it does better than others.  One of Python's major strengths is its ability to readily interface with many other languages and harness their strengths.  When it comes to speed, compiled, statically-typed languages are hard to beat.  Because of this, data science libraries often harness compiled shared libraries for their compute-intensive operations.  In a small environment with few of these dependencies, this often works quite well.  However, as the number of binary dependencies grows, the risk of incompatibility increases.\nThe Anaconda distribution and its package manager, Conda, have been very successful as platforms in part because the wide array of packages built for Anaconda by Continuum are built to be binary compatible.  Unfortunately, as more people and more communities build their own Conda packages that are not binary compatible with Anaconda or each other, more users have expressed confusion and consternation when things don't just work in all cases.\nTo address this issue, we have needed to develop tooling at both package build time, to better track and establish the actual limits of binary compatibility, and also at package install time, to allow conda to make better use of this information to inform users of what they can expect to work.  We\u2019ll talk about new features in our build tool, conda-build, that helps us choose our desired binary compatibility.  We\u2019ll also talk about our new automated build system that helps us flesh out a much larger range of compatibility by building more combinations of packages with limited binary compatibility.  Finally, we\u2019ll also talk about the extra data that you\u2019ll soon be able to provide to Conda, so that you will have more power to say exactly what you want.\n \nPython is a wonderful language, capable of gluing just about any number of other languages together, allowing us to reap the strengths of each language.  The cost of this is myriad opportunities for library and binary incompatibility.  This talk will cover some of the common ways that incompatibility arises, as well as several approaches that Continuum uses or is developing to reduce headache.", "An Algorithm of Style\n \nI will go into some detail about the types of algorithm and recommender system that have worked well, as well as those that have not! Interestingly we've found that some standard stalwarts of recommender systems have not thus far been a good fit for our data. I will describe some of the common pitfalls of deploying machine-learning systems in production (some of which we've avoided!), and emphasise the importance of using simple debuggable models when a stylist wants to know why The AI thinks it's a good idea to put a horizontal striped t-shirt on a larger gent.\n \nHow do eight stylists style half a million clients? At Thread we use machine-learning to help our stylists make personalised clothing recommendations to our users. In this talk I will give some insight into how we blend our stylists' expertise together with ML models that are continually learning from user feedback and sales data.", "Open Data Dashboards & Python Web Scraping\n \nThis talk will cover a basic scenario of curating open data into visualizations for an audience. The main goal is to automate data scraping/downloading and reshaping. I use python to automate data gathering, and Tableau and D3 as visualization tools -- but the process can be applied to numerous analytical/visualization suites. \nI'll discuss situations where a dashboard makes sense (and when one doesn't). I will make a case also that automation makes for a more seamless data gathering and updating process, but not always for smarter data analysis.\nSome python packages I'll cover for web scraping and downloading/reshaping open data include: openpyxl, pandas, xlsxwriter, and BeautifulSoup. I'll also touch on APIs.\nThe case study for this talk, and the source of the example data visualizations, will be the Economic Intelligence Dashboard (https://open.dc.gov/economic-intelligence/). This public dashboard is a curated collection of open data from disparate sources which, taken together, form a narrative of the economic health of DC. As my main field of focus is on economic data, throughout the talk I will touch on the best (most authoritative; easiest to scrape) sources of economic data and data on the District and the region. \n \nDistilling a world of data down to a few key indicators can be an effective way of keeping an audience informed, and this concept is at the heart of a good dashboard. This talk will cover a few methods of scraping and reshaping open data for dashboard visualization, to automate the boring stuff so you have more time and energy to focus on the analysis and content. ", "Finding needles in haystacks with Deep Neural Networks\n \nWe trained a network to perform multi-task classification. Using this as a basis, we trained a manifold that maps images to a space where duplicates have a small distance and different products have a large distance. \n \nWith an inventory of millions of products, finding duplicates is intractable by hand. Neural networks can be used to learn a representation that maps duplicates close together in a manifold. ", "Beginning Julia: Language and Landscape\n \nWhy is an alternative to Python needed for numerical work?  Data Science (clustering) and Machine Learning (neural network activation functions) provide two good reasons to look forward to Julia 1.0.  This talk is built on the just released version 0.6.\nCalculation, precision, storage, representation and graphing all need to be considered, especially at scale. Exponential and Matrix operations  are prone to underflow, overflow and and accumulated rounding errors could lead to  dire consequences.  For abritrary precision values, it may take longer to display or transmit the result than to calculate it.  Speed does matter when billions of flops are being done.  Julia's clever common sense is indeed a \"A fresh approach to technical computing.\"\nA very brief recap language and version  differences.  While the focus is Julia, some compare and contrast with Python is appropriate.  \nJulia's Type system, Just-in-Time, and Dispatch style are worth a good look. Graphing as well. GrElegance lurks above and under the hood.\n \nMath operations are discounted in the small where I/O dominates, yet gate compute costs in the large.  Be it square or root, exponential or matrix ops, time/space efficacy matters for science. engineering, and machine learning.  Julia provides right sizing for precision, accuracy, and performance.  Julia's Type system, Just-in-Time, and Dispatch are introduced via REPL, IJulia, Juno, JuliaBox", "Interactively Analyse 100GB of Data using Spark, Amazon EMR and Zeppelin\n \nYou may have been hearing a lot of buzz around Big Data, Apache Spark, Amazon Elastic Map Reduce (EMR) and Apache Zeppelin. What\u2019s the fuss about, and how can you benefit from these state of the art technologies?\nIn this highly interactive session, you will learn how to leverage Spark to rapidly mine a large real-world data set. We will conduct the analysis live entirely using an iPython Notebook to show you how easy it can be to get to grips with these technologies. \nIn the first part of the session, we will use a sample of data from the Open Library dataset, and you will learn how to apply common Spark patterns to extract insights and aggregate data. In the second part of the session, you will see how to leverage Spark on Amazon EMR to scale your data processing queries over a cluster of machines and interactively analyse a large data set (100GB) with a Zeppelin Notebook. Along the way you will learn gotchas as well as useful performance and monitoring tips.\n \nIn this highly interactive session, you will learn how to leverage Apache Spark, Amazon Elastic Map Reduce (EMR) and Apache Zeppelin to rapidly mine a large real-world data set. You will learn how to apply common Spark patterns to extract insights as well as learn useful performance and monitoring tips.", "Hierarchical Bayesian Modelling with PyMC3 and PySTAN\n \nOverview\nBayesian inference bridges the gap between white-box model introspection and black-box predictive performance. We gain the ability to fully specify a model and fit it to observed data according to our prior knowledge. Small datasets are handled well and the overall method and results are very intuitive: lending to both statistical insight and future prediction.\nThis talk will demonstrate the use of Bayesian inference in a real-world scenario: using a set of hierarchical models to compare exhaust emissions data from a set of vehicle manufacturers.\nThis will be interesting to people who work in the Type A side of data science, and will demonstrate usage of the tools as well as some theory.\nThe Frameworks\nPyMC3 and PySTAN are two of the leading frameworks for Bayesian inference in Python: offering concise model specification, MCMC sampling, and a growing amount of built-in conveniences for model validation, verification and prediction. \nPyMC3 is an iteration upon the prior PyMC2, and comprises a comprehensive package of symbolic statistical modelling syntax and very efficient gradient-based samplers using the Theano library (of deep-learning fame) for gradient computation. Of particular interest is that it includes the Non U-Turn Sampler (NUTS) developed recently by Hoffman & Gelman in 2014, which is only otherwise available in STAN.\nPySTAN is a wrapper around STAN, a major3 open-source framework for Bayesian inference developed by Gelman, Carpenter, Hoffman and many others. STAN also has HMC and NUTS samplers, and recently, Variational Inference - which is a very efficient way to approximate the joint probability distribution. Models are specified in a custom syntax and compiled to C++.\nThe Real-World Problem & Dataset\nI'm currently quite interested in road traffic and vehicle insurance, so I've dug into the UK VCA (Vehicle Type Approval) to find their Car Fuel and Emissions Information for August 2015. The raw dataset is available for direct download and is small but varied enough for our use here: roughly 2500 cars and 10 features inc hierarchies of car parent-manufacturer > manufacturer > model.\nI will investigate the car emissions data from the point-of-view of the Volkswagen Emissions Scandal which seems to have meaningfully damaged their sales. Perhaps we can find unusual results in the emissions data for Volkswagen.\n \nCan we use Bayesian inference to determine unusual car emissions test for Volkswagen? In this worked example, I'll demonstrate hierarchical linear regression using both PyMC3 and PySTAN, and compare the flexibility and modelling strengths of each framework.", "Scaling Scikit-Learn\n \nI worked as part of a team to create software which moves data to and from scikit-learn models running in AWS\u2019s EC2 service, and my talk will highlight some of the challenges we faced and the solutions we came up with. This project is possible because scikit-learn has a standardized API for all model types. No matter what algorithm you\u2019re using, it has the same methods with the same arguments as any other algorithm.\nData start and end either as tables in AWS\u2019s Redshift (a postgres database) or CSVs stored in AWS\u2019s S3 (a key-value store). The training data need to fit in memory, but we can make predictions on arbitrarily-large Redshift tables in roughly constant time, given a large enough pool of EC2 instances. The software and execution environment are packaged into Docker containers for reproducibility and speed in setting up on new EC2 instances.\nThe challenges on the training side are in massaging input data to match the formats which scikit-learn models expect and in storing enough metadata to ensure that we can reproduce the arrays of features at prediction time. Predictions distribute chunks of data to their own EC2 instances. I\u2019ll show off the custom backend for the joblib library that we use to manage the remote processes for predictions.\nOur software runs in Civis Analytics\u2019s data science platform. For the application described in this talk, the platform mediates interactions with AWS services to provide security and permissioning. The principles I\u2019ll discuss will be of general applicability to anyone interested in cloud-based production systems based on scikit-learn.\n \nWhat do you do if you have a lot of models to fit, don\u2019t want to spend all day with your laptop as a space heater, and have access to AWS? Take it to the cloud! I\u2019ll share my experience setting up a system to take models coded with scikit-learn and run them in a cloud computing environment. This talk will focus on training data that fit in memory and data for prediction which maybe doesn\u2019t.", "Finding the Right Articles - A Supervised Approach to Search\n \nThis talk describes a supervised algorithm for matching hosting-related questions to KB articles and tutorials from the Siteground website.\n-- Part One: Beating Google --\nStarting without any training data, we'll show how different similarity metrics and document representations can act as a decent search engine. We will describe\n how we built word2vec and LDA models, what Word Mover's Distance is and how we tuned it to match questions to KB articles.\nWe will then go into detail about mixing different similarity approaches to build a search engine which performs better than Google's custom site search on our dataset.\nWe'll also state briefly how we applied gearman to run searches in parallel.\n-- Part Two: Knowing The Right Answer --\nA shortcoming of any search engine is that it lacks the knowledge of right and wrong answer. In order not to bother clients with very similar but not helpful\n articles, we created a classifier to tell the difference. We'll describe the specifics of building a supervised model for our task, along with it's \nfeatures and instances.\n \nFinding the knowledge base articles which hold answers to a customer's question can be hard. This talk is about our approach and how it grew with the amout of training \ndata we collected. We'll describe several similarity measures, then move to manually-configurable ensemble search and eventually present a supervised model which \npicks the right answers from a set of search suggestions.", "Design Principles\n \nComing Soon!\n \nComing Soon!", "Twinkle twinkle little star, how I wonder what you are...\n \nIn my pervious life, I was an astronomer and one of the big tasks many PhD students face is manual star classification. But why ask a student to do what a machine should be able to do too? In this talk, I use the spectra (stellar flux vs wavelength) of identified stars to build a classifier which will detect the identity of the stars. Using a very simple non linear SVM, I achieve an 86% accuracy with my model. The next step is to use deep neural nets to achieve a better accuracy. \n \nIn my pervious life, I was an astronomer and one of the big tasks many PhD students face is manual star classification. But why ask a student to do what a machine should be able to achieve quicker? In this talk, I use the spectra (stellar flux vs wavelength) of identified stars to build a classifier which will detect the identity of the stars I am interested in.", "Interactive Data Analysis: Visualization and Beyond (McKinley & Livestream in Cascade)\n \nData analysis is a complex process with frequent shifts among data formats, tools and models, as well as between symbolic and visual thinking. How might the design of improved tools accelerate people's exploration and understanding of data? Covering both interactive demos and principles from academic research, my talk will examine how to craft a careful balance of interactive and automated methods, combining concepts from data visualization, machine learning, and computer systems to design novel interactive analysis tools.\n \nData analysis is a complex process with frequent shifts among data formats, tools and models, as well as between symbolic and visual thinking.", "Forecasting social inequality using agent-based modelling\n \nHow can we assess the future impact of changes to government policy? One tool that is gaining in popularity is agent-based modelling, in which a population of agents - typically representing individual people - is simulated together with an environment they can interact with. The agents' actions, which can in turn influence the other agents and the environment, are governed by a predetermined set of rules or heuristics. After much use in fields such as ecology and epidemiology these models are gaining increasing recognition in economics and policy, and were recently featured in the Bank of England's Quarterly Bulletin1. By running parallel simulations with modifications to the environment or the behavioural rules, a policy-maker can compare the likely outcomes of different options available to them.\nWe have used an agent-based model to assess the outcome of a change to the way in which wealth is inherited within families. The proposed change favours putting wealth into trust funds for grandchildren and great-grandchildren, instead of passing it directly to children. A similar proposal covering only houses was made last year by Gavin Barwell, the housing minister, but was not taken forward as government policy.\nWe developed a simulation of the demographic makeup of the UK, based on data from the census and the ONS, and inserted the different inheritance methods into it. We could then see what the long-term outcome of each scenario would be, in terms of the distribution of wealth and level of inequality in the country, allowing a quantitative assessment of its impact on individuals and society as a whole. The model also provides a base for developing assessments of more complex policies and interventions.\n\n\n\n\nhttps://www.bankofengland.co.uk/publications/Pages/quarterlybulletin/2016/q4/a2.aspx\u00a0\u21a9\n\n\n\n \nAgent-based models, which simulate an entire population of interacting people or other agents, offer a promising approach to assessing the future social impact of government policy. I will describe one such case, looking at a proposed change in how wealth is inherited.", "Logistic Regression: Behind the Scenes\n \nThe logistic regression model is a powerful linear model for predicting the log-odds of a binary or multi-class response. Empirically, carefully built logistic models perform well on many diverse tasks including survival analysis and classification problems.  Business requirements often impose statistical constraints on modelers (e.g. interpretability) making logistic regression a standard industry tool.\nMany organizations, including Capital One, are experimenting with the transition from SAS to open source tools such as Python; during this transition, modelers must confront a multitude of options for performing old tasks in new ways, including which package(s) to use for fitting logistic models.  This naturally raises many questions: are the same tweaks and options still available (e.g., offsets, Firth adjustment)?  Can we reproduce the exact same output and trust the results (e.g., p-values, coefficient estimates)?  How should a modeler navigate the different numerical implementations?\nIn this talk, we will take a deep dive into logistic regression, with a focus on performance and output comparisons between SAS and various Python packages.  We will also dig into the mathematical underpinnings of the implementations and discuss both the numerical and statistical implications (including Bayesian interpretations).  Understanding what's going on behind the scenes can lead to powerful insights and innovations, and hopefully will serve as inspiration for improved future models!\n \nLogistic Regression models are powerful tools in the data science toolkit; in this talk we will explore various implementations of logistic regression in Python and SAS, with a focus on output and performance.   We will also discuss both the numerical and statistical implications (including Bayesian interpretations) of the various options.", "Unconference Presentation \n \n\n \n", "JupyterLab+Real Time Collaboration\n \n.\n \n.", "Computational challenges of genome assembly, and how to beat them.\n \nGenome assembly is a cousin of string reconstruction, but the similarities stop very soon. Genomes are not random, they are  generated by evolution. This makes them simultaneously interesting and a pain to deal with. Sequencing data acquisition techniques vary, and evolve all the time, so you have to start from scratch every 3 or 4 years. DNA sequencing data production vastly outstrips Moore\u2019s law, and the nature of the genome sequence exasperates the explosion on many combinatorial methods, often presenting worst-case scenarios. Assembly algorithms are usually based on heuristics and are resource-hungry. Hence, the methods used are suboptimal, but they enable us to answer biological questions.\nHaving developed bioinformatics methods to enable us to analyse species\u2019 DNA, we move onto the question: how can we get better at big data processing? What genome assembly and bioinformatics show is that you not only do you need top notch computational skills, but actually knowing the nature of your data, its biases and how your results are going to be used allow you to slightly re-define problems in ways that make it more tractable.\nPart of the talk will focus on how to dramatically improve the performance of software without sacrificing the quality of the results. This will be relevant to anybody confronted with vast amounts of  error prone data, or legacy codebases which can no longer cope with the rate of data generation. In our case the performance depends on many factors; including the structure of the genome being assembled and the quality and quantity of sequencing data. One of our success stories is the open source assembly pipeline we use to cope with the massively big and repetitive bread wheat genome. We still need pretty big computers, but we now have  a stable codebase that produces robust results and has reduced RAM requirements from 9TB to 4TB, and runtimes from more than a month to around 10 days. This has taken over a year of intensive effort, which started with detailed profiling, and remains an ongoing task. The whole pipeline, which I will use as a case study in the talk, is open source and available at: https://github.com/bioinfologics/w2rap  and https://github.com/bioinfologics/w2rap-contigger and is used widely by genomic researchers. All the software we develop is open source, with either MIT or GPL licenses, and is available on our research team\u2019s github page: https://github.com/bioinfologics.\n \nI will give an introduction to genome assembly and the challenges it presents. I will include a brief synopsis of the journey from taking a few sample cells to producing a high standard 'reference genome' before moving onto the engineering challenges, with lessons applicable to all data intensive tasks.", "Sustainable scrapers\n \nA recent NPR project that collects structured data about gun sale listings from Armslist.com demonstrates several of the speaker's favorite tricks for writing simple, fast scrapers with Python.\nIn this session, we'll discuss:\n\nScraping legality and ethics\nUsing a model classes to encapsulate and test the scraper\nUsing simple controller scripts to scrape\nOptimizing efficiency using GNU Parallel\nUsing Amazon Elastic Cloud Compute to really fly\nHow frameworks like Scrapy can help (and why knowing what's going on beneath the hood is always helpful)\nAdvanced issues and techniques: Continual scraping, inferred data, caching, and more \n\n \nScraping data from the web is an essential skill, whether you want to or not. Learn the code and systems tricks that go into testable, fast, low-maintenance scraping gleaned from years of real world practice.", "Deep Learning for QSAR\n \nThe phrase \"Deep learning\" has gained buzzword status in recent years.  Regardless of whether the unprecedented hype surrounding technique is well placed, it has established itself as state of the art in many fields, most notably in image and speech recognition and natural language processing, where tech giants such as Google, Facebook, Baidu, Microsoft and Apple have invested billions of dollars in driving the technology forward.  \nLess well known is that it has been shown to be state of the art for Quantitative Structure Activity Relationship (QSAR) problems, including winning the Kaggle Merck Molecular Activity Challenge, and the recent Tox21 toxicity challenge.  These problems aim to predict, given only the chemical structure of a compound, how the application of that compound would affect the activity of a biological system.\nThis talk will cover research undertaken as part of my PhD.  I will cover:\n\ndata formatting and preprocessing using sqlalchemy and pandas\nfeature extraction from a chemical structure using the RDKit cheminformatics toolkit\ntraining of models using Theano and Keras.\nresults and comparison to other techniques\n\n \nThis talk details techniques used to train state of the art neural networks on chemical data, from data processing to predictive screening, all in Python!", "We came, we saw, we hacked. How to win a Big Data hackathon\n \nNowadays the main purpose of the hackathons is about using data science, programming and statistical analysis to solve Big Data challenges in order to deliver valuable business insights for leaders to take decision of important matters. At the essence of it, a good hack must be technically impressive and includes the theme as part of the project criteria. In this talk, we will discuss three aspects of a Big Data hackathon project.\n1)  Framework of your project: How to select your data set? What are the tools and technologies that you need to know for your project? How to come up with a hack that provides a solution to the theme/business question?\n2)  How to build your Data Science/Analysis model your self by providing two examples of structured and unstructured data set cases.\n3)  Finally, don\u2019t let 3 minutes of stage time undo all the hard work & sleepness nights! We will give you a guidance of what is really important for your speech.\n \nHackathon prize money keeps rising each year as organizations learn to take advantage of their value. Runner-up cash prizes are now in the thousands. Hackathons are about bringing ideas to life in a very short time frame and pitches are typically short, 2-3 minutes. In this talk, we will walk through two hackathon examples using structured and unstructured data sets.", "Dimension Reduction and Extracting Topics - A Gentle Introduction\n \nText mining and natural language processing are hugely powerful fields that can unlock insights into the vast amounts of human knowledge, creativity and drivel (!) for automated computing. Examples include the fun of highlighting trends in internet chatter through to more serious analysis of finding patterns and links in leaked data sets of public interest.\nOne key tool is to reduce the many dimensions of text data, and distill out the key themes in that text. People call this topic modelling, latent semantic analysis, and a few other names too. The powerful method at the heart of this is called singular value decomposition (SVD).\nThis talk will gently introduce singular valued decomposition (SVD), explaining the mathematics in an accessible manner, and demonstrate how it can be used, using the Chilcot Iraq Report as an example dataset.\nExample code, notebooks and data sets are public on GitHub, and there is a blog for more discussion of this, and other text mining ideas https://makeyourowntextminingtoolkit.blogspot.co.uk\n \nText mining has many powerful methods for unlocking insights into the messy, ambiguous, but interesting text created by people.\nSingular value decomposition (SVD) is a useful method for reducing the many dimensions of text data, and distill out key themes in that text - called topic modelling or latent semantic analysis.\nThis talk for beginners will gently explain SVD and how to use it.", " Dev Ops meets Data Science: Taking models from prototype to production with Docker and Kubernetes\n \nThe chasm between data science and dev ops is often wide and impenetrable, but the two fields have more in common than meets the eye.  Every data scientist will be able to lean in and help their career by investing in a basic understanding the basic principles of dev ops. In this talk I present the notions of service level indicators, objectives, and agreements. I cover the rigorous monitoring and testing of services. Finally we demonstrate how to build a basic data science workflow and push to production level APIs with Docker and Kubernetes. \nKubernetes is an opinionated container cluster manager with an easy to use, robust interface. It can be use on very small and very large clusters. Docker is a container system that allows one to build code in an isolated environment. Paired with a container manager such as Kubernetes we are able to manage millions of instances as needed for a production deployment. These tools are two of many different options but are considered among the best open source solutions available.\n \nWe present the evolution of a model to a production API that can scale to large e-commerce needs. On the journey we discuss metrics of success and how to use the Kubernetes cluster manager and associated tools for deploy. In addition to the use of these tools we highlight how to make use of the cluster management system for further testing and experimentation with your models.", "Julia for Data Analysis: features, interfaces and future directions\n \nJulia has a rapidly developing ecosystem of packages for data analysis. This talk will give an introduction to some of these, such as Distributions.jl, DataFrames.jl and Gadfly.jl. We will also demonstrate how to leverage existing libraries in Python and R, via the PyCall.jl and RCall.jl packages. Finally, we will describe the future plans, funded by a Moore Foundation grant.\n \nThis will showcase the data analysis features of Julia, a new high-performance, dynamic language for technical computing. We will give an overview of the various data, statistics and graphics libraries, interfaces with Python and R, as well as outline future directions for this new and exciting language.", "Bokeh and Friends\n \nAs Bokeh rapidly approaches a 1.0 release, the project focus is shifting towards making Bokeh a solid, stable, minimal (but extensible) platform for interactive web-based visualization. This is great news for users, but also means that developers of higher-level or domain-specific tools can build upon it with confidence.\nThis talk will briefly discuss some of the history of the project leading up to the present, including lessons learned about OSS development. After this quick status update, I will discuss and demonstrate two tools that are already building on and integrating with Bokeh:\n\n\nDatashader is a sophisticated Python rendering pipeline that can be used together with Bokeh to effectively visualize large (billion+ point) data sets quickly.\n\n\nHoloviews is a very high level data language for slicing and dicing datasets that can automatically build interactive visualizations using Bokeh, based on the structure of your data.\n\n\n \nAs Bokeh rapidly approaches a 1.0 release, the project focus is shifting towards making Bokeh a solid, stable, minimal (but extensible) platform for interactive web-based visualization. This talk will briefly discuss some of the history of the project leading up to the present, including lessons learned about OSS development.", "Bot or Not? The Illusion of Intelligence  \n \nChatbots are programs that employ rules or artificial intelligence that users interact with via a dialogue interface. Chatbots are extremely useful tools for accomplishing a range of tasks. From ordering pizza to checking the weather to buying shoes, there's a chatbot for that. This bot revolution has led to the creation of several Make-A-Bot services (all with really cool .ai domains!), but these bots are not extremely powerful examples of artificial intelligence. Most are simple template-based implementations that can be developed independently in Python. \nThis talk will provide an overview on the structure of chatbots and how tools such as SpaCy, NLTK, sklearn, and Keras can be used to develop bots ranging from a basic weather bot to a more sophisticated deep learning neural conversation model. The applicability of these tools will be demonstrated through MalBot, a chatbot capable of ingesting a piece of malware and responding to natural language inquiries into the malicious software\u00e2\u0080\u0099s capabilities. (e.g. \"Do you record key strokes?\", \"What malware family are you from?\")\nAttendees will gain a better understanding of the foundations of chatbot development and the Python tools necessary to successfully build and deploy their own bot.\n \nChatbots have become wildly popular interfaces for online services we use everyday. While these bots are not going to pass the Turing Test, they provide a platform to use natural language to perform a task. This talk covers the basics of chatbot development using Python NLP and Deep Learning libraries and provides a demonstration of Malbot, a simple bot that can converse about a piece of malware.", "Detecting novel anomalies in Twitter\n \n We are overwhelmed by the amount of information available via news, blogs, social media, etc. and there is growing demand for automatic or semi-automatic systems that filter information and surface the most relevant, interesting, novel pieces to the users. Finance professionals face multiple challenges as the volume of data they are analysing increases and at the same time the data sources get more diverse. In order to address the information overload problem both established financial news providers and social media services present condensed information: in the form of news headlines (e.g. Bloomberg and Reuters) or by imposing limits on the text length (a Twitter message is formed of maximum 140 characters). On the other hand, in recent years, an increasing number of content filtering and alerting systems have been developed. \n Knowsis is a fintech startup focusing on social media analytics. One of the systems that we developed - Anomaly Detection and Alerting - is aimed at informing the user whenever it identifies an unexpected and novel tweet. Anomaly detection is driven by social volume, i.e. not all tweets which deviate from what is expected are anomalies, but a critical number of tweets is required. In addition, we do not want to alert the user about old information. \n The talk will present the approach that we took in building the system, highlighting issues that we encountered along the way and how we tackled them. In the second part of the talk we are going to show how one can build a similar anomaly detection system from scratch and use it to detect a particular anomalous scenario. We are going to use Poisson models to identify anomalies and Locality-Sensitive Hashing for novelty detection. The code will be written in Python using the scikit-learn and statsmodels libraries and made available to the audience. \n \nIn recent years Twitter has gained importance as a datasource for finance professionals alongside news. However, several challenges appear when identifying relevant information and finding the latest unexpected developments. In this talk we are going to present our experience developing an anomaly detection and alerting system and analyse a particular anomalous scenario. ", "Machine Learning Infrastructure at Stripe: Bridging from Python -> JVM\n \nThe Stripe Machine Learning Infrastructure team exists to help engineers, data scientists, and analysts at Stripe develop and ship models to production. They own and operate the primary service that provides an API for scoring models for applications such as fraud and NLP, and are always looking for ways to help internal Stripe customers ship ML for new applications or model types. \nML models at Stripe are trained and evaluated in Python, with scikit-learn as an integral piece in our pipeline. However, the primary scoring service is written in Scala, which presents us with a problem: how do we serialize and export models from Python to the JVM? This talk will discuss our serialization framework for serializing and packaging machine learning components; by the end you will learn how we export models, transformers, encoders, and pipelines from the world of scikit to that of our Scala service. \nWe'll then cover what happens after the model has been loaded by our Scala service, namely how we name models uniquely and use metadata we call \"tags\" to keep track of what model is currently running in production, history of production models, etc. We'll discuss how we score candidate models in parallel to the production model to evaluate them for promotion to production. \nBy the end of the talk you should have a clear idea of how we serialize, package, promote, and evaluate candidate models across the entire machine learning infra stack, from the start of training in Python to the final scoring in Scala.\n \nMachine learning at Stripe has a foundation built on Python and the PyData stack, with scikit-learn and pandas continuing to be core components of an ML pipeline that feeds a production system written in Scala. This talk will cover the ML Infra team\u2019s work to bridge the serialization and scoring gap between Python and the JVM, as well as how ML Engineers ship models to production.", "GraphGen: Conducting Graph Analytics over Relational Databases\n \nAnalyzing interconnection structures among underlying entities or objects in a dataset through the use of graph analytics (network science) has been shown to provide tremendous value in many application domains. However, graphs are not the primary representation choice for storing most data today. Individuals and businesses instead tend to choose more general purpose storage systems, the most popular being relational databases, often preferred for the strong integrity and consistency guarantees they provide. In these cases, users are therefore forced to manually extract data from their data stores, construct the requisite graphs, and then load them into a graph engine (like NetworkX) to execute their graph analysis task. Moreover, users may not know exactly which graphs in their dataset they would like to analyze, or they may be unsure about whether a particular graph would be useful or not. This process is not only tedious and cumbersome, but also computationally intensive and time-consuming.\nWe present a system called GraphGen, geared towards making the extraction of graphs from relational databases effortless as well as enabling the visual exploration of the wide variety of inherent or hidden graphs inside structured relational datasets. GraphGen consists of a Web application for visual exploration of the potential graphs, as well as a Python library called GraphGenPy for using our intuitive domain specific language (DSL) to declare graph extraction tasks, that are automatically executed over a relational database. In this talk, I will discuss the challenges in achieving the above goals as well as provide an overview of the end-to-end process using GraphGenPy and NetworkX for leveraging graph analysis over a relational database schema.\n \nApplying graph analytics on data stored in relational databases can provide tremendous value in many application domains. We discuss the importance of leveraging these analyses, and the challenges in enabling them. We present a tool, called GraphGen, that allows users to visually explore, and rapidly analyze (using NetworkX) different graph structures present in their databases.", "Python flying at 40,000 feet\n \nIntroduction\nIn this talk we present data exploration and machine learning algorithms applied to aircraft  black-box data. We believe that safety algorithms have to be open source so everybody could apply them to increase flight safety: it is all about using Python to fly safer.\nData\nWe shall firstly briefly explain how the black boxes data is downloaded from an aircraft upon landing. Shortly after that, a Python tool to analyse the flight data is launched (you can find it in the open source repo FlightDataAnalyzer of Flight Data Services). To be able to analyse the data properly and to give context to it, a flight has to be segmented into taxi-out, take-off roll, take-off, climb, cruise, holding, descent, landing, touchdown, decelerating and taxi-in phases. A set of sophisticated deterministic and probabilistic algorithms is applied to the data to detect key-points of a flight, like take-off and touchdown points, and values of flight parameters at those points. The detection of these points and the calculation of the associated values of parameters (like acceleration at touchdown) are difficult problems which we\u00e2\u0080\u0099ll share and try to solve with the help of the audience.\nWe plan to present a simplified version (due to short time available for presentations) of detecting the end-point of take-off phase as an important datum for further flight analysis.\nWe want to stress about the importance of having a large number and good-quality data. Even with a not so sophisticated algorithm, one can get great results if he/she has a good dataset at hand. We have 4,000,000 flights in the database that grows daily and that enables us to give better predictions. We want to go farther by promoting flight data-sharing concepts between different airlines so we get better statistics on relevant parameters - everything for the sake of flight safety!\nAlgorithms and tools\nThe detection of the switching-phase points is very challenging since the parameters vary very much amongst aircraft types. It is even a challenge for an experienced pilot when he looks at the data! However, we can learn from the data across multiple flights first and this is where Bayesian logic gets in the play. Assuming the independence between the parameters (for the sake of simplicity), we obtain simple formulas to calculate the posterior probability for the end-of-take-off point. This assumption, though very strong, gives great initial results. Furthermore, sometimes a sensor on one engine stops working (not the engine itself! :). In this case we can just omit this signal and use what we are left with. Again, the results are more than acceptable.\nWe will show how we use R for data exploration and Jupyter-notebooks (Python) for prototyping and how these two are complimentary. The fine-tuned algorithms are integrated in the system by using Python, exclusively. We make heavy use of third party open source libraries such as numpy, scipy, pandas, pymc, etc..\n \nHave you ever seen real black-boxes' data? For safety, airlines worldwide are obliged to monitor their flights' data - the same that is stored in aircraft black boxes.  Look how we do it with Python and open source technologies at hand. It's never easy with these multivariate time-series - they can be incomplete and contain varying number of flight parameters - we'll deal with it!", "BrainDrain: Using Machine Learning and Brain Waves to Detect Errors in Human Problem Solving\n \nAs more medical devices become cheaper and readily available to the public, they can be used in everyday life. The Muse Headband is an EEG machine that provides realtime\nmeasurements of brain waves. This talk will discuss the process of gathering and cleaning the data from the headband, using Keras to develop a model, and creating a basic real-time feedback end user program.\n \nThe Muse Headband is a simple to use EEG machine. Using this headband, a python model was developed using big data and machine learning techniques to interpret brain wave patterns to create a feedback system that helps the user understand their cognitive thinking while solving a problem.", "H2O Deep Water with Python early sneek \n \n\n \nPython as a language for DeepLearning. Python is emerging as the facto language to specify Deep Learning Networks. In this talk we will explore some of the popular libraries like Tensorflow and Keras to see the semantics used to describe such networks and look a bit more under the hood at what is the python layer actually doing for these well known deep learning libraries.   ", "Irregular time series and how to whip them\n \nIrregular time series and how to whip them\nHistory of irregular time series\nStatisticians have long grappled with what to do in the case of missing data, and missing data in a time series is a special, but very common, case of the general problem of missing data. Luckily, irregular time series offer more information and more promising techniques than simple guesswork and rules of thumb.\nYour best options\nI'll discuss best-practices for irregular time series, emphasizing in particular early-stage decision making driven by data and the purpose of a particular analysis. I'll also highlight best-Python practices and state of the art frameworks that correspond to statistical best practices.\nIn particular I'll cover the following topics:\n\nVisualizing irregular time series\nDrawing inferences from patterns of missing data\nCorrelation techniques for irregular time series\nCausal analysis for irregular time series\n\n \nThis talk will present best-practices and most commonly used methods for dealing with irregular time series. Though we'd all like data to come at regular and reliable intervals, the reality is that most time series data doesn't come this way. Fortunately, there is a long-standing theoretical framework for knowing what does and doesn't make sense for corralling this irregular data.", "Writing a Book in Jupyter Notebooks\n \nI will describe an on-going project to write a book exploring mathematical and computational aspects of wave propagation problems.  Jointly with David Ketcheson and Mauricio del Razo, we are developing the book as a series of Jupyter notebooks that include Python code for the reader to experiment with, and that employ interactive widgets and animation tools to provide a more active experience for students or researchers grappling with this theory.  We are still experimenting with the best approach to developing material that works well in the notebook and also translates well to static webpages and hardcopy, with the hope of expanding its accessibility and usefulness.  The current state of the project can be found in the Github repository https://github.com/clawpack/riemann_book.\n \nI will describe an on-going project to write a book exploring mathematical and computational aspects of wave propagation problems.", "What's new in High Performance Python?\n \nScaling up the performance of Python applications without having to resort to writing the performance-critical sections in native code can be challenging. This process of performance optimisation has three parts: determining which parts of an application are performance\ncritical; understanding their performance; and acting on that understanding to implement optimisations. This talk is about the latest developments and tools for all these aspects of High Performance Python.\nThe Python profiler and Kernprof provide some explanation of the performance of Python programs; this talk will demonstrate the Accelerate Profiler, which is Numpy-aware and builds on the standard profiling tools by capturing the flow of arrays throughout program\nexecution. Interactive experimentation and exploration of captured profile information is done in an IPython notebook, simplifying the performance optimisation workflow.\nThe second part of this talk is about Numba, a tool that compiles user-selected Python functions to native code at runtime, for performance boosts of up to 100x for particular samples of code. Numba is in continuous development, with many additions made over the past\nyear, making it applicable for optimising an ever-increasing selection of performance-critical code. We will introduce and demonstrate  applications of recent additions including JIT classes, parallel Multicore/CUDA ufuncs and gufuncs, the @generate_jit decorator (similar to generated functions in Julia) and support for CFFI. The combination of Numba and CFFI is particularly powerful, enabling seamless integration of optimised native code into High Performance Python code.\n \nA tour of recent tool developments for understanding and optimising the performance of Python code, covering profiling tools, Numba, and CFFI. The talk will include demonstrations of these tools and their features using motivating examples.", "Upgrading Legacy Projects: Lessons Learned\n \nHave a finicky legacy system you just can't seem to upgrade? Have you upgraded a tool just to watch it sink back into decay, despite your best intentions? We've been there, made it through to the other side, and want to share what we learned! Come learn about software rot, how to avoid it, how to reverse its effects, and what to do when you can't. This talk will include technical lessons (e.g., testing strategies, designing upgrade paths) and non-technical lessons (e.g., empathize with the people who wrote the code, understand the processes that lead to the current situation).\n \nStuck with a legacy system? Get unstuck! Learn how to design an upgrade path, overcome common obstacles, and reduce the risk of code rot in the future.", "Exposing Algorithms \n \nAn algorithm is set of steps that perform calculations, process data, or automate tasks. Algorithms are everywhere we look (and even places we don\u00e2\u0080\u0099t look) controlling what we see, do, and where we go. They\u00e2\u0080\u0099re great for solving our problems and helping us make better and quicker decisions, or taking the decision-making out of our hands. Their guidance is perfect in their objective and unbiased calculation. Except they are not, actually. Like everything else, they are created by people, and people have biases that get encoded into the algorithms they create. Algorithms learn from data, which is also created by people, so the algorithms also learn biases from data. This can be a problem when algorithms encode these biases into their calculations and go on to perpetuate the bias. \nIn this talk you will hear why we should care about algorithmic accountability, and details on a case study on how computational journalism can be used to investigate algorithms and advocate the need for transparency and accountability.\n \nAlgorithms have become an integral part of our everyday lives. While algorithms can make our lives simpler and make decisions faster, there is a growing need for algorithms to be transparent and for the users of those algorithms to be accountable for the automated decisions made by them. This talk covers where algorithms are used, how they can go wrong, and how they can be investigated.", "Word Embeddings for fun and profit in Gensim\n \nA tour of  word embeddings, their Python implementations and their use in the industry. \nWe will start with theory and academic results for word2vec, glove, swivel and Word Movers Distance.\nThen proceed to their Python open source implementations mainly in the Gensim package\n \nPython has great open source libraries to extract data from its most raw format - the human readable text. We will discuss a family of algorithms called word embeddings - Word2Vec being most famous and how they can be used in practice using Gensim package", "Robust Algorithms for Machine Learning\n \nRobust algorithms are under-appreciated, particularly by people new to data analysis. This talk will review the basic idea of robust or non-parametric algorithms and look at some of the more important named algorithms, as well as looking at how to apply the philosophy of robustness to any problem.\n \nMany companies implementing Machine Learning (ML) have learned that noise and other errors in the data set can cause stability issues resulting in time loss and headache.", "KEYNOTE: Picasso's terminal; data science and AI in the visual arts\n \nOver the past several years, two trends in machine learning have converged to pique the curiosity of artists working with code: the proliferation of powerful open source deep learning frameworks like TensorFlow and Torch, and the emergence of data-intensive generative models for hallucinating images, sounds, and text as though they came from the oeuvre of Shakespeare, Picasso, or just a gigantic database of digitized cats. This talk will review these developments and offer a set of interdisciplinary tools and learning resources for artists and data scientists alike, if ever there was a difference to begin with.\n \nA talk about the flourishing intersection between machine learning and art, a survey of recent works emerging from it, and a primer on how to get started with it for seasoned developers and newcomers alike.", "Guillotina - An Async REST Resource DB to manage millions of objects\n \nOne of the most common problems on my daily basis data management is a way to store and secure objects with information in a transactional scalable infrastructure. Having an async API that holds security, triggers and transactional operations allows to provide an interface for managing the data that will be processed by the machine learning engines on batch operations. Including async queues of operations allows to provide model inference on the objects on real time operations on objects on the stack. \nAfter 15 years on Plone/Zope framework we started to design a new concept using all the lessons learned on content management to provide a CRUD  storage layer for big data objects.\n \nGuillotina is a new framework designed to be horizontal scalable, build on top of aiohttp and offering a Resource REST API to store, read and query object oriented datasets with millions of elements. Based on JSON schema types allows to define a Traversal API with strong security and granularity. With an abstracted storage layer its possible to provide support for Couch, Cassandra and Postgres.", "Deep Learning for detection on a phone: how to stay sane and build a pipeline you can trust\n \nDeep Learning has gone through the hype phase where it seemed like a skeleton key, followed by a phase of despair for many who found the building blocks to be too esoteric and the training code and process too unreliable. Deploying on a device with strong hardware limitations adds that extra spice to the mix.\nThis talk addresses a very specific use case: preparing a Deep Neural Network to be used for detection in real time in a mobile phone app. It is meant for hands-on engineers and data scientists who live in that area where writing scalable and testable code is every bit as important (and troublesome!) as understanding your loss function.\nWe will cover different steps of the process, such as:\n Defining a good model for you:\n  * Your device: it is what it is.\n  * Cargo cult or what is this layer doing and do we need it?\n Gathering the right training data: who, where and how will be using your app?\n Training:\n  * Transfer learning as a small company's best friend.\n  * Integrating different sources in your data pipeline.\n Evaluating earlier rather than later.\nDepending on time and interest, we may also go over data augmentation and/or model persistence.\n \nDeploying a deep model on a mobile device to be used for real-time detection is not quite trivial yet. Defining your Deep Learning architecture, gathering the right data, designing your training process, evaluating your models and turning this into a pipeline that keeps everyone on the team (somewhat) sane - these all have their pitfalls.", "Keynote: Become a Data Superhero: How Data Can Change the World\n \n.\n \nThe capacity to gather and interpret data can be low for many nonprofits. Working together, data scientists and organizations can be a world-changing combination. Byte Back has found a way to use data analysis for good and will help you learn how to tap into your own superpowers. ", "Making Sense Out of Flight Test Data with Python\n \nBackground\nIn the military aviation development and test process, it is common to record all of the digital communication that occurs between the myriad of on-aircraft systems (computers, data link radios, sensors, etc.). One of our main research areas is sensor fusion whereby on-aircraft, real-time software compares and fuses tracks from all of the onboard sensors as well as those reported by other aircraft in the area. Track association is one component of sensor fusion. By association we are referring to the decision made by the fusion engine as to whether two or more track reports represent the same physical entity (\u00e2\u0080\u009ctrack matching\"). Track reports may be provided by the same sensor, different sensors on a single aircraft, or multiple sensors across multiple aircraft. Assessing the performance of the sensor fusion algorithms requires an aggregate analysis across a large set of flight test data both from a mission perspective (single mission, but looking at data from multiple aircraft) and across multiple missions (evaluating performance over time and across a wide range of test conditions). This talk will focus on the post-flight assessment process related to track association decisions. \nData Preparation\nIn order to analyze and compare tracks (time series reports on the position and movement of a physical entity) it is necessary to address any time alignment issues, perform coordinate transformations to get all tracks into a common coordinate frame, and store the data in a format that facilitates aggregate analysis. The time alignment related challenges stem from the fact that the data is recorded on multiple asynchronous digital interfaces. Also, time synchronization between aircraft is not robust and can require time shifting some of the recordings.\u00c2\u00a0Prior to ingesting the data, the track reports vary widely in the coordinate frame used. Some sensors only report in a single dimension (e.g. azimuth only), others in two (e.g. azimuth and elevation), and others in three (e.g. azimuth, elevation, and range or latitude, longitude and altitude). Some of the sensors report the tracks kinematic information in addition to its position which can also vary by coordinate frame. We have found pandas\u00e2\u0080\u0099 powerful data manipulation capabilities and its extensive time series data support to be very effective in both preparing the data for analysis as well as the analysis itself.\nStatistical Comparison\nOnce the tracks are on the same time scale and in the same coordinate frame, we can compare tracks to each other (either from the same aircraft or different aircraft) and compare them to \u00e2\u0080\u009ctruth\u00e2\u0080\u009d (the best state estimates available for the relevant physical entities in the airspace). In addition to the track matching analysis, we analyze sensor error over time and as software and hardware configurations change. This talk will discuss this analysis process, its challenges, and how we leverage Python and pandas to get the job done.\u00c2\u00a0\nVisualization\nA key component to our analysis is\u00c2\u00a0visualization. We need to be able to explore the data visually and we need to effectively communicate the results. Towards\u00c2\u00a0this end, we utilize a mixture of matplotlib,\u00c2\u00a0PyQtGraph, and\u00c2\u00a0Bokeh. We will discuss how and why we use these tools along with new methodologies and frameworks we are investigating.\u00c2\u00a0\n \nThe assessment of complex algorithms like sensor fusion requires an aggregate analysis across a large heterogeneous data set that represents the possible operating conditions. This talk will discuss the process and tools we use to analyze this data, where we want to take things, hurdles we have left to overcome, lessons we have learned along the way, and best practices we can recommend.\u00c2\u00a0", "Finding Driving-Style Patterns in Caterpillar Machine Data\n \nCaterpillar earth-moving machines are large electro-hydraulic-mechanical systems that use embedded control units to regulate the performance of the engine, upper drivetrain, lower drivetrain, hydraulics, and other systems.  These systems work seamlessly with the operator's inputs to move the machine and perform work such as digging, loading, dozing, hauling, etc.  The function of the machine is managed by very complex embedded software which allow for a range of operator environments and styles while continuously attempting to optimize machine productivity, fuel usage, and operator comfort.  \nThe design and testing of the integrated powertrain system (IPS) control software is therefore critical to proper performance of the engine and transmission across a wide variety of operation and driving styles.  Caterpillar IPS control software has historically been validated by manually selecting cycles from limited test data.  Consequently the risk that the software may not be optimal is higher than desired since the sample size is small.  \nThe advent of relatively cheap, integrated data-loggers that can record many parameters at high sampling frequencies has greatly increased the amount and variety of recorded operator styles and customer applications.  There is now ample data from which to sample.  However, in order to avoid sample bias, we desire that extracted time histories only be drawn from predominant patterns.  \nThe data is so large that manually looking for patterns would be exorbitantly time-consuming.  A programmatic approach is needed to reduce and categorize the data.  However, simple reduction of data into summary statistics does not preserve the time-dependence of patterns (gear shifting, engine acceleration or deceleration).  Nor does attempting to match waveforms exactly since the patterns can be time-dilated or time-reordered.\nTherefore, we present a method to divide the data into time-period sessions and pull out specially-engineered features that can be subsequently mined for patterns.  This involves domain expertise as well as programming knowledge.  Python is used to create the features (numpy, scipy, pandas), and the feature space is grouped into patterns through dimensionality reduction and unsupervised learning (scikit-learn).  The scalability problem is addressed by building a functional programming pipeline on a distributed task queue (pytoolz, celery).\nWe will discuss this method and (if time permits) some challenges we face related to data storage, data access, distributed computing, function serialization, workflow design, and analytical modeling.  We will conclude with a reflection on the value to controls development that this method realizes, what works well, and where the process can be improved.\n \nIdentifying predominant driving-style patterns in logged time series data of Caterpillar machines is daunting due to the nature and size of the data.  However, insight gained from field data can deliver optimized powertrain control software and better machine performance.  A solution for finding patterns was built using engineered features, dimensionality reduction, and unsupervised learning.", "Using Spark -- With PySpark\n \nSpark is one of the most popular Big Data frameworks. It is a great choice when you need to scale up your \ndata science jobs. It is written in Java/Scala but also supports other languages like R and Python. \nPySpark is the API to use Spark from Python. With Spark DataFrames the overhead of calling Spark from Python is claimed to be near zero compared to Java/Scala. Spark development is supported by a number of notebooks, the Spark Notebook, the IPython and Jupyter notebooks, and the new Apache Zeppelin notebooks. Are you interested already? \nIn this tutorial we show you how to get started with Spark. We provide a small cluster for you so that we don't loose time with installing servers. Then we use some publicly available data set and show you how to use this data in Spark. We will use machine learning algorithms to analyze the data and make predictions. Results will be visualized in the notebooks. You will be able to see what performance difference it makes when you run your code on a cluster instead of a single node.\nParticipants should bring their own laptop with a web browser installed and be able to access Wifi. All coding will be done in Jupyter notebooks on the cluster we provide. At the end of the tutorial you can export all your notebooks to take them with you.\nPlease download Docker and our Docker container before the workshop. On Mac and Windows install Docker Toolbox, on Linux install Docker Engine (www.docker.com). Then download the Docker image we have prepared for the workshop https://hub.docker.com/r/gerhardt/pyspark-workshop/. The Docker image contains Python, Spark, PySpark and Jupyter. Overall your downloads will be around 1 GB.\n \nSpark is one of the most popular Big Data frameworks for scaling up your tasks in a cluster. Although it is written in Java/Scala it supports Python via PySpark. We show you how to get started with Spark. We provide a small cluster of compute nodes to work on. You will use some publicly available data set to run machine learning algorithms on and show results in a Jupyter notebook.  ", "Using Support Vector Machines in Scikit-Learn to discover genetic aetiologies in Schizophrenia\n \nUsing Support Vector Machines to gain insights into the genetic aetiology of Schizophrenia\nBackground\nSchizophrenia is a debilitating psychiatric disorder which affects approximately 1% of the general population. Results from twin studies suggest that genetic factors account for 80% of the variation of the disorder. However, to date, these have not been fully identified. Traditional methods involve performing a Genome Wide Association Study (GWAS), to find estimates of association that different mutations in DNA have with the disease.\nSchizophrenia is a highly polygenic disorder, meaning that it arises from a combination of genetic mutations, all contributing a small level of effect. These studies require very large sample sizes to have the statistical power to identify mutations of interest.\nThe common approach is to combine this information into a single polygenic risk score for an individual, which can then be assessed for its predictive power to identify cases of the disease using a single feature Logistic Regression model.\nThe machine learning approach\nInstead of creating this single score, different features from the individual association scores were created for use as inputs to machine learning algorithms. These features were either the information from the individual mutations themselves, or collections of mutations in genes which are known to be involved in various functional networks.\nThe algorithm chosen was the Support Vector Machine (SVM). This was for a number of reasons:\n\nThey can cope with a large number of input features, and can apply penalty procedures such as L1 regression to identify important features.\nKernel methods can be used to find evidence for interactions between the features - something that is lacking in the traditional methods.\n\nPython libraries used\nThis work was made possible by the use of two seminal libraries for Python: Pandas and scikit-learn. In addition the Joblib modules were used to carry out some simple parallel processing tasks on an HPC cluster if the desired information was not already provided by the modules in scikit-learn which already have these built in.\nPandas made collecting information from sub-sets of the data very quick and efficient to carry out. Examples of this functionality will be given in the talk.\nResults so far\nWhile not improving on predictive power, the models did provide a rich insight into the association that different gene-networks have with the disorder, and identified genes which are targets of the Fragile X Mental Retardation Protein (FMRP) supporting findings from recent research in molecular biology. The information showing this were the coefficients assigned to the features seen in the boxplot provided. This was created by building multiple models, each with different train/test splits of the data to get the distributions of the coefficients. As can be seen, those assigned to the FMRP feature are consistently higher.\nThe findings also show evidence for pair wise interactions between single point mutations. A series of procedures were carried out to simulate positive cases based on differing levels of contributions from main effects and interactions. This showed that without the contribution of the interactions, the results seen in the real world dataset by the kernel based models would not have been possible. Full details of how this was carried out will be discussed.\n \nThis study uses machine learning methods in scikit-learn to find insights into the genetic aetiology of schizophrenia. It expands on the method of using a single genetic risk score per sample, by creating features from sub-sets of the genome - from single DNA mutations to functional gene-networks. The results have identified potential risk networks and evidence for interactions between mutations.", "Astrophysics to data Science: how the Milky Way is like my company.\n \nWhat happens when you move to a company which is no longer a start-up but still not established? When I moved to Data science, I choose a company which had been around for 10 years, as a small-medium sized Enterprise (SME) but the data science team was brand new (and both of us had moved from academia for this, our first data science job). \nYou don't have the excitement of a start-up: everything is new, let's try that, you can build the structures from the ground up. You don't have the structures of a bigger older company: there are things in place for you to work with, you know what's expected.\nInstead you have the best and worst of both worlds: starting a data team but no one understands how to use data. You get to put those structures in place, but have to shoe horn them into everyone else's. \nBut middle sized companies are like our Milky Way in many ways:  numerous, survive in many different environments, and you find them everywhere! \nSo what can you use from academia? Some things (like being able to communicate with a wide range of people) will be exceptionally useful. In a SME you will be expected to do this straight away, more than you might in a large company and with more people than you would find in a start-up.\nBut you have a great mixture of guidance (like from a large company) and leeway to invent (like from a start-up) on projects but you need to prove your worth quickly, which means creating useful products. So you need to break that habit of spending hours researching and not getting anything \"done\".\nSo here are some lessons learnt about the differences and how to use your academic experience to get the best of both worlds, not the worst.\n \nThe Milky Way is your average galaxy. There are thousands of them out there. Just like a medium size company. Not too young, not too old but has its own set of unique challenges no one ever mentioned.\nHard earned lessons of moving from Academia to data science in industry in your average sized company.", "Large Scale Vandalism Detection in Knowledge Bases\n \nKnowledge bases are an important source of information for many AI system: they rely on the bases for enriching the information they process to make better user experience. Obtaining such Knowledge Bases is difficult, and which is why this process is crowd-sourced. One of such bases is Wikidata: they allow everybody on the Internet to edit the content and add new information.\nUnfortunately, Wikidata is often targeted by vandals, who misuse the system and put false or offensive information there. This may lead to incorrect behaviour of the AI systems. To keep the base clean, Wikidata employs moderators who manually inspect each revision and revert vandalic ones.\nTo help moderators fight vandals, the organizers of WSDM Cup 2017 challenged the participants to build a Machine Learning model which automatically detects if an edit should be rolled back. In this talk we will discuss the second place solution to the Cup: how to process half of terabyte of revisions, extract meaningful features and create a production ready model that scales to a large number of testing examples.\n \nWikidata is a Knowledge Base where anybody can add new information. Unfortunately, it is targeted by vandals, who put inaccurate or offensive information there. To fight them, Wikidata employs moderators, who manually inspect each suggested edit. In this talk we will look into how we can use Machine Learning to automatically detect vandalic revisions and help the moderators.", "Keynote: Extending from Open to Usable: A Commerce Data Conundrum\n \nComing Soon!\n \nComing Soon!", "Matplotlib 2.0 or \"One does not simply change all the defaults\"\n \nTBD\n \nFor the first time in over a decade, matplotlib is changing the default styles.  This talk will provide a high-level overview of the changes, the reasoning behind the changes, and the challenges along the way.", "Altair: Declarative, Statistical Visualization for Python\n \nAltair provides a Python API for building statistical visualizations in a declarative manner. By statistical visualization we mean:\n\nThe data source is a DataFrame that consists of columns of different data types (quantitative, ordinal, nominal and date/time).\nThe DataFrame is in a tidy format where the rows correspond to samples and the columns correspond the observed variables.\nThe data is mapped to the visual properties (position, color, size, shape, faceting, etc.) using the group-by operation of Pandas and SQL.\n\nThe Altair API contains no actual visualization rendering code but instead emits JSON data structures following the Vega-Lite specification. For convenience, Altair can optionally use ipyvega to display client-side renderings seamlessly in the Jupyter notebook.\nAltair has the following features:\n\nCarefully-designed, declarative Python API based on traitlets.\nAuto-generated internal Python API that guarantees visualizations are type-checked and in full conformance with the Vega-Lite specification.\nAuto-generate Altair Python code from a Vega-Lite JSON spec.\nDisplay visualizations in the live Jupyter Notebook, on GitHub and nbviewer.\nExport visualizations to PNG images, stand-alone HTML pages and the Online Vega-Lite Editor.\nSerialize visualizations as JSON files.\nExplore Altair with 40 example datasets and over 70 examples.\n\nAltair is developed by Brian Granger and Jake Vanderplas in close collaboration with the UW Interactive Data Lab.\n \nAltair is a declarative statistical visualization library for Python. Altair provides a user-centric Python API on top of the declarative visualization stack of Vega-Lite, Vega and D3.js. A wide range of statistical visualizations can be created with only a few carefully design abstractions.", "Visualizing research data: Challenges of combining different datasources\n \nIntroduction\n\nGive talk goals: This talk aims to give the tools to solve the engineering challenges related to combining different datasources\nSet the context: We use geodata from humanitarian projects as an example, but solutions will apply to other areas as well.\nGo through talk outline\n\nPart 2: Quickly introduce the project\nGive the audience idea of real world project in preparation for the part 3\n\nShow screenshots of the final project\nGo through the used technologies (ESRI shapefiles, geo/topo json, xls, API\u00e2\u0080\u0099s, python libraries)\nIntroduce the data pipeline\n\nPart 3: Explain common problems and our solutions for them\nThis is the meat of the talk, each point introduces problem and suggests at least one solution. Solutions are based on Python technologies\n - Handling different data formats\n - How to manage the data sources (validation, automation, etc)\n - Normalizing units\n - Mapping problems (different projects may follow different standards for the id\u00e2\u0080\u0099s)\n - Normalizing data and metadata\nPart 4: Wrap up\n\nQuickly explain how we applied these problems in the project\nSum up the things you should consider (check-list)\n\n \nYour source data has multiple formats? You have multiple API\u00e2\u0080\u0099s to pull data from? This talk will go through some common problems with solutions that you will face when trying to combine multiple different research data sources in programmatic way. We go through a real world web project on that visualizes poverty data with JSON API's, Shapefiles and Excel spreadsheets as data sources.", "Modelling a text corpus using Deep Boltzmann Machines in python\n \nDeep Boltzmann machines (DBMs) are exciting for a variety of reasons, principal among which is the fact that they are able to learn probabilistic representations of data in an entirely unsupervised manner. This allows DBMs to leverage large quantities of unlabelled data which are often available. The resulting representations can then be fine-tuned using limited labelled data or studied to obtain a more comprehensive understanding of the data at hand. \nThis talk will begin by providing a high level description of DBMs and the training algorithms involved in learning such models. A topic modelling example will be used as a motivating example to discuss practical aspects of fitting DBMs and potential pitfalls. The entire code for this project is written in python using only standard libraries (e.g., numpy). \n \nDeep Boltzmann machines (DBMs) are exciting for a variety of reasons, principal among which is the fact that they are able to learn probabilistic representations of data in an entirely unsupervised manner. In this talk I will discuss the process of fitting and interpreting DBMs using a topic modelling example as motivation. ", "Robot detection in IT environments\n \nThe robot detection module that I will present is a part of the machine learning-driven, real-time user behavior analytic (UBA) tool of Balabit, called Blindspotter. Its main purpose is to detect internal and external attackers in an IT environment. Blindspotter gathers contextual information about the activities of users, and notifies the security analyst when an unusual event is found. We use the Python language both for the proof of concept experimentation of data scientists and for production as well.\nHumans and robots have different behavior patterns, for instance robots can be active for very long time periods, or they can have extremely high or very low diversity in some features. This implicates that in many aspects humans and robots need different monitoring techniques. Therefore, we should be able to identify the usage type of the account and handle humans and scripts separately in the UBA tool.\nFurthermore, it is also critical to know for security reasons when a human user's account becomes a scripted account or when a scheduled robot suddenly starts to behave like it was used by a human.\nFor example, an impostor coming from outside might attack a more easily crackable scripted account and start some manual activities there to look around in the system. Or a malicious insider could start running scripts to collect sensitive data. With the robot detection algorithm we can notice the sudden change in the usage patterns of the accounts.\nIn the presentation I will introduce the methodologies that we developed to detect scripted accounts (robots) in IT systems, based on their activities. Different aspects of robotic behavior has been investigated, for example rare scheduled activities surrounded by normal behavior or periodic patterns with some noise in it. I will illustrate the cases with examples found in real data.\n \nIn this talk, I will present the robot detection module of a machine learning-driven user behavior analytic tool. New methodologies were developed to distinguish between scripted accounts and human users, based on their activities. I will demonstrate a couple of statistical methods, like hypothesis tests from the SciPy library that we use to capture different aspects of robotic operation.", "Spherical Voronoi Diagrams in Python\n \nImagine you are in London and want to travel somewhere within London. Your smartphone ran out of battery and you are not familiar with the area. So you just go the closest tube station. In this manner, you could get a map of London partitioned into regions of closest distance to the various tube stations. Mathematically, this is called a Voronoi diagram. It is defined by a finite set of points in the plane, which are called generators (the tube stations in this example), and consists of a finite number of polygons representing the regions of closest distance to the generators (see Wikipedia  for pictures and more details). Calculating Voronoi diagrams in the plane is a well known problem in computational geometry and can be easily handled in Python using scipy.spatial.Voronoi.\nImagine you are in London and want to travel to some destination far away. Since you are close to an airport, you want to go by plane. In a similar fashion, the airports partition the surface of the Earth into regions of closest distance. Mathematically, this is called a spherical Voronoi diagram. Analogously, this is defined by a finite set of generator points on a sphere and consists of finitely many spherical polygons representing the regions of closest spherical distance to the generator points. (Play with this online sandbox if you like.)\nAt PyData London 2015, Tyler Reddy discussed the problem of calculating spherical Voronoi diagrams in Python in this talk. While the problem had been studied and solved conceptually by various computer scientists, there was no ready-to-go implementation available in Python. This initiated a collaboration, which ultimately led to the creation of scipy.spatial.SphericalVoronoi. \nThis talk will give a self-contained introduction to the topic of Voronoi diagrams and tell the story of the pull request mentioned above. We will also highlight some applications of spherical Voronoi diagrams in computational virology aimed at getting a better insight into the Dengue virus and Hepatitis C, which was Tylers original motivation to consider the problem. \n \nSpherical Voronoi diagrams partition the surface of a sphere into regions of closest distance to a set of generator points. As discussed by Tyler Reddy at PyData 2015, there was no ready-to-go implementation of this in Python. After a self-contained introduction, we will tell the story about how this talk led to an extension of scipy to handle this problem.", "Assurance Scoring: Using Machine Learning and Analytics to Reduce Risk in the Public Sector\n \nTraditionally, risk scoring frameworks are built around the customer journey to identify non-compliant or fraudulent behaviour. These frameworks combine data from different sources and historical known fraud to identify high risk transactions or applications. In the public sector, however, the emphasis is often on identifying low-risk customers. In this talk we will discuss an Assurance Scoring framework which applies these traditional machine learning and analytics techniques but changes this emphasis and identifies those customers posing minimum risk. The advantage of this approach is that low risk transactions can be automated (which account for the majority of customers) and resources can be focused more effectively to handle those exceptional high risk cases. \nThis framework has been developed in Python, in particular with Pandas and Scikit-Learn. But we also go beyond Machine Learning to incorporate other techniques such as rules based linking, anomaly detection and graph based analysis, and show how these can be used to boost the confidence of the low-risk group.\nIn particular, we will showcase how different python packages have been integrated to address the data pre-processing, feature engineering, model building & validation problems and how we have solved the challenges faced during the integration process by developing a range of testing procedures. \n \nWe will talk about a framework we have developed to use Machine Learning and other advanced analytical methods to reduce risk in the Public Sector. This python-based assurance scoring framework, developed with Pandas & Scikit-Learn, changes the emphasis of traditional risk-scoring frameworks to identifying compliant behaviour; we discuss some of the challenges faced and present a case study. ", "Customising nbconvert: how to turn Jupyter notebooks into anything you want\n \nJupyter Notebooks are code-centric documents including prose, maths, code, and rich output, such as images and HTML. These are all stored in a structured JSON file, with metadata about each input and output. nbconvert is a highly extensible tool for converting those JSON notebooks to other formats, such as HTML, LaTeX, PDF, and restructuredText. nbconvert powers the web service nbviewer, which renders any notebooks on the Web as HTML, so that they can be read by anyone with a browser and internet connection. We will cover the various ways you can extend nbconvert, from defining your own export formats to customising the output with Jinja templates, enabling things like hiding input or boilerplate cells.\n \nnbconvert is a set of tools to convert Jupyter notebooks into other document formats. We'll describe the different ways you can extend and customise nbconvert to modify the output and define extra output formats.", "Gotta catch'em all: recognizing sloppy work in crowdsourcing tasks\n \nIn 2016 nobody needs convincing that crowdsourced work is a solution to many problems from data labeling, to gathering subjective opinions, to producing transcripts etc. Turns out it can also work really well for functional software testing - but it's not easy to get right.\nOne well-known problem with crowdsourcing is sloppy work - where people perform only the absolute minimum actions allowing them to get paid, without actually fulfilling the intended tasks. In many scenarios this can be counteracted by asking multiple workers to complete the same task, but that dramatically increases cost and can still be error-prone. Detecting lazy work is another way to increase quality of gathered data and we have found a way to do this reliably for quite a large variety of tasks.\nIn this talk I will describe how we have trained a machine learning model to discriminate between good and sloppy work. The outline is as follows:\n- describe the specific problem we had (5m)\n- overview of the solution (2m)\n- ML model details (10m)\n  - data capture\n  - labelling\n  - balancing the dataset\n  - feature engineering\n  - training, retraining\n  - model itself\n- model persistence (10m)\n- productizing the result by putting it behind an HTTP API (3m)\n- limitations, trade-offs, what could we do better (3m)\n \nIf you have ever used crowdsourcing, you know that dealing with sloppy workers is a major part of the effort. Come see this talk if you want to learn about how to solve this problem using machine learning and some elbow grease. As a bonus, you will also find out how to properly persist your ML models and use them to serve predictions through an HTTP API."]}, "data": [{"ncat": 5, "s": 1.0, "cat": 6, "x": 0.0, "os": 0.9785781537138807, "ncat25k": 3, "term": "implementing", "bg": 2.04294026967979e-06, "y": 0.9277389277389277, "cat25k": 45}, {"ncat": 6, "s": 0.9968919968919969, "cat": 3, "x": 0.000777000777000777, "os": 0.8855260791451548, "ncat25k": 3, "term": "sampling", "bg": 3.597180529900664e-06, "y": 0.7451437451437452, "cat25k": 22}, {"ncat": 6, "s": 0.9968919968919969, "cat": 3, "x": 0.001554001554001554, "os": 0.8855260791451548, "ncat25k": 3, "term": "variational", "bg": 0.00012529847887646643, "y": 0.7886557886557887, "cat25k": 22}, {"ncat": 6, "s": 0.9992229992229993, "cat": 6, "x": 0.002331002331002331, "os": 0.9678501883833653, "ncat25k": 3, "term": "theano", "bg": 0.0002890640276821316, "y": 0.9269619269619269, "cat25k": 45}, {"ncat": 7, "s": 0.9984459984459985, "cat": 4, "x": 0.003108003108003108, "os": 0.9070498601640798, "ncat25k": 4, "term": "speech", "bg": 9.60390886774044e-07, "y": 0.8010878010878011, "cat25k": 30}, {"ncat": 7, "s": 0.9588189588189588, "cat": 2, "x": 0.003885003885003885, "os": 0.7686629679445819, "ncat25k": 4, "term": "comprehensive", "bg": 4.779351671387074e-07, "y": 0.5322455322455323, "cat25k": 15}, {"ncat": 7, "s": 0.9588189588189588, "cat": 2, "x": 0.004662004662004662, "os": 0.7686629679445819, "ncat25k": 4, "term": "k", "bg": 1.3281191375991194e-07, "y": 0.5532245532245532, "cat25k": 15}, {"ncat": 7, "s": 0.9588189588189588, "cat": 2, "x": 0.005439005439005439, "os": 0.7686629679445819, "ncat25k": 4, "term": "face", "bg": 2.652980726951208e-07, "y": 0.5874125874125874, "cat25k": 15}, {"ncat": 7, "s": 0.9588189588189588, "cat": 2, "x": 0.006216006216006216, "os": 0.7686629679445819, "ncat25k": 4, "term": "activity", "bg": 3.2601379695404306e-07, "y": 0.5936285936285937, "cat25k": 15}, {"ncat": 7, "s": 0.9588189588189588, "cat": 2, "x": 0.006993006993006993, "os": 0.7686629679445819, "ncat25k": 4, "term": "inputs", "bg": 3.3044196612969847e-06, "y": 0.5944055944055944, "cat25k": 15}, {"ncat": 7, "s": 0.9945609945609947, "cat": 3, "x": 0.00777000777000777, "os": 0.8579079149669719, "ncat25k": 4, "term": "inside", "bg": 4.3159043448856426e-07, "y": 0.7676767676767676, "cat25k": 22}, {"ncat": 7, "s": 0.9588189588189588, "cat": 2, "x": 0.008547008547008548, "os": 0.7686629679445819, "ncat25k": 4, "term": "c++", "bg": 0.0, "y": 0.6293706293706294, "cat25k": 15}, {"ncat": 7, "s": 0.9588189588189588, "cat": 2, "x": 0.009324009324009324, "os": 0.7686629679445819, "ncat25k": 4, "term": "fashion", "bg": 1.1363299338508254e-06, "y": 0.6627816627816627, "cat25k": 15}, {"ncat": 7, "s": 0.9588189588189588, "cat": 2, "x": 0.010101010101010102, "os": 0.7686629679445819, "ncat25k": 4, "term": "successfully", "bg": 1.1023050410944831e-06, "y": 0.6651126651126651, "cat25k": 15}, {"ncat": 7, "s": 0.9588189588189588, "cat": 2, "x": 0.010878010878010878, "os": 0.7686629679445819, "ncat25k": 4, "term": "hadoop", "bg": 0.00034006665306400053, "y": 0.6728826728826729, "cat25k": 15}, {"ncat": 7, "s": 0.9588189588189588, "cat": 2, "x": 0.011655011655011656, "os": 0.7686629679445819, "ncat25k": 4, "term": "manually", "bg": 3.6999041388473115e-06, "y": 0.6744366744366744, "cat25k": 15}, {"ncat": 7, "s": 0.9588189588189588, "cat": 2, "x": 0.012432012432012432, "os": 0.7686629679445819, "ncat25k": 4, "term": "loss", "bg": 2.886921729729948e-07, "y": 0.6853146853146853, "cat25k": 15}, {"ncat": 7, "s": 0.9588189588189588, "cat": 2, "x": 0.01320901320901321, "os": 0.7686629679445819, "ncat25k": 4, "term": "simultaneously", "bg": 2.347505397306159e-06, "y": 0.6923076923076923, "cat25k": 15}, {"ncat": 7, "s": 0.9588189588189588, "cat": 2, "x": 0.013986013986013986, "os": 0.7686629679445819, "ncat25k": 4, "term": "scenario", "bg": 1.9820870861674703e-06, "y": 0.6938616938616938, "cat25k": 15}, {"ncat": 8, "s": 0.9308469308469308, "cat": 2, "x": 0.014763014763014764, "os": 0.7399484573381458, "ncat25k": 5, "term": "reading", "bg": 2.0962808592064092e-07, "y": 0.5174825174825175, "cat25k": 15}, {"ncat": 8, "s": 0.6573426573426573, "cat": 1, "x": 0.01554001554001554, "os": 0.5453620342895181, "ncat25k": 5, "term": "csv", "bg": 1.5733223270695877e-05, "y": 0.2602952602952603, "cat25k": 7}, {"ncat": 8, "s": 0.9308469308469308, "cat": 2, "x": 0.016317016317016316, "os": 0.7399484573381458, "ncat25k": 5, "term": "-", "bg": 0.0, "y": 0.5337995337995338, "cat25k": 15}, {"ncat": 8, "s": 0.9891219891219892, "cat": 3, "x": 0.017094017094017096, "os": 0.8318378822596328, "ncat25k": 5, "term": "scalability", "bg": 1.083930726972631e-05, "y": 0.7226107226107226, "cat25k": 22}, {"ncat": 8, "s": 0.6573426573426573, "cat": 1, "x": 0.017871017871017872, "os": 0.5453620342895181, "ncat25k": 5, "term": "detecting", "bg": 8.517946165925004e-06, "y": 0.2867132867132867, "cat25k": 7}, {"ncat": 8, "s": 0.6573426573426573, "cat": 1, "x": 0.018648018648018648, "os": 0.5453620342895181, "ncat25k": 5, "term": "storing", "bg": 5.154643160806568e-06, "y": 0.28826728826728826, "cat25k": 7}, {"ncat": 8, "s": 0.6573426573426573, "cat": 1, "x": 0.019425019425019424, "os": 0.5453620342895181, "ncat25k": 5, "term": "organized", "bg": 9.988727720767115e-07, "y": 0.2898212898212898, "cat25k": 7}, {"ncat": 8, "s": 0.9308469308469308, "cat": 2, "x": 0.020202020202020204, "os": 0.7399484573381458, "ncat25k": 5, "term": "amazon", "bg": 4.763909654903405e-07, "y": 0.5446775446775447, "cat25k": 15}, {"ncat": 8, "s": 0.9308469308469308, "cat": 2, "x": 0.02097902097902098, "os": 0.7399484573381458, "ncat25k": 5, "term": "c", "bg": 9.384307782732717e-08, "y": 0.5454545454545454, "cat25k": 15}, {"ncat": 8, "s": 0.6573426573426573, "cat": 1, "x": 0.021756021756021756, "os": 0.5453620342895181, "ncat25k": 5, "term": "released", "bg": 4.557200038067811e-07, "y": 0.3006993006993007, "cat25k": 7}, {"ncat": 8, "s": 0.6573426573426573, "cat": 1, "x": 0.022533022533022532, "os": 0.5453620342895181, "ncat25k": 5, "term": "publicly", "bg": 2.6531651114094176e-06, "y": 0.3146853146853147, "cat25k": 7}, {"ncat": 8, "s": 0.6573426573426573, "cat": 1, "x": 0.023310023310023312, "os": 0.5453620342895181, "ncat25k": 5, "term": "textual", "bg": 1.1253072954537585e-05, "y": 0.3177933177933178, "cat25k": 7}, {"ncat": 8, "s": 0.9891219891219892, "cat": 3, "x": 0.024087024087024088, "os": 0.8318378822596328, "ncat25k": 5, "term": "link", "bg": 1.247296111489513e-07, "y": 0.7428127428127428, "cat25k": 22}, {"ncat": 8, "s": 0.6573426573426573, "cat": 1, "x": 0.024864024864024864, "os": 0.5453620342895181, "ncat25k": 5, "term": "ground", "bg": 3.2896472022792383e-07, "y": 0.33022533022533024, "cat25k": 7}, {"ncat": 8, "s": 0.6573426573426573, "cat": 1, "x": 0.02564102564102564, "os": 0.5453620342895181, "ncat25k": 5, "term": "introducing", "bg": 2.263266419023408e-06, "y": 0.337995337995338, "cat25k": 7}, {"ncat": 8, "s": 0.6573426573426573, "cat": 1, "x": 0.02641802641802642, "os": 0.5453620342895181, "ncat25k": 5, "term": "contrast", "bg": 9.959388380680357e-07, "y": 0.3442113442113442, "cat25k": 7}, {"ncat": 8, "s": 0.6573426573426573, "cat": 1, "x": 0.027195027195027196, "os": 0.5453620342895181, "ncat25k": 5, "term": "selecting", "bg": 1.698920836046494e-06, "y": 0.35353535353535354, "cat25k": 7}, {"ncat": 8, "s": 0.6573426573426573, "cat": 1, "x": 0.027972027972027972, "os": 0.5453620342895181, "ncat25k": 5, "term": "covering", "bg": 1.662175596065758e-06, "y": 0.3581973581973582, "cat25k": 7}, {"ncat": 8, "s": 0.6573426573426573, "cat": 1, "x": 0.028749028749028748, "os": 0.5453620342895181, "ncat25k": 5, "term": "processed", "bg": 1.803115287359854e-06, "y": 0.3651903651903652, "cat25k": 7}, {"ncat": 8, "s": 0.6573426573426573, "cat": 1, "x": 0.029526029526029528, "os": 0.5453620342895181, "ncat25k": 5, "term": "strings", "bg": 1.6209224783288741e-06, "y": 0.3776223776223776, "cat25k": 7}, {"ncat": 8, "s": 0.6573426573426573, "cat": 1, "x": 0.030303030303030304, "os": 0.5453620342895181, "ncat25k": 5, "term": "depends", "bg": 1.1525811557277022e-06, "y": 0.3822843822843823, "cat25k": 7}, {"ncat": 8, "s": 0.6573426573426573, "cat": 1, "x": 0.03108003108003108, "os": 0.5453620342895181, "ncat25k": 5, "term": "constraints", "bg": 3.0055860695594054e-06, "y": 0.38461538461538464, "cat25k": 7}, {"ncat": 8, "s": 0.9308469308469308, "cat": 2, "x": 0.031857031857031856, "os": 0.7399484573381458, "ncat25k": 5, "term": "elements", "bg": 6.772344741965086e-07, "y": 0.5959595959595959, "cat25k": 15}, {"ncat": 8, "s": 0.9308469308469308, "cat": 2, "x": 0.03263403263403263, "os": 0.7399484573381458, "ncat25k": 5, "term": "managing", "bg": 9.534389290783261e-07, "y": 0.5990675990675991, "cat25k": 15}, {"ncat": 8, "s": 0.9308469308469308, "cat": 2, "x": 0.03341103341103341, "os": 0.7399484573381458, "ncat25k": 5, "term": "cython", "bg": 0.0004080661067092869, "y": 0.6037296037296037, "cat25k": 15}, {"ncat": 8, "s": 0.6573426573426573, "cat": 1, "x": 0.03418803418803419, "os": 0.5453620342895181, "ncat25k": 5, "term": "markov", "bg": 2.4087372930744118e-05, "y": 0.3947163947163947, "cat25k": 7}, {"ncat": 8, "s": 0.9308469308469308, "cat": 2, "x": 0.03496503496503497, "os": 0.7399484573381458, "ncat25k": 5, "term": "findings", "bg": 1.0538879242886915e-06, "y": 0.6083916083916084, "cat25k": 15}, {"ncat": 8, "s": 0.6573426573426573, "cat": 1, "x": 0.035742035742035744, "os": 0.5453620342895181, "ncat25k": 5, "term": "inspired", "bg": 1.8885987695950708e-06, "y": 0.4017094017094017, "cat25k": 7}, {"ncat": 8, "s": 0.9308469308469308, "cat": 2, "x": 0.03651903651903652, "os": 0.7399484573381458, "ncat25k": 5, "term": "style", "bg": 3.1638721864450897e-07, "y": 0.6138306138306139, "cat25k": 15}, {"ncat": 8, "s": 0.6573426573426573, "cat": 1, "x": 0.037296037296037296, "os": 0.5453620342895181, "ncat25k": 5, "term": "students", "bg": 1.0735958913914676e-07, "y": 0.4063714063714064, "cat25k": 7}, {"ncat": 8, "s": 0.6573426573426573, "cat": 1, "x": 0.03807303807303807, "os": 0.5453620342895181, "ncat25k": 5, "term": "strengths", "bg": 3.2835807009733354e-06, "y": 0.4257964257964258, "cat25k": 7}, {"ncat": 8, "s": 0.9308469308469308, "cat": 2, "x": 0.03885003885003885, "os": 0.7399484573381458, "ncat25k": 5, "term": "typical", "bg": 9.61047680362461e-07, "y": 0.6317016317016317, "cat25k": 15}, {"ncat": 8, "s": 0.9308469308469308, "cat": 2, "x": 0.039627039627039624, "os": 0.7399484573381458, "ncat25k": 5, "term": "vs", "bg": 6.983055294034167e-07, "y": 0.6324786324786325, "cat25k": 15}, {"ncat": 8, "s": 0.6573426573426573, "cat": 1, "x": 0.04040404040404041, "os": 0.5453620342895181, "ncat25k": 5, "term": "loading", "bg": 1.2821591746997823e-06, "y": 0.4296814296814297, "cat25k": 7}, {"ncat": 8, "s": 0.6573426573426573, "cat": 1, "x": 0.041181041181041184, "os": 0.5453620342895181, "ncat25k": 5, "term": "nets", "bg": 6.6003727973060566e-06, "y": 0.4405594405594406, "cat25k": 7}, {"ncat": 8, "s": 0.6573426573426573, "cat": 1, "x": 0.04195804195804196, "os": 0.5453620342895181, "ncat25k": 5, "term": "subject", "bg": 8.582503802156466e-08, "y": 0.44522144522144524, "cat25k": 7}, {"ncat": 8, "s": 0.6573426573426573, "cat": 1, "x": 0.042735042735042736, "os": 0.5453620342895181, "ncat25k": 5, "term": "highlighting", "bg": 6.119998898400198e-06, "y": 0.4467754467754468, "cat25k": 7}, {"ncat": 8, "s": 0.6573426573426573, "cat": 1, "x": 0.04351204351204351, "os": 0.5453620342895181, "ncat25k": 5, "term": "readily", "bg": 2.601717769703459e-06, "y": 0.4483294483294483, "cat25k": 7}, {"ncat": 8, "s": 0.6573426573426573, "cat": 1, "x": 0.04428904428904429, "os": 0.5453620342895181, "ncat25k": 5, "term": "global", "bg": 3.2062170172447665e-07, "y": 0.44988344988344986, "cat25k": 7}, {"ncat": 8, "s": 0.9308469308469308, "cat": 2, "x": 0.045066045066045064, "os": 0.7399484573381458, "ncat25k": 5, "term": "nature", "bg": 2.9711555759949046e-07, "y": 0.6410256410256411, "cat25k": 15}, {"ncat": 8, "s": 0.6573426573426573, "cat": 1, "x": 0.04584304584304584, "os": 0.5453620342895181, "ncat25k": 5, "term": "obtain", "bg": 6.495685922672886e-07, "y": 0.45143745143745145, "cat25k": 7}, {"ncat": 8, "s": 0.9308469308469308, "cat": 2, "x": 0.046620046620046623, "os": 0.7399484573381458, "ncat25k": 5, "term": "fact", "bg": 2.981083502693478e-07, "y": 0.6449106449106449, "cat25k": 15}, {"ncat": 8, "s": 0.9308469308469308, "cat": 2, "x": 0.0473970473970474, "os": 0.7399484573381458, "ncat25k": 5, "term": "produce", "bg": 6.197063346888564e-07, "y": 0.6464646464646465, "cat25k": 15}, {"ncat": 8, "s": 0.6573426573426573, "cat": 1, "x": 0.048174048174048176, "os": 0.5453620342895181, "ncat25k": 5, "term": "numerous", "bg": 9.080482129207591e-07, "y": 0.45454545454545453, "cat25k": 7}, {"ncat": 8, "s": 0.6573426573426573, "cat": 1, "x": 0.04895104895104895, "os": 0.5453620342895181, "ncat25k": 5, "term": "house", "bg": 1.1234593844824428e-07, "y": 0.4592074592074592, "cat25k": 7}, {"ncat": 8, "s": 0.6573426573426573, "cat": 1, "x": 0.04972804972804973, "os": 0.5453620342895181, "ncat25k": 5, "term": "requests", "bg": 6.601122108244375e-07, "y": 0.46386946386946387, "cat25k": 7}, {"ncat": 8, "s": 0.9308469308469308, "cat": 2, "x": 0.050505050505050504, "os": 0.7399484573381458, "ncat25k": 5, "term": "exactly", "bg": 6.901469724444391e-07, "y": 0.6588966588966589, "cat25k": 15}, {"ncat": 8, "s": 0.6573426573426573, "cat": 1, "x": 0.05128205128205128, "os": 0.5453620342895181, "ncat25k": 5, "term": "simulation", "bg": 3.316452997017158e-06, "y": 0.47241647241647244, "cat25k": 7}, {"ncat": 8, "s": 0.9308469308469308, "cat": 2, "x": 0.052059052059052056, "os": 0.7399484573381458, "ncat25k": 5, "term": "classes", "bg": 5.311044269576269e-07, "y": 0.662004662004662, "cat25k": 15}, {"ncat": 8, "s": 0.6573426573426573, "cat": 1, "x": 0.05283605283605284, "os": 0.5453620342895181, "ncat25k": 5, "term": "published", "bg": 2.58697993927582e-07, "y": 0.47707847707847706, "cat25k": 7}, {"ncat": 8, "s": 0.6573426573426573, "cat": 1, "x": 0.053613053613053616, "os": 0.5453620342895181, "ncat25k": 5, "term": "bias", "bg": 3.5854727400989415e-06, "y": 0.47785547785547783, "cat25k": 7}, {"ncat": 8, "s": 0.6573426573426573, "cat": 1, "x": 0.05439005439005439, "os": 0.5453620342895181, "ncat25k": 5, "term": "list", "bg": 5.077127307298125e-08, "y": 0.4794094794094794, "cat25k": 7}, {"ncat": 8, "s": 0.9308469308469308, "cat": 2, "x": 0.05516705516705517, "os": 0.7399484573381458, "ncat25k": 5, "term": "architectures", "bg": 1.0310184772830831e-05, "y": 0.6705516705516705, "cat25k": 15}, {"ncat": 8, "s": 0.9308469308469308, "cat": 2, "x": 0.055944055944055944, "os": 0.7399484573381458, "ncat25k": 5, "term": "metadata", "bg": 5.093860370095923e-06, "y": 0.6721056721056721, "cat25k": 15}, {"ncat": 8, "s": 0.6573426573426573, "cat": 1, "x": 0.05672105672105672, "os": 0.5453620342895181, "ncat25k": 5, "term": "crucial", "bg": 2.200310023682337e-06, "y": 0.48951048951048953, "cat25k": 7}, {"ncat": 8, "s": 0.6573426573426573, "cat": 1, "x": 0.057498057498057496, "os": 0.5453620342895181, "ncat25k": 5, "term": "lightweight", "bg": 2.4348844268579654e-06, "y": 0.49417249417249415, "cat25k": 7}, {"ncat": 8, "s": 0.9308469308469308, "cat": 2, "x": 0.05827505827505827, "os": 0.7399484573381458, "ncat25k": 5, "term": "extracted", "bg": 4.59066309207034e-06, "y": 0.6806526806526807, "cat25k": 15}, {"ncat": 8, "s": 0.9308469308469308, "cat": 2, "x": 0.059052059052059055, "os": 0.7399484573381458, "ncat25k": 5, "term": "shown", "bg": 2.903879005931942e-07, "y": 0.6822066822066822, "cat25k": 15}, {"ncat": 8, "s": 0.9308469308469308, "cat": 2, "x": 0.05982905982905983, "os": 0.7399484573381458, "ncat25k": 5, "term": "uk", "bg": 1.631834409689163e-07, "y": 0.682983682983683, "cat25k": 15}, {"ncat": 8, "s": 0.6573426573426573, "cat": 1, "x": 0.06060606060606061, "os": 0.5453620342895181, "ncat25k": 5, "term": "extension", "bg": 6.361725269819564e-07, "y": 0.4996114996114996, "cat25k": 7}, {"ncat": 8, "s": 0.9308469308469308, "cat": 2, "x": 0.061383061383061384, "os": 0.7399484573381458, "ncat25k": 5, "term": "traffic", "bg": 4.88154708082177e-07, "y": 0.6899766899766899, "cat25k": 15}, {"ncat": 8, "s": 0.6573426573426573, "cat": 1, "x": 0.06216006216006216, "os": 0.5453620342895181, "ncat25k": 5, "term": "close", "bg": 2.0375901484294584e-07, "y": 0.5034965034965035, "cat25k": 7}, {"ncat": 8, "s": 0.9308469308469308, "cat": 2, "x": 0.06293706293706294, "os": 0.7399484573381458, "ncat25k": 5, "term": "finance", "bg": 2.2757417136184078e-07, "y": 0.6907536907536908, "cat25k": 15}, {"ncat": 8, "s": 0.9308469308469308, "cat": 2, "x": 0.06371406371406371, "os": 0.7399484573381458, "ncat25k": 5, "term": "enabling", "bg": 2.384654272823526e-06, "y": 0.6930846930846931, "cat25k": 15}, {"ncat": 8, "s": 0.9308469308469308, "cat": 2, "x": 0.0644910644910645, "os": 0.7399484573381458, "ncat25k": 5, "term": "increasing", "bg": 9.798926380631106e-07, "y": 0.6946386946386947, "cat25k": 15}, {"ncat": 8, "s": 0.6573426573426573, "cat": 1, "x": 0.06526806526806526, "os": 0.5453620342895181, "ncat25k": 5, "term": "visually", "bg": 6.397246625052577e-06, "y": 0.5073815073815073, "cat25k": 7}, {"ncat": 9, "s": 0.6301476301476301, "cat": 1, "x": 0.06604506604506605, "os": 0.5276111917496155, "ncat25k": 5, "term": "covered", "bg": 6.352613736600782e-07, "y": 0.25485625485625485, "cat25k": 7}, {"ncat": 9, "s": 0.9953379953379954, "cat": 4, "x": 0.06682206682206682, "os": 0.8648326160284714, "ncat25k": 5, "term": "turn", "bg": 3.23199592430397e-07, "y": 0.8034188034188035, "cat25k": 30}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.0675990675990676, "os": 0.165419234639531, "ncat25k": 5, "term": "beginners", "bg": 3.972024370355525e-06, "y": 0.006993006993006993, "cat25k": 0}, {"ncat": 9, "s": 0.6301476301476301, "cat": 1, "x": 0.06837606837606838, "os": 0.5276111917496155, "ncat25k": 5, "term": "html", "bg": 3.364913850994885e-07, "y": 0.26573426573426573, "cat25k": 7}, {"ncat": 9, "s": 0.982905982905983, "cat": 3, "x": 0.06915306915306915, "os": 0.807575820492458, "ncat25k": 5, "term": "relatively", "bg": 1.3123668263144083e-06, "y": 0.7132867132867133, "cat25k": 22}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.06993006993006994, "os": 0.165419234639531, "ncat25k": 5, "term": "tackle", "bg": 2.7426473433955984e-06, "y": 0.01554001554001554, "cat25k": 0}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.0707070707070707, "os": 0.165419234639531, "ncat25k": 5, "term": "furthermore", "bg": 1.7271632957764555e-06, "y": 0.019425019425019424, "cat25k": 0}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.07148407148407149, "os": 0.165419234639531, "ncat25k": 5, "term": "days", "bg": 5.896508338036236e-08, "y": 0.024087024087024088, "cat25k": 0}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.07226107226107226, "os": 0.165419234639531, "ncat25k": 5, "term": "20", "bg": 0.0, "y": 0.02564102564102564, "cat25k": 0}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.07303807303807304, "os": 0.165419234639531, "ncat25k": 5, "term": "gets", "bg": 3.679092918823288e-07, "y": 0.027972027972027972, "cat25k": 0}, {"ncat": 9, "s": 0.6301476301476301, "cat": 1, "x": 0.07381507381507381, "os": 0.5276111917496155, "ncat25k": 5, "term": "match", "bg": 4.4734308681678625e-07, "y": 0.2874902874902875, "cat25k": 7}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.07459207459207459, "os": 0.165419234639531, "ncat25k": 5, "term": "events", "bg": 1.7879041490282473e-07, "y": 0.030303030303030304, "cat25k": 0}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.07536907536907538, "os": 0.165419234639531, "ncat25k": 5, "term": "minimal", "bg": 1.6798931655142459e-06, "y": 0.03807303807303807, "cat25k": 0}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.07614607614607614, "os": 0.165419234639531, "ncat25k": 5, "term": "communication", "bg": 3.721151193939113e-07, "y": 0.04195804195804196, "cat25k": 0}, {"ncat": 9, "s": 0.6301476301476301, "cat": 1, "x": 0.07692307692307693, "os": 0.5276111917496155, "ncat25k": 5, "term": "strategy", "bg": 3.8402611936195103e-07, "y": 0.30924630924630925, "cat25k": 7}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.0777000777000777, "os": 0.165419234639531, "ncat25k": 5, "term": "computers", "bg": 1.561336492587854e-07, "y": 0.059052059052059055, "cat25k": 0}, {"ncat": 9, "s": 0.9145299145299145, "cat": 2, "x": 0.07847707847707848, "os": 0.7146681591158386, "ncat25k": 5, "term": "completely", "bg": 8.115308564464809e-07, "y": 0.5664335664335665, "cat25k": 15}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.07925407925407925, "os": 0.165419234639531, "ncat25k": 5, "term": "debugging", "bg": 9.212132378342277e-06, "y": 0.0707070707070707, "cat25k": 0}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.08003108003108003, "os": 0.165419234639531, "ncat25k": 5, "term": "necessarily", "bg": 1.1260254573086529e-06, "y": 0.07226107226107226, "cat25k": 0}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.08080808080808081, "os": 0.165419234639531, "ncat25k": 5, "term": "manager", "bg": 2.7409989022299394e-07, "y": 0.07847707847707848, "cat25k": 0}, {"ncat": 9, "s": 0.982905982905983, "cat": 3, "x": 0.08158508158508158, "os": 0.807575820492458, "ncat25k": 5, "term": "6", "bg": 0.0, "y": 0.7513597513597513, "cat25k": 22}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.08236208236208237, "os": 0.165419234639531, "ncat25k": 5, "term": "months", "bg": 1.796785839415805e-07, "y": 0.08003108003108003, "cat25k": 0}, {"ncat": 9, "s": 0.6301476301476301, "cat": 1, "x": 0.08313908313908314, "os": 0.5276111917496155, "ncat25k": 5, "term": "trade", "bg": 1.8148128820390957e-07, "y": 0.3480963480963481, "cat25k": 7}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.08391608391608392, "os": 0.165419234639531, "ncat25k": 5, "term": "measures", "bg": 6.491128898690605e-07, "y": 0.08391608391608392, "cat25k": 0}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.08469308469308469, "os": 0.165419234639531, "ncat25k": 5, "term": "covers", "bg": 6.648496527127619e-07, "y": 0.08857808857808858, "cat25k": 0}, {"ncat": 9, "s": 0.6301476301476301, "cat": 1, "x": 0.08547008547008547, "os": 0.5276111917496155, "ncat25k": 5, "term": "generates", "bg": 4.518873640619251e-06, "y": 0.3628593628593629, "cat25k": 7}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.08624708624708624, "os": 0.165419234639531, "ncat25k": 5, "term": "interactions", "bg": 2.140174164080897e-06, "y": 0.09246309246309246, "cat25k": 0}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.08702408702408702, "os": 0.165419234639531, "ncat25k": 5, "term": "vast", "bg": 1.6619047555736964e-06, "y": 0.09557109557109557, "cat25k": 0}, {"ncat": 9, "s": 0.9145299145299145, "cat": 2, "x": 0.08780108780108781, "os": 0.7146681591158386, "ncat25k": 5, "term": "tricks", "bg": 2.7613948712810295e-06, "y": 0.5881895881895882, "cat25k": 15}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.08857808857808858, "os": 0.165419234639531, "ncat25k": 5, "term": "section", "bg": 1.291046816262413e-07, "y": 0.10411810411810411, "cat25k": 0}, {"ncat": 9, "s": 0.982905982905983, "cat": 3, "x": 0.08935508935508936, "os": 0.807575820492458, "ncat25k": 5, "term": "combination", "bg": 9.166206270453639e-07, "y": 0.7637917637917638, "cat25k": 22}, {"ncat": 9, "s": 0.6301476301476301, "cat": 1, "x": 0.09013209013209013, "os": 0.5276111917496155, "ncat25k": 5, "term": "hood", "bg": 2.1591354821529457e-06, "y": 0.38694638694638694, "cat25k": 7}, {"ncat": 9, "s": 0.6301476301476301, "cat": 1, "x": 0.09090909090909091, "os": 0.5276111917496155, "ncat25k": 5, "term": "download", "bg": 1.1778363187595644e-07, "y": 0.39937839937839936, "cat25k": 7}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.09168609168609168, "os": 0.165419234639531, "ncat25k": 5, "term": "reporting", "bg": 5.247540248305733e-07, "y": 0.11344211344211344, "cat25k": 0}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.09246309246309246, "os": 0.165419234639531, "ncat25k": 5, "term": "course", "bg": 1.235300853339653e-07, "y": 0.11655011655011654, "cat25k": 0}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.09324009324009325, "os": 0.165419234639531, "ncat25k": 5, "term": "scraping", "bg": 6.728279767255345e-05, "y": 0.11732711732711733, "cat25k": 0}, {"ncat": 9, "s": 0.9145299145299145, "cat": 2, "x": 0.09401709401709402, "os": 0.7146681591158386, "ncat25k": 5, "term": "trends", "bg": 8.462349219688702e-07, "y": 0.6114996114996115, "cat25k": 15}, {"ncat": 9, "s": 0.6301476301476301, "cat": 1, "x": 0.0947940947940948, "os": 0.5276111917496155, "ncat25k": 5, "term": "extremely", "bg": 1.0735017126274727e-06, "y": 0.40404040404040403, "cat25k": 7}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.09557109557109557, "os": 0.165419234639531, "ncat25k": 5, "term": "recommendation", "bg": 2.2137929595517334e-06, "y": 0.13286713286713286, "cat25k": 0}, {"ncat": 9, "s": 0.6301476301476301, "cat": 1, "x": 0.09634809634809635, "os": 0.5276111917496155, "ncat25k": 5, "term": "anything", "bg": 2.628021436245251e-07, "y": 0.4133644133644134, "cat25k": 7}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.09712509712509712, "os": 0.165419234639531, "ncat25k": 5, "term": "expected", "bg": 3.672127622298466e-07, "y": 0.1414141414141414, "cat25k": 0}, {"ncat": 9, "s": 0.6301476301476301, "cat": 1, "x": 0.0979020979020979, "os": 0.5276111917496155, "ncat25k": 5, "term": "trivial", "bg": 6.07196883865592e-06, "y": 0.41414141414141414, "cat25k": 7}, {"ncat": 9, "s": 0.9145299145299145, "cat": 2, "x": 0.09867909867909867, "os": 0.7146681591158386, "ncat25k": 5, "term": "generating", "bg": 3.1222586151199615e-06, "y": 0.6231546231546231, "cat25k": 15}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.09945609945609946, "os": 0.165419234639531, "ncat25k": 5, "term": "experiment", "bg": 1.8793135699796302e-06, "y": 0.14296814296814297, "cat25k": 0}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.10023310023310024, "os": 0.165419234639531, "ncat25k": 5, "term": "interpret", "bg": 4.269692403596168e-06, "y": 0.15073815073815075, "cat25k": 0}, {"ncat": 9, "s": 0.982905982905983, "cat": 3, "x": 0.10101010101010101, "os": 0.807575820492458, "ncat25k": 5, "term": "distance", "bg": 6.357838987928405e-07, "y": 0.7816627816627817, "cat25k": 22}, {"ncat": 9, "s": 0.982905982905983, "cat": 3, "x": 0.10178710178710179, "os": 0.807575820492458, "ncat25k": 5, "term": "put", "bg": 2.6013957060269896e-07, "y": 0.7824397824397824, "cat25k": 22}, {"ncat": 9, "s": 0.9145299145299145, "cat": 2, "x": 0.10256410256410256, "os": 0.7146681591158386, "ncat25k": 5, "term": "classifier", "bg": 3.33434475124121e-05, "y": 0.634032634032634, "cat25k": 15}, {"ncat": 9, "s": 0.6301476301476301, "cat": 1, "x": 0.10334110334110334, "os": 0.5276111917496155, "ncat25k": 5, "term": "execution", "bg": 1.368159062172568e-06, "y": 0.4421134421134421, "cat25k": 7}, {"ncat": 9, "s": 0.6301476301476301, "cat": 1, "x": 0.10411810411810411, "os": 0.5276111917496155, "ncat25k": 5, "term": "combined", "bg": 6.675991691861859e-07, "y": 0.44366744366744365, "cat25k": 7}, {"ncat": 9, "s": 0.9953379953379954, "cat": 4, "x": 0.1048951048951049, "os": 0.8648326160284714, "ncat25k": 5, "term": "black", "bg": 1.633935006615946e-07, "y": 0.8508158508158508, "cat25k": 30}, {"ncat": 9, "s": 0.6301476301476301, "cat": 1, "x": 0.10567210567210568, "os": 0.5276111917496155, "ncat25k": 5, "term": "suitable", "bg": 9.550191700998015e-07, "y": 0.4491064491064491, "cat25k": 7}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.10644910644910645, "os": 0.165419234639531, "ncat25k": 5, "term": "procedures", "bg": 5.396340338966137e-07, "y": 0.17094017094017094, "cat25k": 0}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.10722610722610723, "os": 0.165419234639531, "ncat25k": 5, "term": "event", "bg": 2.706297883911458e-07, "y": 0.18414918414918416, "cat25k": 0}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.108003108003108, "os": 0.165419234639531, "ncat25k": 5, "term": "piece", "bg": 5.403406753398399e-07, "y": 0.18492618492618493, "cat25k": 0}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.10878010878010878, "os": 0.165419234639531, "ncat25k": 5, "term": "queries", "bg": 2.0257077164406235e-06, "y": 0.1864801864801865, "cat25k": 0}, {"ncat": 9, "s": 0.6301476301476301, "cat": 1, "x": 0.10955710955710955, "os": 0.5276111917496155, "ncat25k": 5, "term": "worked", "bg": 7.843989327468121e-07, "y": 0.45765345765345766, "cat25k": 7}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.11033411033411034, "os": 0.165419234639531, "ncat25k": 5, "term": "members", "bg": 9.701298432365423e-08, "y": 0.19036519036519037, "cat25k": 0}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.1111111111111111, "os": 0.165419234639531, "ncat25k": 5, "term": "matching", "bg": 8.018092941266152e-07, "y": 0.19114219114219114, "cat25k": 0}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.11188811188811189, "os": 0.165419234639531, "ncat25k": 5, "term": "survey", "bg": 3.802736468176182e-07, "y": 0.1926961926961927, "cat25k": 0}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.11266511266511267, "os": 0.165419234639531, "ncat25k": 5, "term": "2017", "bg": 0.0, "y": 0.19347319347319347, "cat25k": 0}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.11344211344211344, "os": 0.165419234639531, "ncat25k": 5, "term": "articles", "bg": 2.3739052892370493e-07, "y": 0.19425019425019424, "cat25k": 0}, {"ncat": 9, "s": 0.982905982905983, "cat": 3, "x": 0.11421911421911422, "os": 0.807575820492458, "ncat25k": 5, "term": "preprocessing", "bg": 6.477868552615534e-05, "y": 0.7917637917637917, "cat25k": 22}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.11499611499611499, "os": 0.165419234639531, "ncat25k": 5, "term": "front", "bg": 1.9801204422119817e-07, "y": 0.20046620046620048, "cat25k": 0}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.11577311577311578, "os": 0.165419234639531, "ncat25k": 5, "term": "planning", "bg": 1.7308312411269658e-07, "y": 0.20279720279720279, "cat25k": 0}, {"ncat": 9, "s": 0.6301476301476301, "cat": 1, "x": 0.11655011655011654, "os": 0.5276111917496155, "ncat25k": 5, "term": "determine", "bg": 6.538223728535566e-07, "y": 0.4731934731934732, "cat25k": 7}, {"ncat": 9, "s": 0.6301476301476301, "cat": 1, "x": 0.11732711732711733, "os": 0.5276111917496155, "ncat25k": 5, "term": "generated", "bg": 5.706077262776063e-07, "y": 0.473970473970474, "cat25k": 7}, {"ncat": 9, "s": 0.9145299145299145, "cat": 2, "x": 0.11810411810411811, "os": 0.7146681591158386, "ncat25k": 5, "term": "re", "bg": 6.497044580274237e-08, "y": 0.6666666666666666, "cat25k": 15}, {"ncat": 9, "s": 0.6301476301476301, "cat": 1, "x": 0.11888111888111888, "os": 0.5276111917496155, "ncat25k": 5, "term": "rules", "bg": 2.454583631688826e-07, "y": 0.4763014763014763, "cat25k": 7}, {"ncat": 9, "s": 0.6301476301476301, "cat": 1, "x": 0.11965811965811966, "os": 0.5276111917496155, "ncat25k": 5, "term": "describes", "bg": 1.7153053758160566e-06, "y": 0.47863247863247865, "cat25k": 7}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.12043512043512043, "os": 0.165419234639531, "ncat25k": 5, "term": "economic", "bg": 3.005519722842993e-07, "y": 0.21056721056721056, "cat25k": 0}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.12121212121212122, "os": 0.165419234639531, "ncat25k": 5, "term": "progress", "bg": 3.766289594819427e-07, "y": 0.21134421134421136, "cat25k": 0}, {"ncat": 9, "s": 0.9145299145299145, "cat": 2, "x": 0.12198912198912198, "os": 0.7146681591158386, "ncat25k": 5, "term": "optimized", "bg": 5.980870782888011e-06, "y": 0.675990675990676, "cat25k": 15}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.12276612276612277, "os": 0.165419234639531, "ncat25k": 5, "term": "sure", "bg": 2.711340912718149e-07, "y": 0.21678321678321677, "cat25k": 0}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.12354312354312354, "os": 0.165419234639531, "ncat25k": 5, "term": "default", "bg": 3.356374471147263e-07, "y": 0.21833721833721834, "cat25k": 0}, {"ncat": 9, "s": 0.6301476301476301, "cat": 1, "x": 0.12432012432012432, "os": 0.5276111917496155, "ncat25k": 5, "term": "chain", "bg": 5.548468773814201e-07, "y": 0.4887334887334887, "cat25k": 7}, {"ncat": 9, "s": 0.982905982905983, "cat": 3, "x": 0.1250971250971251, "os": 0.807575820492458, "ncat25k": 5, "term": "optimize", "bg": 5.355714985272677e-06, "y": 0.7956487956487956, "cat25k": 22}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.1258741258741259, "os": 0.165419234639531, "ncat25k": 5, "term": "studies", "bg": 3.32894159630887e-07, "y": 0.22144522144522144, "cat25k": 0}, {"ncat": 9, "s": 0.9145299145299145, "cat": 2, "x": 0.12665112665112666, "os": 0.7146681591158386, "ncat25k": 5, "term": "representing", "bg": 2.269628640119577e-06, "y": 0.6783216783216783, "cat25k": 15}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.12742812742812742, "os": 0.165419234639531, "ncat25k": 5, "term": "place", "bg": 9.046177838565881e-08, "y": 0.22455322455322455, "cat25k": 0}, {"ncat": 9, "s": 0.6301476301476301, "cat": 1, "x": 0.1282051282051282, "os": 0.5276111917496155, "ncat25k": 5, "term": "measure", "bg": 6.832001517449647e-07, "y": 0.4902874902874903, "cat25k": 7}, {"ncat": 9, "s": 0.6301476301476301, "cat": 1, "x": 0.128982128982129, "os": 0.5276111917496155, "ncat25k": 5, "term": "jobs", "bg": 1.655691053877379e-07, "y": 0.494949494949495, "cat25k": 7}, {"ncat": 9, "s": 0.9145299145299145, "cat": 2, "x": 0.12975912975912976, "os": 0.7146681591158386, "ncat25k": 5, "term": "interested", "bg": 4.669280140582686e-07, "y": 0.6814296814296814, "cat25k": 15}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.13053613053613053, "os": 0.165419234639531, "ncat25k": 5, "term": "manual", "bg": 4.014777593365661e-07, "y": 0.23076923076923078, "cat25k": 0}, {"ncat": 9, "s": 0.6301476301476301, "cat": 1, "x": 0.13131313131313133, "os": 0.5276111917496155, "ncat25k": 5, "term": "feedback", "bg": 2.77914338119477e-07, "y": 0.49572649572649574, "cat25k": 7}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.1320901320901321, "os": 0.165419234639531, "ncat25k": 5, "term": "frame", "bg": 7.792610410412331e-07, "y": 0.23776223776223776, "cat25k": 0}, {"ncat": 9, "s": 0.9145299145299145, "cat": 2, "x": 0.13286713286713286, "os": 0.7146681591158386, "ncat25k": 5, "term": "validation", "bg": 2.9719390656475046e-06, "y": 0.6860916860916861, "cat25k": 15}, {"ncat": 9, "s": 0.6301476301476301, "cat": 1, "x": 0.13364413364413363, "os": 0.5276111917496155, "ncat25k": 5, "term": "goals", "bg": 5.422348485824098e-07, "y": 0.4988344988344988, "cat25k": 7}, {"ncat": 9, "s": 0.6301476301476301, "cat": 1, "x": 0.13442113442113443, "os": 0.5276111917496155, "ncat25k": 5, "term": "experimental", "bg": 1.2318786796601123e-06, "y": 0.5011655011655012, "cat25k": 7}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.1351981351981352, "os": 0.165419234639531, "ncat25k": 5, "term": "analysts", "bg": 3.7242714360750757e-06, "y": 0.2432012432012432, "cat25k": 0}, {"ncat": 9, "s": 0.6301476301476301, "cat": 1, "x": 0.13597513597513597, "os": 0.5276111917496155, "ncat25k": 5, "term": "tracking", "bg": 1.2592489870522444e-06, "y": 0.5027195027195027, "cat25k": 7}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.13675213675213677, "os": 0.165419234639531, "ncat25k": 5, "term": "lab", "bg": 5.611108648457871e-07, "y": 0.24397824397824397, "cat25k": 0}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.13752913752913754, "os": 0.165419234639531, "ncat25k": 5, "term": "say", "bg": 1.2741866021177327e-07, "y": 0.24475524475524477, "cat25k": 0}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.1383061383061383, "os": 0.165419234639531, "ncat25k": 5, "term": "health", "bg": 9.987877055326055e-08, "y": 0.2494172494172494, "cat25k": 0}, {"ncat": 9, "s": 0.9145299145299145, "cat": 2, "x": 0.13908313908313907, "os": 0.7146681591158386, "ncat25k": 5, "term": "define", "bg": 6.076572380083747e-07, "y": 0.6954156954156954, "cat25k": 15}, {"ncat": 9, "s": 0.39238539238539244, "cat": 0, "x": 0.13986013986013987, "os": 0.165419234639531, "ncat25k": 5, "term": "increases", "bg": 1.0037882970330023e-06, "y": 0.25097125097125095, "cat25k": 0}, {"ncat": 9, "s": 0.6301476301476301, "cat": 1, "x": 0.14063714063714064, "os": 0.5276111917496155, "ncat25k": 5, "term": "error", "bg": 2.234285152586427e-07, "y": 0.5081585081585082, "cat25k": 7}, {"ncat": 10, "s": 0.6037296037296038, "cat": 1, "x": 0.1414141414141414, "os": 0.5126948093508952, "ncat25k": 6, "term": "dashboards", "bg": 7.986451220400216e-05, "y": 0.257964257964258, "cat25k": 7}, {"ncat": 10, "s": 0.8974358974358975, "cat": 2, "x": 0.14219114219114218, "os": 0.6923655084958391, "ncat25k": 6, "term": "arrays", "bg": 8.150320679161427e-06, "y": 0.5268065268065268, "cat25k": 15}, {"ncat": 10, "s": 0.3154623154623155, "cat": 0, "x": 0.14296814296814297, "os": 0.14024305637885748, "ncat25k": 6, "term": "dask", "bg": 0.0007819804504887377, "y": 0.008547008547008548, "cat25k": 0}, {"ncat": 10, "s": 0.3154623154623155, "cat": 0, "x": 0.14374514374514374, "os": 0.14024305637885748, "ncat25k": 6, "term": "previous", "bg": 9.910297340038535e-08, "y": 0.011655011655011656, "cat25k": 0}, {"ncat": 10, "s": 0.3154623154623155, "cat": 0, "x": 0.1445221445221445, "os": 0.14024305637885748, "ncat25k": 6, "term": "check", "bg": 8.993956286224564e-08, "y": 0.020202020202020204, "cat25k": 0}, {"ncat": 10, "s": 0.6037296037296038, "cat": 1, "x": 0.1452991452991453, "os": 0.5126948093508952, "ncat25k": 6, "term": "hidden", "bg": 1.501274619683977e-06, "y": 0.2828282828282828, "cat25k": 7}, {"ncat": 10, "s": 0.6037296037296038, "cat": 1, "x": 0.14607614607614608, "os": 0.5126948093508952, "ncat25k": 6, "term": "designing", "bg": 2.293679370314117e-06, "y": 0.2905982905982906, "cat25k": 7}, {"ncat": 10, "s": 0.6037296037296038, "cat": 1, "x": 0.14685314685314685, "os": 0.5126948093508952, "ncat25k": 6, "term": "internet", "bg": 1.2126041179315863e-07, "y": 0.30303030303030304, "cat25k": 7}, {"ncat": 10, "s": 0.6037296037296038, "cat": 1, "x": 0.14763014763014762, "os": 0.5126948093508952, "ncat25k": 6, "term": "aimed", "bg": 2.167918064522169e-06, "y": 0.3038073038073038, "cat25k": 7}, {"ncat": 10, "s": 0.6037296037296038, "cat": 1, "x": 0.14840714840714841, "os": 0.5126948093508952, "ncat25k": 6, "term": "choose", "bg": 2.662885664898695e-07, "y": 0.31002331002331, "cat25k": 7}, {"ncat": 10, "s": 0.993006993006993, "cat": 4, "x": 0.14918414918414918, "os": 0.8448002402885461, "ncat25k": 6, "term": "inference", "bg": 4.403866734798446e-05, "y": 0.8275058275058275, "cat25k": 30}, {"ncat": 10, "s": 0.3154623154623155, "cat": 0, "x": 0.14996114996114995, "os": 0.14024305637885748, "ncat25k": 6, "term": "fundamental", "bg": 1.2688570395903063e-06, "y": 0.046620046620046623, "cat25k": 0}, {"ncat": 10, "s": 0.6037296037296038, "cat": 1, "x": 0.15073815073815075, "os": 0.5126948093508952, "ncat25k": 6, "term": "leave", "bg": 3.0953059474263685e-07, "y": 0.3108003108003108, "cat25k": 7}, {"ncat": 10, "s": 0.9751359751359752, "cat": 3, "x": 0.15151515151515152, "os": 0.7851705407002099, "ncat25k": 6, "term": "off", "bg": 1.6188443619099716e-07, "y": 0.7373737373737373, "cat25k": 22}, {"ncat": 10, "s": 0.3154623154623155, "cat": 0, "x": 0.1522921522921523, "os": 0.14024305637885748, "ncat25k": 6, "term": "unstructured", "bg": 2.9469503597546323e-05, "y": 0.06060606060606061, "cat25k": 0}, {"ncat": 10, "s": 0.3154623154623155, "cat": 0, "x": 0.15306915306915306, "os": 0.14024305637885748, "ncat25k": 6, "term": "gensim", "bg": 0.00034006665306400053, "y": 0.06371406371406371, "cat25k": 0}, {"ncat": 10, "s": 0.6037296037296038, "cat": 1, "x": 0.15384615384615385, "os": 0.5126948093508952, "ncat25k": 6, "term": "raw", "bg": 1.0345463269646264e-06, "y": 0.32167832167832167, "cat25k": 7}, {"ncat": 10, "s": 0.6037296037296038, "cat": 1, "x": 0.15462315462315462, "os": 0.5126948093508952, "ncat25k": 6, "term": "begin", "bg": 5.268643985827524e-07, "y": 0.3317793317793318, "cat25k": 7}, {"ncat": 10, "s": 0.3154623154623155, "cat": 0, "x": 0.1554001554001554, "os": 0.14024305637885748, "ncat25k": 6, "term": "longer", "bg": 4.060420234299536e-07, "y": 0.07459207459207459, "cat25k": 0}, {"ncat": 10, "s": 0.3154623154623155, "cat": 0, "x": 0.1561771561771562, "os": 0.14024305637885748, "ncat25k": 6, "term": "numerical", "bg": 5.818240353205978e-06, "y": 0.07536907536907538, "cat25k": 0}, {"ncat": 10, "s": 0.8974358974358975, "cat": 2, "x": 0.15695415695415696, "os": 0.6923655084958391, "ncat25k": 6, "term": "typically", "bg": 1.2779185263043556e-06, "y": 0.5796425796425796, "cat25k": 15}, {"ncat": 10, "s": 0.6037296037296038, "cat": 1, "x": 0.15773115773115773, "os": 0.5126948093508952, "ncat25k": 6, "term": "syntax", "bg": 1.8795324926059192e-06, "y": 0.3395493395493395, "cat25k": 7}, {"ncat": 10, "s": 0.6037296037296038, "cat": 1, "x": 0.1585081585081585, "os": 0.5126948093508952, "ncat25k": 6, "term": "tuning", "bg": 4.474709272216176e-06, "y": 0.34265734265734266, "cat25k": 7}, {"ncat": 10, "s": 0.3154623154623155, "cat": 0, "x": 0.1592851592851593, "os": 0.14024305637885748, "ncat25k": 6, "term": "entities", "bg": 2.61074906306743e-06, "y": 0.08469308469308469, "cat25k": 0}, {"ncat": 10, "s": 0.6037296037296038, "cat": 1, "x": 0.16006216006216006, "os": 0.5126948093508952, "ncat25k": 6, "term": "intuition", "bg": 1.6872733715043783e-05, "y": 0.34965034965034963, "cat25k": 7}, {"ncat": 10, "s": 0.6037296037296038, "cat": 1, "x": 0.16083916083916083, "os": 0.5126948093508952, "ncat25k": 6, "term": "mathematical", "bg": 3.6070507742895194e-06, "y": 0.351981351981352, "cat25k": 7}, {"ncat": 10, "s": 0.3154623154623155, "cat": 0, "x": 0.16161616161616163, "os": 0.14024305637885748, "ncat25k": 6, "term": "hours", "bg": 1.3107446919843492e-07, "y": 0.0947940947940948, "cat25k": 0}, {"ncat": 10, "s": 0.3154623154623155, "cat": 0, "x": 0.1623931623931624, "os": 0.14024305637885748, "ncat25k": 6, "term": "properties", "bg": 4.271761164650335e-07, "y": 0.09945609945609946, "cat25k": 0}, {"ncat": 10, "s": 0.9751359751359752, "cat": 3, "x": 0.16317016317016317, "os": 0.7851705407002099, "ncat25k": 6, "term": "module", "bg": 7.23246970261938e-07, "y": 0.7599067599067599, "cat25k": 22}, {"ncat": 10, "s": 0.3154623154623155, "cat": 0, "x": 0.16394716394716394, "os": 0.14024305637885748, "ncat25k": 6, "term": "regular", "bg": 3.8445326732655187e-07, "y": 0.10178710178710179, "cat25k": 0}, {"ncat": 10, "s": 0.6037296037296038, "cat": 1, "x": 0.16472416472416473, "os": 0.5126948093508952, "ncat25k": 6, "term": "missing", "bg": 9.462668755912603e-07, "y": 0.37606837606837606, "cat25k": 7}, {"ncat": 10, "s": 0.6037296037296038, "cat": 1, "x": 0.1655011655011655, "os": 0.5126948093508952, "ncat25k": 6, "term": "forecasting", "bg": 1.2372308636236972e-05, "y": 0.3815073815073815, "cat25k": 7}, {"ncat": 10, "s": 0.3154623154623155, "cat": 0, "x": 0.16627816627816627, "os": 0.14024305637885748, "ncat25k": 6, "term": "tables", "bg": 7.733277278120606e-07, "y": 0.1048951048951049, "cat25k": 0}, {"ncat": 10, "s": 0.3154623154623155, "cat": 0, "x": 0.16705516705516704, "os": 0.14024305637885748, "ncat25k": 6, "term": "release", "bg": 2.072399053701144e-07, "y": 0.10644910644910645, "cat25k": 0}, {"ncat": 10, "s": 0.6037296037296038, "cat": 1, "x": 0.16783216783216784, "os": 0.5126948093508952, "ncat25k": 6, "term": "trying", "bg": 4.168095469532047e-07, "y": 0.3900543900543901, "cat25k": 7}, {"ncat": 10, "s": 0.8974358974358975, "cat": 2, "x": 0.1686091686091686, "os": 0.6923655084958391, "ncat25k": 6, "term": "probability", "bg": 2.299441381375613e-06, "y": 0.6013986013986014, "cat25k": 15}, {"ncat": 10, "s": 0.3154623154623155, "cat": 0, "x": 0.16938616938616938, "os": 0.14024305637885748, "ncat25k": 6, "term": "platforms", "bg": 2.1091393754702723e-06, "y": 0.108003108003108, "cat25k": 0}, {"ncat": 10, "s": 0.6037296037296038, "cat": 1, "x": 0.17016317016317017, "os": 0.5126948093508952, "ncat25k": 6, "term": "fitting", "bg": 4.306390535522541e-06, "y": 0.39316239316239315, "cat25k": 7}, {"ncat": 10, "s": 0.3154623154623155, "cat": 0, "x": 0.17094017094017094, "os": 0.14024305637885748, "ncat25k": 6, "term": "+", "bg": 0.0, "y": 0.12354312354312354, "cat25k": 0}, {"ncat": 10, "s": 0.3154623154623155, "cat": 0, "x": 0.1717171717171717, "os": 0.14024305637885748, "ncat25k": 6, "term": "historical", "bg": 7.039122598708912e-07, "y": 0.1258741258741259, "cat25k": 0}, {"ncat": 10, "s": 0.3154623154623155, "cat": 0, "x": 0.17249417249417248, "os": 0.14024305637885748, "ncat25k": 6, "term": "twitter", "bg": 0.00024282977632679488, "y": 0.12975912975912976, "cat25k": 0}, {"ncat": 10, "s": 0.3154623154623155, "cat": 0, "x": 0.17327117327117328, "os": 0.14024305637885748, "ncat25k": 6, "term": "filtering", "bg": 3.2062621506063488e-06, "y": 0.13131313131313133, "cat25k": 0}, {"ncat": 10, "s": 0.8974358974358975, "cat": 2, "x": 0.17404817404817405, "os": 0.6923655084958391, "ncat25k": 6, "term": "functional", "bg": 8.098987154958731e-07, "y": 0.62004662004662, "cat25k": 15}, {"ncat": 10, "s": 0.3154623154623155, "cat": 0, "x": 0.17482517482517482, "os": 0.14024305637885748, "ncat25k": 6, "term": "messy", "bg": 1.0167285741398476e-05, "y": 0.13908313908313907, "cat25k": 0}, {"ncat": 10, "s": 0.6037296037296038, "cat": 1, "x": 0.17560217560217561, "os": 0.5126948093508952, "ncat25k": 6, "term": "estimate", "bg": 1.1737516801013832e-06, "y": 0.4172494172494173, "cat25k": 7}, {"ncat": 10, "s": 0.8974358974358975, "cat": 2, "x": 0.17637917637917638, "os": 0.6923655084958391, "ncat25k": 6, "term": "samples", "bg": 1.0064644203253076e-06, "y": 0.6254856254856255, "cat25k": 15}, {"ncat": 10, "s": 0.3154623154623155, "cat": 0, "x": 0.17715617715617715, "os": 0.14024305637885748, "ncat25k": 6, "term": "computations", "bg": 1.2171765520674658e-05, "y": 0.14763014763014762, "cat25k": 0}, {"ncat": 10, "s": 0.3154623154623155, "cat": 0, "x": 0.17793317793317792, "os": 0.14024305637885748, "ncat25k": 6, "term": "properly", "bg": 9.409467910115306e-07, "y": 0.14840714840714841, "cat25k": 0}, {"ncat": 10, "s": 0.6037296037296038, "cat": 1, "x": 0.17871017871017872, "os": 0.5126948093508952, "ncat25k": 6, "term": "response", "bg": 2.850934980163254e-07, "y": 0.42735042735042733, "cat25k": 7}, {"ncat": 10, "s": 0.6037296037296038, "cat": 1, "x": 0.1794871794871795, "os": 0.5126948093508952, "ncat25k": 6, "term": "luigi", "bg": 3.693763278584286e-05, "y": 0.432012432012432, "cat25k": 7}, {"ncat": 10, "s": 0.6037296037296038, "cat": 1, "x": 0.18026418026418026, "os": 0.5126948093508952, "ncat25k": 6, "term": "successful", "bg": 5.560792998739183e-07, "y": 0.4366744366744367, "cat25k": 7}, {"ncat": 10, "s": 0.6037296037296038, "cat": 1, "x": 0.18104118104118105, "os": 0.5126948093508952, "ncat25k": 6, "term": "involved", "bg": 4.3678598091715643e-07, "y": 0.4397824397824398, "cat25k": 7}, {"ncat": 10, "s": 0.9751359751359752, "cat": 3, "x": 0.18181818181818182, "os": 0.7851705407002099, "ncat25k": 6, "term": "bit", "bg": 3.405021480030771e-07, "y": 0.7847707847707848, "cat25k": 22}, {"ncat": 10, "s": 0.6037296037296038, "cat": 1, "x": 0.1825951825951826, "os": 0.5126948093508952, "ncat25k": 6, "term": "actual", "bg": 5.155780501738061e-07, "y": 0.44133644133644134, "cat25k": 7}, {"ncat": 10, "s": 0.6037296037296038, "cat": 1, "x": 0.18337218337218336, "os": 0.5126948093508952, "ncat25k": 6, "term": "workflows", "bg": 5.147317845268409e-05, "y": 0.4428904428904429, "cat25k": 7}, {"ncat": 10, "s": 0.6037296037296038, "cat": 1, "x": 0.18414918414918416, "os": 0.5126948093508952, "ncat25k": 6, "term": "adding", "bg": 9.659900179049965e-07, "y": 0.44755244755244755, "cat25k": 7}, {"ncat": 10, "s": 0.3154623154623155, "cat": 0, "x": 0.18492618492618493, "os": 0.14024305637885748, "ncat25k": 6, "term": "financial", "bg": 2.559820995759589e-07, "y": 0.1717171717171717, "cat25k": 0}, {"ncat": 10, "s": 0.3154623154623155, "cat": 0, "x": 0.1857031857031857, "os": 0.14024305637885748, "ncat25k": 6, "term": "availability", "bg": 2.726094806489523e-07, "y": 0.17327117327117328, "cat25k": 0}, {"ncat": 10, "s": 0.3154623154623155, "cat": 0, "x": 0.1864801864801865, "os": 0.14024305637885748, "ncat25k": 6, "term": "interfaces", "bg": 1.78256354676993e-06, "y": 0.17404817404817405, "cat25k": 0}, {"ncat": 10, "s": 0.8974358974358975, "cat": 2, "x": 0.18725718725718726, "os": 0.6923655084958391, "ncat25k": 6, "term": "dimensional", "bg": 2.255707998599581e-06, "y": 0.6418026418026418, "cat25k": 15}, {"ncat": 10, "s": 0.3154623154623155, "cat": 0, "x": 0.18803418803418803, "os": 0.14024305637885748, "ncat25k": 6, "term": "javascript", "bg": 1.236292319155353e-06, "y": 0.17637917637917638, "cat25k": 0}, {"ncat": 10, "s": 0.8974358974358975, "cat": 2, "x": 0.1888111888111888, "os": 0.6923655084958391, "ncat25k": 6, "term": "added", "bg": 2.3623103773460104e-07, "y": 0.6425796425796426, "cat25k": 15}, {"ncat": 10, "s": 0.3154623154623155, "cat": 0, "x": 0.1895881895881896, "os": 0.14024305637885748, "ncat25k": 6, "term": "describing", "bg": 2.5846618926238404e-06, "y": 0.17793317793317792, "cat25k": 0}, {"ncat": 10, "s": 0.3154623154623155, "cat": 0, "x": 0.19036519036519037, "os": 0.14024305637885748, "ncat25k": 6, "term": "amounts", "bg": 7.406895907626042e-07, "y": 0.18337218337218336, "cat25k": 0}, {"ncat": 10, "s": 0.3154623154623155, "cat": 0, "x": 0.19114219114219114, "os": 0.14024305637885748, "ncat25k": 6, "term": "binary", "bg": 3.385839917649924e-06, "y": 0.18803418803418803, "cat25k": 0}, {"ncat": 10, "s": 0.3154623154623155, "cat": 0, "x": 0.1919191919191919, "os": 0.14024305637885748, "ncat25k": 6, "term": "word2vec", "bg": 0.0, "y": 0.1895881895881896, "cat25k": 0}, {"ncat": 10, "s": 0.8974358974358975, "cat": 2, "x": 0.1926961926961927, "os": 0.6923655084958391, "ncat25k": 6, "term": "vector", "bg": 2.618120397757318e-06, "y": 0.6526806526806527, "cat25k": 15}, {"ncat": 10, "s": 0.6037296037296038, "cat": 1, "x": 0.19347319347319347, "os": 0.5126948093508952, "ncat25k": 6, "term": "valuable", "bg": 1.3676568084064393e-06, "y": 0.4630924630924631, "cat25k": 7}, {"ncat": 10, "s": 0.3154623154623155, "cat": 0, "x": 0.19425019425019424, "os": 0.14024305637885748, "ncat25k": 6, "term": "signal", "bg": 6.515463104161494e-07, "y": 0.19658119658119658, "cat25k": 0}, {"ncat": 10, "s": 0.6037296037296038, "cat": 1, "x": 0.19502719502719504, "os": 0.5126948093508952, "ncat25k": 6, "term": "serve", "bg": 7.06224798469926e-07, "y": 0.4662004662004662, "cat25k": 7}, {"ncat": 10, "s": 0.6037296037296038, "cat": 1, "x": 0.1958041958041958, "os": 0.5126948093508952, "ncat25k": 6, "term": "scala", "bg": 4.2944116490871075e-05, "y": 0.466977466977467, "cat25k": 7}, {"ncat": 10, "s": 0.6037296037296038, "cat": 1, "x": 0.19658119658119658, "os": 0.5126948093508952, "ncat25k": 6, "term": "comparison", "bg": 6.145101888722204e-07, "y": 0.4693084693084693, "cat25k": 7}, {"ncat": 10, "s": 0.3154623154623155, "cat": 0, "x": 0.19735819735819735, "os": 0.14024305637885748, "ncat25k": 6, "term": "request", "bg": 2.4050421805516955e-07, "y": 0.20124320124320125, "cat25k": 0}, {"ncat": 10, "s": 0.6037296037296038, "cat": 1, "x": 0.19813519813519814, "os": 0.5126948093508952, "ncat25k": 6, "term": "instance", "bg": 9.87761896043911e-07, "y": 0.4700854700854701, "cat25k": 7}, {"ncat": 10, "s": 0.3154623154623155, "cat": 0, "x": 0.1989121989121989, "os": 0.14024305637885748, "ncat25k": 6, "term": "incorporate", "bg": 3.2440003227780323e-06, "y": 0.20745920745920746, "cat25k": 0}, {"ncat": 10, "s": 0.6037296037296038, "cat": 1, "x": 0.19968919968919968, "os": 0.5126948093508952, "ncat25k": 6, "term": "side", "bg": 1.6988385515082432e-07, "y": 0.4801864801864802, "cat25k": 7}, {"ncat": 10, "s": 0.3154623154623155, "cat": 0, "x": 0.20046620046620048, "os": 0.14024305637885748, "ncat25k": 6, "term": "clear", "bg": 2.2582227082720414e-07, "y": 0.21367521367521367, "cat25k": 0}, {"ncat": 10, "s": 0.8974358974358975, "cat": 2, "x": 0.20124320124320125, "os": 0.6923655084958391, "ncat25k": 6, "term": "solved", "bg": 3.8079677037471424e-06, "y": 0.6736596736596736, "cat25k": 15}, {"ncat": 10, "s": 0.8974358974358975, "cat": 2, "x": 0.20202020202020202, "os": 0.6923655084958391, "ncat25k": 6, "term": "cross", "bg": 4.84205486048157e-07, "y": 0.6752136752136753, "cat25k": 15}, {"ncat": 10, "s": 0.3154623154623155, "cat": 0, "x": 0.20279720279720279, "os": 0.14024305637885748, "ncat25k": 6, "term": "sensitive", "bg": 1.3610907194709839e-06, "y": 0.21756021756021757, "cat25k": 0}, {"ncat": 10, "s": 0.3154623154623155, "cat": 0, "x": 0.20357420357420358, "os": 0.14024305637885748, "ncat25k": 6, "term": "helping", "bg": 9.002767164174586e-07, "y": 0.2191142191142191, "cat25k": 0}, {"ncat": 10, "s": 0.3154623154623155, "cat": 0, "x": 0.20435120435120435, "os": 0.14024305637885748, "ncat25k": 6, "term": "significant", "bg": 3.763498664685014e-07, "y": 0.22921522921522922, "cat25k": 0}, {"ncat": 10, "s": 0.3154623154623155, "cat": 0, "x": 0.20512820512820512, "os": 0.14024305637885748, "ncat25k": 6, "term": "programmers", "bg": 4.693561011960638e-06, "y": 0.23154623154623155, "cat25k": 0}, {"ncat": 10, "s": 0.6037296037296038, "cat": 1, "x": 0.20590520590520592, "os": 0.5126948093508952, "ncat25k": 6, "term": "led", "bg": 5.493431883811043e-07, "y": 0.4972804972804973, "cat25k": 7}, {"ncat": 10, "s": 0.8974358974358975, "cat": 2, "x": 0.2066822066822067, "os": 0.6923655084958391, "ncat25k": 6, "term": "maintain", "bg": 9.679462482509286e-07, "y": 0.6845376845376845, "cat25k": 15}, {"ncat": 10, "s": 0.3154623154623155, "cat": 0, "x": 0.20745920745920746, "os": 0.14024305637885748, "ncat25k": 6, "term": "detail", "bg": 4.384816013558553e-07, "y": 0.24164724164724163, "cat25k": 0}, {"ncat": 10, "s": 0.9751359751359752, "cat": 3, "x": 0.20823620823620823, "os": 0.7851705407002099, "ncat25k": 6, "term": "compute", "bg": 5.3652663768528615e-06, "y": 0.7972027972027972, "cat25k": 22}, {"ncat": 10, "s": 0.6037296037296038, "cat": 1, "x": 0.20901320901320902, "os": 0.5126948093508952, "ncat25k": 6, "term": "program", "bg": 1.1733852560655014e-07, "y": 0.5019425019425019, "cat25k": 7}, {"ncat": 10, "s": 0.3154623154623155, "cat": 0, "x": 0.2097902097902098, "os": 0.14024305637885748, "ncat25k": 6, "term": "reports", "bg": 5.408536698238737e-07, "y": 0.24242424242424243, "cat25k": 0}, {"ncat": 10, "s": 0.3154623154623155, "cat": 0, "x": 0.21056721056721056, "os": 0.14024305637885748, "ncat25k": 6, "term": "attempt", "bg": 8.671823169227358e-07, "y": 0.2463092463092463, "cat25k": 0}, {"ncat": 10, "s": 0.3154623154623155, "cat": 0, "x": 0.21134421134421136, "os": 0.14024305637885748, "ncat25k": 6, "term": "outcomes", "bg": 1.668625099500462e-06, "y": 0.24708624708624707, "cat25k": 0}, {"ncat": 10, "s": 0.3154623154623155, "cat": 0, "x": 0.21212121212121213, "os": 0.14024305637885748, "ncat25k": 6, "term": "trust", "bg": 3.618288247170191e-07, "y": 0.24786324786324787, "cat25k": 0}, {"ncat": 10, "s": 0.6037296037296038, "cat": 1, "x": 0.2128982128982129, "os": 0.5126948093508952, "ncat25k": 6, "term": "digital", "bg": 2.3707872191552493e-07, "y": 0.5066045066045066, "cat25k": 7}, {"ncat": 11, "s": 0.9557109557109558, "cat": 3, "x": 0.21367521367521367, "os": 0.7645605844138219, "ncat25k": 6, "term": "recognition", "bg": 1.891022106903892e-06, "y": 0.696969696969697, "cat25k": 22}, {"ncat": 11, "s": 0.8787878787878788, "cat": 2, "x": 0.21445221445221446, "os": 0.6726182063079778, "ncat25k": 6, "term": "convolutional", "bg": 0.0002848387223509999, "y": 0.5112665112665112, "cat25k": 15}, {"ncat": 11, "s": 0.9557109557109558, "cat": 3, "x": 0.21522921522921523, "os": 0.7645605844138219, "ncat25k": 6, "term": "server", "bg": 3.2487325898227614e-07, "y": 0.7000777000777001, "cat25k": 22}, {"ncat": 11, "s": 0.8787878787878788, "cat": 2, "x": 0.216006216006216, "os": 0.6726182063079778, "ncat25k": 6, "term": "enables", "bg": 1.982539631852303e-06, "y": 0.5283605283605284, "cat25k": 15}, {"ncat": 11, "s": 0.5843045843045843, "cat": 1, "x": 0.21678321678321677, "os": 0.5, "ncat25k": 6, "term": "something", "bg": 1.9703862018340703e-07, "y": 0.27195027195027194, "cat25k": 7}, {"ncat": 11, "s": 0.2377622377622378, "cat": 0, "x": 0.21756021756021757, "os": 0.1181323185433224, "ncat25k": 6, "term": "domains", "bg": 1.7680623732623884e-06, "y": 0.02097902097902098, "cat25k": 0}, {"ncat": 11, "s": 0.5843045843045843, "cat": 1, "x": 0.21833721833721834, "os": 0.5, "ncat25k": 6, "term": "issue", "bg": 1.867053760848399e-07, "y": 0.2812742812742813, "cat25k": 7}, {"ncat": 11, "s": 0.5843045843045843, "cat": 1, "x": 0.2191142191142191, "os": 0.5, "ncat25k": 6, "term": "scratch", "bg": 3.43228342423604e-06, "y": 0.28205128205128205, "cat25k": 7}, {"ncat": 11, "s": 0.2377622377622378, "cat": 0, "x": 0.2198912198912199, "os": 0.1181323185433224, "ncat25k": 6, "term": "developer", "bg": 8.331516831233102e-07, "y": 0.02641802641802642, "cat25k": 0}, {"ncat": 11, "s": 0.8787878787878788, "cat": 2, "x": 0.22066822066822067, "os": 0.6726182063079778, "ncat25k": 6, "term": "motivation", "bg": 3.4978888222944667e-06, "y": 0.5407925407925408, "cat25k": 15}, {"ncat": 11, "s": 0.5843045843045843, "cat": 1, "x": 0.22144522144522144, "os": 0.5, "ncat25k": 6, "term": "involve", "bg": 1.9844759408338423e-06, "y": 0.29526029526029524, "cat25k": 7}, {"ncat": 11, "s": 0.2377622377622378, "cat": 0, "x": 0.2222222222222222, "os": 0.1181323185433224, "ncat25k": 6, "term": "dependencies", "bg": 9.157249198969627e-06, "y": 0.037296037296037296, "cat25k": 0}, {"ncat": 11, "s": 0.2377622377622378, "cat": 0, "x": 0.222999222999223, "os": 0.1181323185433224, "ncat25k": 6, "term": "collect", "bg": 1.618835172132923e-06, "y": 0.039627039627039624, "cat25k": 0}, {"ncat": 11, "s": 0.8787878787878788, "cat": 2, "x": 0.22377622377622378, "os": 0.6726182063079778, "ncat25k": 6, "term": "description", "bg": 1.8328178219254265e-07, "y": 0.5516705516705517, "cat25k": 15}, {"ncat": 11, "s": 0.2377622377622378, "cat": 0, "x": 0.22455322455322455, "os": 0.1181323185433224, "ncat25k": 6, "term": "statsmodels", "bg": 0.0002210564799306223, "y": 0.04895104895104895, "cat25k": 0}, {"ncat": 11, "s": 0.2377622377622378, "cat": 0, "x": 0.22533022533022534, "os": 0.1181323185433224, "ncat25k": 6, "term": "illustrate", "bg": 4.8391301744163645e-06, "y": 0.05128205128205128, "cat25k": 0}, {"ncat": 11, "s": 0.2377622377622378, "cat": 0, "x": 0.2261072261072261, "os": 0.1181323185433224, "ncat25k": 6, "term": "artificial", "bg": 2.9847808821053528e-06, "y": 0.05516705516705517, "cat25k": 0}, {"ncat": 11, "s": 0.2377622377622378, "cat": 0, "x": 0.22688422688422688, "os": 0.1181323185433224, "ncat25k": 6, "term": "takes", "bg": 3.7995333067780494e-07, "y": 0.05672105672105672, "cat25k": 0}, {"ncat": 11, "s": 0.8787878787878788, "cat": 2, "x": 0.22766122766122765, "os": 0.6726182063079778, "ncat25k": 6, "term": "applying", "bg": 1.7859453791960444e-06, "y": 0.5648795648795649, "cat25k": 15}, {"ncat": 11, "s": 0.8787878787878788, "cat": 2, "x": 0.22843822843822845, "os": 0.6726182063079778, "ncat25k": 6, "term": "exciting", "bg": 1.8803879357835768e-06, "y": 0.5734265734265734, "cat25k": 15}, {"ncat": 11, "s": 0.5843045843045843, "cat": 1, "x": 0.22921522921522922, "os": 0.5, "ncat25k": 6, "term": "later", "bg": 4.471872083464151e-07, "y": 0.3372183372183372, "cat25k": 7}, {"ncat": 11, "s": 0.5843045843045843, "cat": 1, "x": 0.22999222999222999, "os": 0.5, "ncat25k": 6, "term": "gap", "bg": 1.725375441696113e-06, "y": 0.33877233877233875, "cat25k": 7}, {"ncat": 11, "s": 0.2377622377622378, "cat": 0, "x": 0.23076923076923078, "os": 0.1181323185433224, "ncat25k": 6, "term": "nodes", "bg": 2.4721136748499517e-06, "y": 0.08158508158508158, "cat25k": 0}, {"ncat": 11, "s": 0.8787878787878788, "cat": 2, "x": 0.23154623154623155, "os": 0.6726182063079778, "ncat25k": 6, "term": "compared", "bg": 9.207089746826612e-07, "y": 0.5858585858585859, "cat25k": 15}, {"ncat": 11, "s": 0.5843045843045843, "cat": 1, "x": 0.23232323232323232, "os": 0.5, "ncat25k": 6, "term": "free", "bg": 3.7467033730670275e-08, "y": 0.3644133644133644, "cat25k": 7}, {"ncat": 11, "s": 0.8787878787878788, "cat": 2, "x": 0.2331002331002331, "os": 0.6726182063079778, "ncat25k": 6, "term": "identifying", "bg": 3.0415865997157128e-06, "y": 0.5897435897435898, "cat25k": 15}, {"ncat": 11, "s": 0.5843045843045843, "cat": 1, "x": 0.2338772338772339, "os": 0.5, "ncat25k": 6, "term": "view", "bg": 4.648097887454118e-08, "y": 0.3752913752913753, "cat25k": 7}, {"ncat": 11, "s": 0.2377622377622378, "cat": 0, "x": 0.23465423465423466, "os": 0.1181323185433224, "ncat25k": 6, "term": "command", "bg": 6.668000933653491e-07, "y": 0.10722610722610723, "cat25k": 0}, {"ncat": 11, "s": 0.2377622377622378, "cat": 0, "x": 0.23543123543123542, "os": 0.1181323185433224, "ncat25k": 6, "term": "supports", "bg": 1.1022504716770885e-06, "y": 0.10955710955710955, "cat25k": 0}, {"ncat": 11, "s": 0.8787878787878788, "cat": 2, "x": 0.23620823620823622, "os": 0.6726182063079778, "ncat25k": 6, "term": "mainly", "bg": 2.071310215196338e-06, "y": 0.6060606060606061, "cat25k": 15}, {"ncat": 11, "s": 0.2377622377622378, "cat": 0, "x": 0.236985236985237, "os": 0.1181323185433224, "ncat25k": 6, "term": "integrating", "bg": 4.246088943804973e-06, "y": 0.1111111111111111, "cat25k": 0}, {"ncat": 11, "s": 0.2377622377622378, "cat": 0, "x": 0.23776223776223776, "os": 0.1181323185433224, "ncat25k": 6, "term": "ask", "bg": 2.816429834959465e-07, "y": 0.11266511266511267, "cat25k": 0}, {"ncat": 11, "s": 0.2377622377622378, "cat": 0, "x": 0.23853923853923853, "os": 0.1181323185433224, "ncat25k": 6, "term": "depending", "bg": 1.0390796927838089e-06, "y": 0.11810411810411811, "cat25k": 0}, {"ncat": 11, "s": 0.2377622377622378, "cat": 0, "x": 0.23931623931623933, "os": 0.1181323185433224, "ncat25k": 6, "term": "intro", "bg": 1.9907582366792562e-06, "y": 0.11888111888111888, "cat25k": 0}, {"ncat": 11, "s": 0.2377622377622378, "cat": 0, "x": 0.2400932400932401, "os": 0.1181323185433224, "ncat25k": 6, "term": "home", "bg": 1.8794493231651138e-08, "y": 0.12043512043512043, "cat25k": 0}, {"ncat": 11, "s": 0.2377622377622378, "cat": 0, "x": 0.24087024087024086, "os": 0.1181323185433224, "ncat25k": 6, "term": "ui", "bg": 3.6228925143643913e-06, "y": 0.12432012432012432, "cat25k": 0}, {"ncat": 11, "s": 0.5843045843045843, "cat": 1, "x": 0.24164724164724163, "os": 0.5, "ncat25k": 6, "term": "native", "bg": 9.07074317959482e-07, "y": 0.40792540792540793, "cat25k": 7}, {"ncat": 11, "s": 0.2377622377622378, "cat": 0, "x": 0.24242424242424243, "os": 0.1181323185433224, "ncat25k": 6, "term": "factors", "bg": 6.631279318247647e-07, "y": 0.13364413364413363, "cat25k": 0}, {"ncat": 11, "s": 0.2377622377622378, "cat": 0, "x": 0.2432012432012432, "os": 0.1181323185433224, "ncat25k": 6, "term": "matrix", "bg": 1.1273255360688332e-06, "y": 0.13597513597513597, "cat25k": 0}, {"ncat": 11, "s": 0.5843045843045843, "cat": 1, "x": 0.24397824397824397, "os": 0.5, "ncat25k": 6, "term": "evaluation", "bg": 6.80167987039229e-07, "y": 0.411033411033411, "cat25k": 7}, {"ncat": 11, "s": 0.2377622377622378, "cat": 0, "x": 0.24475524475524477, "os": 0.1181323185433224, "ncat25k": 6, "term": "quick", "bg": 3.2671266454270764e-07, "y": 0.13752913752913754, "cat25k": 0}, {"ncat": 11, "s": 0.2377622377622378, "cat": 0, "x": 0.24553224553224554, "os": 0.1181323185433224, "ncat25k": 6, "term": "rows", "bg": 3.467886900470712e-06, "y": 0.1383061383061383, "cat25k": 0}, {"ncat": 11, "s": 0.2377622377622378, "cat": 0, "x": 0.2463092463092463, "os": 0.1181323185433224, "ncat25k": 6, "term": "java", "bg": 5.768076218061328e-07, "y": 0.14374514374514374, "cat25k": 0}, {"ncat": 11, "s": 0.2377622377622378, "cat": 0, "x": 0.24708624708624707, "os": 0.1181323185433224, "ncat25k": 6, "term": "appropriate", "bg": 3.43894638592312e-07, "y": 0.1452991452991453, "cat25k": 0}, {"ncat": 11, "s": 0.5843045843045843, "cat": 1, "x": 0.24786324786324787, "os": 0.5, "ncat25k": 6, "term": "reasons", "bg": 8.151403516808928e-07, "y": 0.42035742035742035, "cat25k": 7}, {"ncat": 11, "s": 0.5843045843045843, "cat": 1, "x": 0.24864024864024864, "os": 0.5, "ncat25k": 6, "term": "improved", "bg": 8.722485193036229e-07, "y": 0.4226884226884227, "cat25k": 7}, {"ncat": 11, "s": 0.2377622377622378, "cat": 0, "x": 0.2494172494172494, "os": 0.1181323185433224, "ncat25k": 6, "term": "story", "bg": 2.1652609744303756e-07, "y": 0.14918414918414918, "cat25k": 0}, {"ncat": 11, "s": 0.5843045843045843, "cat": 1, "x": 0.2501942501942502, "os": 0.5, "ncat25k": 6, "term": "old", "bg": 1.201130011103146e-07, "y": 0.425019425019425, "cat25k": 7}, {"ncat": 11, "s": 0.5843045843045843, "cat": 1, "x": 0.25097125097125095, "os": 0.5, "ncat25k": 6, "term": "mix", "bg": 8.603245039560947e-07, "y": 0.4289044289044289, "cat25k": 7}, {"ncat": 11, "s": 0.5843045843045843, "cat": 1, "x": 0.2517482517482518, "os": 0.5, "ncat25k": 6, "term": "summary", "bg": 3.720042451636441e-07, "y": 0.43045843045843046, "cat25k": 7}, {"ncat": 11, "s": 0.5843045843045843, "cat": 1, "x": 0.25252525252525254, "os": 0.5, "ncat25k": 6, "term": "keeping", "bg": 1.023705636436613e-06, "y": 0.43278943278943277, "cat25k": 7}, {"ncat": 11, "s": 0.2377622377622378, "cat": 0, "x": 0.2533022533022533, "os": 0.1181323185433224, "ncat25k": 6, "term": "deploying", "bg": 1.079301691805402e-05, "y": 0.15462315462315462, "cat25k": 0}, {"ncat": 11, "s": 0.5843045843045843, "cat": 1, "x": 0.2540792540792541, "os": 0.5, "ncat25k": 6, "term": "engineers", "bg": 2.607336830452198e-06, "y": 0.4382284382284382, "cat25k": 7}, {"ncat": 11, "s": 0.8787878787878788, "cat": 2, "x": 0.25485625485625485, "os": 0.6726182063079778, "ncat25k": 6, "term": "upon", "bg": 3.2680102616675633e-07, "y": 0.6379176379176379, "cat25k": 15}, {"ncat": 11, "s": 0.8787878787878788, "cat": 2, "x": 0.2556332556332556, "os": 0.6726182063079778, "ncat25k": 6, "term": "engines", "bg": 1.4238134620850933e-06, "y": 0.6386946386946387, "cat25k": 15}, {"ncat": 11, "s": 0.2377622377622378, "cat": 0, "x": 0.2564102564102564, "os": 0.1181323185433224, "ncat25k": 6, "term": "benefit", "bg": 5.855151334959867e-07, "y": 0.16394716394716394, "cat25k": 0}, {"ncat": 11, "s": 0.9557109557109558, "cat": 3, "x": 0.2571872571872572, "os": 0.7645605844138219, "ncat25k": 6, "term": "computation", "bg": 5.164775268576922e-06, "y": 0.7855477855477856, "cat25k": 22}, {"ncat": 11, "s": 0.2377622377622378, "cat": 0, "x": 0.257964257964258, "os": 0.1181323185433224, "ncat25k": 6, "term": "parameter", "bg": 1.4273952294131914e-06, "y": 0.17016317016317017, "cat25k": 0}, {"ncat": 11, "s": 0.5843045843045843, "cat": 1, "x": 0.25874125874125875, "os": 0.5, "ncat25k": 6, "term": "objective", "bg": 1.2394732752076439e-06, "y": 0.4506604506604507, "cat25k": 7}, {"ncat": 11, "s": 0.2377622377622378, "cat": 0, "x": 0.2595182595182595, "os": 0.1181323185433224, "ncat25k": 6, "term": "touch", "bg": 7.03679257365059e-07, "y": 0.17715617715617715, "cat25k": 0}, {"ncat": 11, "s": 0.2377622377622378, "cat": 0, "x": 0.2602952602952603, "os": 0.1181323185433224, "ncat25k": 6, "term": "docker", "bg": 0.00022702094041154356, "y": 0.18026418026418026, "cat25k": 0}, {"ncat": 11, "s": 0.8787878787878788, "cat": 2, "x": 0.26107226107226106, "os": 0.6726182063079778, "ncat25k": 6, "term": "thus", "bg": 4.70074660951096e-07, "y": 0.6433566433566433, "cat25k": 15}, {"ncat": 11, "s": 0.2377622377622378, "cat": 0, "x": 0.2618492618492618, "os": 0.1181323185433224, "ncat25k": 6, "term": "and/or", "bg": 0.0, "y": 0.18104118104118105, "cat25k": 0}, {"ncat": 11, "s": 0.9557109557109558, "cat": 3, "x": 0.26262626262626265, "os": 0.7645605844138219, "ncat25k": 6, "term": "5", "bg": 0.0, "y": 0.790986790986791, "cat25k": 22}, {"ncat": 11, "s": 0.5843045843045843, "cat": 1, "x": 0.2634032634032634, "os": 0.5, "ncat25k": 6, "term": "customers", "bg": 5.211089634578971e-07, "y": 0.45843045843045843, "cat25k": 7}, {"ncat": 11, "s": 0.5843045843045843, "cat": 1, "x": 0.2641802641802642, "os": 0.5, "ncat25k": 6, "term": "capture", "bg": 1.4492370714396956e-06, "y": 0.45998445998445997, "cat25k": 7}, {"ncat": 11, "s": 0.2377622377622378, "cat": 0, "x": 0.26495726495726496, "os": 0.1181323185433224, "ncat25k": 6, "term": "volume", "bg": 3.4945725524874044e-07, "y": 0.1919191919191919, "cat25k": 0}, {"ncat": 11, "s": 0.2377622377622378, "cat": 0, "x": 0.26573426573426573, "os": 0.1181323185433224, "ncat25k": 6, "term": "batch", "bg": 5.784215805570879e-06, "y": 0.19735819735819735, "cat25k": 0}, {"ncat": 11, "s": 0.9883449883449884, "cat": 4, "x": 0.2665112665112665, "os": 0.8257374459562334, "ncat25k": 6, "term": "trained", "bg": 5.099906810131416e-06, "y": 0.8523698523698524, "cat25k": 30}, {"ncat": 11, "s": 0.5843045843045843, "cat": 1, "x": 0.26728826728826727, "os": 0.5, "ncat25k": 6, "term": "gives", "bg": 6.991973171099745e-07, "y": 0.46464646464646464, "cat25k": 7}, {"ncat": 11, "s": 0.2377622377622378, "cat": 0, "x": 0.2680652680652681, "os": 0.1181323185433224, "ncat25k": 6, "term": "communicate", "bg": 1.6780521231974001e-06, "y": 0.19968919968919968, "cat25k": 0}, {"ncat": 11, "s": 0.5843045843045843, "cat": 1, "x": 0.26884226884226886, "os": 0.5, "ncat25k": 6, "term": "easier", "bg": 9.63853992431522e-07, "y": 0.4755244755244755, "cat25k": 7}, {"ncat": 11, "s": 0.2377622377622378, "cat": 0, "x": 0.26961926961926963, "os": 0.1181323185433224, "ncat25k": 6, "term": "likely", "bg": 4.3020731493328285e-07, "y": 0.2128982128982129, "cat25k": 0}, {"ncat": 11, "s": 0.2377622377622378, "cat": 0, "x": 0.2703962703962704, "os": 0.1181323185433224, "ncat25k": 6, "term": "lines", "bg": 4.6799460917009696e-07, "y": 0.21522921522921523, "cat25k": 0}, {"ncat": 11, "s": 0.5843045843045843, "cat": 1, "x": 0.27117327117327117, "os": 0.5, "ncat25k": 6, "term": "communities", "bg": 6.962687006748746e-07, "y": 0.48484848484848486, "cat25k": 7}, {"ncat": 11, "s": 0.2377622377622378, "cat": 0, "x": 0.27195027195027194, "os": 0.1181323185433224, "ncat25k": 6, "term": "knowing", "bg": 2.2025917759765932e-06, "y": 0.22066822066822067, "cat25k": 0}, {"ncat": 11, "s": 0.2377622377622378, "cat": 0, "x": 0.2727272727272727, "os": 0.1181323185433224, "ncat25k": 6, "term": "vectors", "bg": 8.94124379289444e-06, "y": 0.222999222999223, "cat25k": 0}, {"ncat": 11, "s": 0.5843045843045843, "cat": 1, "x": 0.27350427350427353, "os": 0.5, "ncat25k": 6, "term": "gain", "bg": 8.100239844986333e-07, "y": 0.49805749805749805, "cat25k": 7}, {"ncat": 11, "s": 0.2377622377622378, "cat": 0, "x": 0.2742812742812743, "os": 0.1181323185433224, "ncat25k": 6, "term": "collaboration", "bg": 1.9225338725023035e-06, "y": 0.23931623931623933, "cat25k": 0}, {"ncat": 11, "s": 0.5843045843045843, "cat": 1, "x": 0.27505827505827507, "os": 0.5, "ncat25k": 6, "term": "alternative", "bg": 5.03418381613823e-07, "y": 0.5042735042735043, "cat25k": 7}, {"ncat": 11, "s": 0.8787878787878788, "cat": 2, "x": 0.27583527583527584, "os": 0.6726182063079778, "ncat25k": 6, "term": "automatic", "bg": 1.1859649972197842e-06, "y": 0.6915306915306916, "cat25k": 15}, {"ncat": 12, "s": 0.5672105672105672, "cat": 1, "x": 0.2766122766122766, "os": 0.2750877327842265, "ncat25k": 7, "term": "modules", "bg": 1.609323972485871e-06, "y": 0.2517482517482518, "cat25k": 7}, {"ncat": 12, "s": 0.5672105672105672, "cat": 1, "x": 0.2773892773892774, "os": 0.2750877327842265, "ncat25k": 7, "term": "taken", "bg": 3.2133139901012027e-07, "y": 0.26107226107226106, "cat25k": 7}, {"ncat": 12, "s": 0.8593628593628594, "cat": 2, "x": 0.27816627816627815, "os": 0.6550573779384496, "ncat25k": 7, "term": "json", "bg": 0.00017903921901208045, "y": 0.5198135198135199, "cat25k": 15}, {"ncat": 12, "s": 0.5672105672105672, "cat": 1, "x": 0.2789432789432789, "os": 0.2750877327842265, "ncat25k": 7, "term": "flask", "bg": 3.470693933700281e-05, "y": 0.2680652680652681, "cat25k": 7}, {"ncat": 12, "s": 0.18181818181818185, "cat": 0, "x": 0.27972027972027974, "os": 0.09884373416999315, "ncat25k": 7, "term": "me", "bg": 4.940578218877934e-08, "y": 0.016317016317016316, "cat25k": 0}, {"ncat": 12, "s": 0.8593628593628594, "cat": 2, "x": 0.2804972804972805, "os": 0.6550573779384496, "ncat25k": 7, "term": "unsupervised", "bg": 5.83557745613448e-05, "y": 0.5314685314685315, "cat25k": 15}, {"ncat": 12, "s": 0.5672105672105672, "cat": 1, "x": 0.2812742812742813, "os": 0.2750877327842265, "ncat25k": 7, "term": "discover", "bg": 1.232183350730864e-06, "y": 0.27816627816627815, "cat25k": 7}, {"ncat": 12, "s": 0.18181818181818185, "cat": 0, "x": 0.28205128205128205, "os": 0.09884373416999315, "ncat25k": 7, "term": "component", "bg": 8.026090009489347e-07, "y": 0.04040404040404041, "cat25k": 0}, {"ncat": 12, "s": 0.18181818181818185, "cat": 0, "x": 0.2828282828282828, "os": 0.09884373416999315, "ncat25k": 7, "term": "above", "bg": 1.8308281419438523e-07, "y": 0.06837606837606838, "cat25k": 0}, {"ncat": 12, "s": 0.18181818181818185, "cat": 0, "x": 0.2836052836052836, "os": 0.09884373416999315, "ncat25k": 7, "term": "ones", "bg": 7.089162192451013e-07, "y": 0.06915306915306915, "cat25k": 0}, {"ncat": 12, "s": 0.18181818181818185, "cat": 0, "x": 0.28438228438228436, "os": 0.09884373416999315, "ncat25k": 7, "term": "least", "bg": 3.233123856843742e-07, "y": 0.07148407148407149, "cat25k": 0}, {"ncat": 12, "s": 0.8593628593628594, "cat": 2, "x": 0.2851592851592852, "os": 0.6550573779384496, "ncat25k": 7, "term": "complexity", "bg": 4.354263235469374e-06, "y": 0.5773115773115773, "cat25k": 15}, {"ncat": 12, "s": 0.18181818181818185, "cat": 0, "x": 0.28593628593628595, "os": 0.09884373416999315, "ncat25k": 7, "term": "differences", "bg": 1.0351047412534744e-06, "y": 0.07614607614607614, "cat25k": 0}, {"ncat": 12, "s": 0.18181818181818185, "cat": 0, "x": 0.2867132867132867, "os": 0.09884373416999315, "ncat25k": 7, "term": "call", "bg": 1.502054149172242e-07, "y": 0.0777000777000777, "cat25k": 0}, {"ncat": 12, "s": 0.5672105672105672, "cat": 1, "x": 0.2874902874902875, "os": 0.2750877327842265, "ncat25k": 7, "term": "avoid", "bg": 8.737005036037006e-07, "y": 0.3434343434343434, "cat25k": 7}, {"ncat": 12, "s": 0.5672105672105672, "cat": 1, "x": 0.28826728826728826, "os": 0.2750877327842265, "ncat25k": 7, "term": "attributes", "bg": 2.635642107267998e-06, "y": 0.3457653457653458, "cat25k": 7}, {"ncat": 12, "s": 0.5672105672105672, "cat": 1, "x": 0.289044289044289, "os": 0.2750877327842265, "ncat25k": 7, "term": "random", "bg": 1.1776397287089474e-06, "y": 0.3605283605283605, "cat25k": 7}, {"ncat": 12, "s": 0.18181818181818185, "cat": 0, "x": 0.2898212898212898, "os": 0.09884373416999315, "ncat25k": 7, "term": "advances", "bg": 4.1370027241074255e-06, "y": 0.09013209013209013, "cat25k": 0}, {"ncat": 12, "s": 0.18181818181818185, "cat": 0, "x": 0.2905982905982906, "os": 0.09884373416999315, "ncat25k": 7, "term": "video", "bg": 9.301622220971427e-08, "y": 0.09634809634809635, "cat25k": 0}, {"ncat": 12, "s": 0.5672105672105672, "cat": 1, "x": 0.2913752913752914, "os": 0.2750877327842265, "ncat25k": 7, "term": "intuitive", "bg": 7.799909383405692e-06, "y": 0.3861693861693862, "cat25k": 7}, {"ncat": 12, "s": 0.5672105672105672, "cat": 1, "x": 0.29215229215229216, "os": 0.2750877327842265, "ncat25k": 7, "term": "line", "bg": 1.35652650963373e-07, "y": 0.39083139083139085, "cat25k": 7}, {"ncat": 12, "s": 0.9502719502719503, "cat": 3, "x": 0.29292929292929293, "os": 0.7456315872341845, "ncat25k": 7, "term": "probabilistic", "bg": 5.169471428008325e-05, "y": 0.77000777000777, "cat25k": 22}, {"ncat": 12, "s": 0.5672105672105672, "cat": 1, "x": 0.2937062937062937, "os": 0.2750877327842265, "ncat25k": 7, "term": "perspective", "bg": 1.292026686442058e-06, "y": 0.3978243978243978, "cat25k": 7}, {"ncat": 12, "s": 0.5672105672105672, "cat": 1, "x": 0.29448329448329447, "os": 0.2750877327842265, "ncat25k": 7, "term": "did", "bg": 1.3067760859322888e-07, "y": 0.4048174048174048, "cat25k": 7}, {"ncat": 12, "s": 0.18181818181818185, "cat": 0, "x": 0.29526029526029524, "os": 0.09884373416999315, "ncat25k": 7, "term": "encountered", "bg": 4.496937264513062e-06, "y": 0.1250971250971251, "cat25k": 0}, {"ncat": 12, "s": 0.8593628593628594, "cat": 2, "x": 0.29603729603729606, "os": 0.6550573779384496, "ncat25k": 7, "term": "dive", "bg": 4.061022355211415e-06, "y": 0.6146076146076146, "cat25k": 15}, {"ncat": 12, "s": 0.8593628593628594, "cat": 2, "x": 0.29681429681429683, "os": 0.6550573779384496, "ncat25k": 7, "term": "wrong", "bg": 6.900127893870513e-07, "y": 0.6153846153846154, "cat25k": 15}, {"ncat": 12, "s": 0.18181818181818185, "cat": 0, "x": 0.2975912975912976, "os": 0.09884373416999315, "ncat25k": 7, "term": "increasingly", "bg": 2.22927717677969e-06, "y": 0.12742812742812742, "cat25k": 0}, {"ncat": 12, "s": 0.18181818181818185, "cat": 0, "x": 0.29836829836829837, "os": 0.09884373416999315, "ncat25k": 7, "term": "post", "bg": 1.0685009306681266e-07, "y": 0.13053613053613053, "cat25k": 0}, {"ncat": 12, "s": 0.18181818181818185, "cat": 0, "x": 0.29914529914529914, "os": 0.09884373416999315, "ncat25k": 7, "term": "collaborative", "bg": 4.125042613163424e-06, "y": 0.1351981351981352, "cat25k": 0}, {"ncat": 12, "s": 0.8593628593628594, "cat": 2, "x": 0.2999222999222999, "os": 0.6550573779384496, "ncat25k": 7, "term": "functionality", "bg": 2.7057133165964923e-06, "y": 0.6262626262626263, "cat25k": 15}, {"ncat": 12, "s": 0.5672105672105672, "cat": 1, "x": 0.3006993006993007, "os": 0.2750877327842265, "ncat25k": 7, "term": "static", "bg": 1.6267032557415084e-06, "y": 0.4219114219114219, "cat25k": 7}, {"ncat": 12, "s": 0.18181818181818185, "cat": 0, "x": 0.3014763014763015, "os": 0.09884373416999315, "ncat25k": 7, "term": "reproducible", "bg": 4.2902190156807506e-05, "y": 0.1522921522921523, "cat25k": 0}, {"ncat": 12, "s": 0.18181818181818185, "cat": 0, "x": 0.30225330225330227, "os": 0.09884373416999315, "ncat25k": 7, "term": "organizations", "bg": 5.897030041903558e-07, "y": 0.15306915306915306, "cat25k": 0}, {"ncat": 12, "s": 0.9502719502719503, "cat": 3, "x": 0.30303030303030304, "os": 0.7456315872341845, "ncat25k": 7, "term": "challenge", "bg": 9.72028712756146e-07, "y": 0.7832167832167832, "cat25k": 22}, {"ncat": 12, "s": 0.5672105672105672, "cat": 1, "x": 0.3038073038073038, "os": 0.2750877327842265, "ncat25k": 7, "term": "huge", "bg": 3.9571970907308836e-07, "y": 0.43512043512043513, "cat25k": 7}, {"ncat": 12, "s": 0.18181818181818185, "cat": 0, "x": 0.3045843045843046, "os": 0.09884373416999315, "ncat25k": 7, "term": "kinds", "bg": 7.514504076690717e-07, "y": 0.1655011655011655, "cat25k": 0}, {"ncat": 12, "s": 0.18181818181818185, "cat": 0, "x": 0.30536130536130535, "os": 0.09884373416999315, "ncat25k": 7, "term": "usually", "bg": 2.7485600267591346e-07, "y": 0.16627816627816627, "cat25k": 0}, {"ncat": 12, "s": 0.18181818181818185, "cat": 0, "x": 0.3061383061383061, "os": 0.09884373416999315, "ncat25k": 7, "term": "choice", "bg": 4.3898185825527487e-07, "y": 0.16938616938616938, "cat25k": 0}, {"ncat": 12, "s": 0.9502719502719503, "cat": 3, "x": 0.30691530691530694, "os": 0.7456315872341845, "ncat25k": 7, "term": "box", "bg": 2.125788591540995e-07, "y": 0.7878787878787878, "cat25k": 22}, {"ncat": 12, "s": 0.18181818181818185, "cat": 0, "x": 0.3076923076923077, "os": 0.09884373416999315, "ncat25k": 7, "term": "contains", "bg": 5.428532813608015e-07, "y": 0.17249417249417248, "cat25k": 0}, {"ncat": 12, "s": 0.5672105672105672, "cat": 1, "x": 0.3084693084693085, "os": 0.2750877327842265, "ncat25k": 7, "term": "leading", "bg": 6.037025932328987e-07, "y": 0.4522144522144522, "cat25k": 7}, {"ncat": 12, "s": 0.18181818181818185, "cat": 0, "x": 0.30924630924630925, "os": 0.09884373416999315, "ncat25k": 7, "term": "bringing", "bg": 1.9890444673889008e-06, "y": 0.18181818181818182, "cat25k": 0}, {"ncat": 12, "s": 0.8593628593628594, "cat": 2, "x": 0.31002331002331, "os": 0.6550573779384496, "ncat25k": 7, "term": "quite", "bg": 5.093780748395247e-07, "y": 0.6456876456876457, "cat25k": 15}, {"ncat": 12, "s": 0.5672105672105672, "cat": 1, "x": 0.3108003108003108, "os": 0.2750877327842265, "ncat25k": 7, "term": "consider", "bg": 5.906264736245873e-07, "y": 0.45376845376845376, "cat25k": 7}, {"ncat": 12, "s": 0.5672105672105672, "cat": 1, "x": 0.31157731157731156, "os": 0.2750877327842265, "ncat25k": 7, "term": "almost", "bg": 4.811108295777497e-07, "y": 0.4553224553224553, "cat25k": 7}, {"ncat": 12, "s": 0.8593628593628594, "cat": 2, "x": 0.3123543123543124, "os": 0.6550573779384496, "ncat25k": 7, "term": "10", "bg": 0.0, "y": 0.6511266511266511, "cat25k": 15}, {"ncat": 12, "s": 0.5672105672105672, "cat": 1, "x": 0.31313131313131315, "os": 0.2750877327842265, "ncat25k": 7, "term": "rest", "bg": 6.363510403562786e-07, "y": 0.46231546231546233, "cat25k": 7}, {"ncat": 12, "s": 0.18181818181818185, "cat": 0, "x": 0.3139083139083139, "os": 0.09884373416999315, "ncat25k": 7, "term": "contain", "bg": 8.088944687627705e-07, "y": 0.1989121989121989, "cat25k": 0}, {"ncat": 12, "s": 0.18181818181818185, "cat": 0, "x": 0.3146853146853147, "os": 0.09884373416999315, "ncat25k": 7, "term": "lives", "bg": 6.531729837203166e-07, "y": 0.20202020202020202, "cat25k": 0}, {"ncat": 12, "s": 0.8593628593628594, "cat": 2, "x": 0.31546231546231546, "os": 0.6550573779384496, "ncat25k": 7, "term": "size", "bg": 1.5816765635735776e-07, "y": 0.6612276612276612, "cat25k": 15}, {"ncat": 12, "s": 0.5672105672105672, "cat": 1, "x": 0.3162393162393162, "os": 0.2750877327842265, "ncat25k": 7, "term": "anyone", "bg": 3.499908659114591e-07, "y": 0.4716394716394716, "cat25k": 7}, {"ncat": 12, "s": 0.9502719502719503, "cat": 3, "x": 0.317016317016317, "os": 0.7456315872341845, "ncat25k": 7, "term": "field", "bg": 3.69257758450761e-07, "y": 0.7925407925407926, "cat25k": 22}, {"ncat": 12, "s": 0.5672105672105672, "cat": 1, "x": 0.3177933177933178, "os": 0.2750877327842265, "ncat25k": 7, "term": "active", "bg": 4.987981991817358e-07, "y": 0.47474747474747475, "cat25k": 7}, {"ncat": 12, "s": 0.8593628593628594, "cat": 2, "x": 0.3185703185703186, "os": 0.6550573779384496, "ncat25k": 7, "term": "less", "bg": 2.3360028440972042e-07, "y": 0.6658896658896659, "cat25k": 15}, {"ncat": 12, "s": 0.5672105672105672, "cat": 1, "x": 0.31934731934731936, "os": 0.2750877327842265, "ncat25k": 7, "term": "sense", "bg": 4.92916692339411e-07, "y": 0.48096348096348096, "cat25k": 7}, {"ncat": 12, "s": 0.18181818181818185, "cat": 0, "x": 0.3201243201243201, "os": 0.09884373416999315, "ncat25k": 7, "term": "track", "bg": 3.022145285119142e-07, "y": 0.21445221445221446, "cat25k": 0}, {"ncat": 12, "s": 0.5672105672105672, "cat": 1, "x": 0.3209013209013209, "os": 0.2750877327842265, "ncat25k": 7, "term": "becomes", "bg": 9.427088972213074e-07, "y": 0.4840714840714841, "cat25k": 7}, {"ncat": 12, "s": 0.9502719502719503, "cat": 3, "x": 0.32167832167832167, "os": 0.7456315872341845, "ncat25k": 7, "term": "times", "bg": 1.871289709561704e-07, "y": 0.7948717948717948, "cat25k": 22}, {"ncat": 12, "s": 0.18181818181818185, "cat": 0, "x": 0.32245532245532244, "os": 0.09884373416999315, "ncat25k": 7, "term": "interact", "bg": 2.964173882886231e-06, "y": 0.22766122766122765, "cat25k": 0}, {"ncat": 12, "s": 0.18181818181818185, "cat": 0, "x": 0.32323232323232326, "os": 0.09884373416999315, "ncat25k": 7, "term": "personal", "bg": 1.5829449562364142e-07, "y": 0.2338772338772339, "cat25k": 0}, {"ncat": 12, "s": 0.18181818181818185, "cat": 0, "x": 0.32400932400932403, "os": 0.09884373416999315, "ncat25k": 7, "term": "collecting", "bg": 3.0061742524610903e-06, "y": 0.23465423465423466, "cat25k": 0}, {"ncat": 12, "s": 0.18181818181818185, "cat": 0, "x": 0.3247863247863248, "os": 0.09884373416999315, "ncat25k": 7, "term": "prototype", "bg": 4.733586163186665e-06, "y": 0.23543123543123542, "cat25k": 0}, {"ncat": 12, "s": 0.18181818181818185, "cat": 0, "x": 0.32556332556332557, "os": 0.09884373416999315, "ncat25k": 7, "term": "achieve", "bg": 1.0928807272232362e-06, "y": 0.23853923853923853, "cat25k": 0}, {"ncat": 12, "s": 0.8593628593628594, "cat": 2, "x": 0.32634032634032634, "os": 0.6550573779384496, "ncat25k": 7, "term": "hand", "bg": 3.29450525529183e-07, "y": 0.6868686868686869, "cat25k": 15}, {"ncat": 12, "s": 0.5672105672105672, "cat": 1, "x": 0.3271173271173271, "os": 0.2750877327842265, "ncat25k": 7, "term": "effort", "bg": 8.323999029323783e-07, "y": 0.5003885003885004, "cat25k": 7}, {"ncat": 12, "s": 0.18181818181818185, "cat": 0, "x": 0.3278943278943279, "os": 0.09884373416999315, "ncat25k": 7, "term": "base", "bg": 4.3895853575723315e-07, "y": 0.2501942501942502, "cat25k": 0}, {"ncat": 13, "s": 0.5501165501165501, "cat": 1, "x": 0.32867132867132864, "os": 0.2480583452418879, "ncat25k": 7, "term": "representations", "bg": 8.02010015322847e-06, "y": 0.2540792540792541, "cat25k": 7}, {"ncat": 13, "s": 0.5501165501165501, "cat": 1, "x": 0.32944832944832947, "os": 0.2480583452418879, "ncat25k": 7, "term": "apps", "bg": 4.658537363682462e-06, "y": 0.2564102564102564, "cat25k": 7}, {"ncat": 13, "s": 0.13830613830613833, "cat": 0, "x": 0.33022533022533024, "os": 0.08213270841683162, "ncat25k": 7, "term": "plotting", "bg": 2.0921486890073278e-05, "y": 0.001554001554001554, "cat25k": 0}, {"ncat": 13, "s": 0.13830613830613833, "cat": 0, "x": 0.331002331002331, "os": 0.08213270841683162, "ncat25k": 7, "term": "dataframe", "bg": 0.00027099047016846577, "y": 0.002331002331002331, "cat25k": 0}, {"ncat": 13, "s": 0.13830613830613833, "cat": 0, "x": 0.3317793317793318, "os": 0.08213270841683162, "ncat25k": 7, "term": "visualize", "bg": 1.9552100482154798e-05, "y": 0.004662004662004662, "cat25k": 0}, {"ncat": 13, "s": 0.13830613830613833, "cat": 0, "x": 0.33255633255633255, "os": 0.08213270841683162, "ncat25k": 7, "term": "dealing", "bg": 1.6072218447164244e-06, "y": 0.005439005439005439, "cat25k": 0}, {"ncat": 13, "s": 0.5501165501165501, "cat": 1, "x": 0.3333333333333333, "os": 0.2480583452418879, "ncat25k": 7, "term": "repository", "bg": 3.3295291612927145e-06, "y": 0.26884226884226886, "cat25k": 7}, {"ncat": 13, "s": 0.9261849261849262, "cat": 3, "x": 0.3341103341103341, "os": 0.7282481546648807, "ncat25k": 7, "term": "forward", "bg": 6.495713726374726e-07, "y": 0.7187257187257188, "cat25k": 22}, {"ncat": 13, "s": 0.8360528360528361, "cat": 2, "x": 0.3348873348873349, "os": 0.6393690247857154, "ncat25k": 7, "term": "although", "bg": 4.015206238496199e-07, "y": 0.5345765345765345, "cat25k": 15}, {"ncat": 13, "s": 0.5501165501165501, "cat": 1, "x": 0.3356643356643357, "os": 0.2480583452418879, "ncat25k": 7, "term": "whole", "bg": 3.51616826173147e-07, "y": 0.2975912975912976, "cat25k": 7}, {"ncat": 13, "s": 0.5501165501165501, "cat": 1, "x": 0.33644133644133645, "os": 0.2480583452418879, "ncat25k": 7, "term": "faced", "bg": 3.3811413460283917e-06, "y": 0.3061383061383061, "cat25k": 7}, {"ncat": 13, "s": 0.13830613830613833, "cat": 0, "x": 0.3372183372183372, "os": 0.08213270841683162, "ncat25k": 7, "term": "logistic", "bg": 3.5914479336742447e-05, "y": 0.04351204351204351, "cat25k": 0}, {"ncat": 13, "s": 0.8360528360528361, "cat": 2, "x": 0.337995337995338, "os": 0.6393690247857154, "ncat25k": 7, "term": "entire", "bg": 5.318994647602066e-07, "y": 0.5578865578865578, "cat25k": 15}, {"ncat": 13, "s": 0.5501165501165501, "cat": 1, "x": 0.33877233877233875, "os": 0.2480583452418879, "ncat25k": 7, "term": "solving", "bg": 3.381973405354778e-06, "y": 0.31157731157731156, "cat25k": 7}, {"ncat": 13, "s": 0.8360528360528361, "cat": 2, "x": 0.3395493395493395, "os": 0.6393690247857154, "ncat25k": 7, "term": "helpful", "bg": 7.770953896894855e-07, "y": 0.5594405594405595, "cat25k": 15}, {"ncat": 13, "s": 0.13830613830613833, "cat": 0, "x": 0.34032634032634035, "os": 0.08213270841683162, "ncat25k": 7, "term": "offers", "bg": 2.1096226301413545e-07, "y": 0.053613053613053616, "cat25k": 0}, {"ncat": 13, "s": 0.9782439782439782, "cat": 4, "x": 0.3411033411033411, "os": 0.790730940617375, "ncat25k": 7, "term": "particularly", "bg": 8.515102184182843e-07, "y": 0.8306138306138307, "cat25k": 30}, {"ncat": 13, "s": 0.13830613830613833, "cat": 0, "x": 0.3418803418803419, "os": 0.08213270841683162, "ncat25k": 7, "term": "evaluating", "bg": 4.347320752613284e-06, "y": 0.06293706293706294, "cat25k": 0}, {"ncat": 13, "s": 0.5501165501165501, "cat": 1, "x": 0.34265734265734266, "os": 0.2480583452418879, "ncat25k": 7, "term": "documents", "bg": 4.957017776314975e-07, "y": 0.3209013209013209, "cat25k": 7}, {"ncat": 13, "s": 0.9782439782439782, "cat": 4, "x": 0.3434343434343434, "os": 0.790730940617375, "ncat25k": 7, "term": "technique", "bg": 2.4276567523451823e-06, "y": 0.8337218337218337, "cat25k": 30}, {"ncat": 13, "s": 0.13830613830613833, "cat": 0, "x": 0.3442113442113442, "os": 0.08213270841683162, "ncat25k": 7, "term": "guide", "bg": 1.4988541236804866e-07, "y": 0.06526806526806526, "cat25k": 0}, {"ncat": 13, "s": 0.8360528360528361, "cat": 2, "x": 0.34498834498834496, "os": 0.6393690247857154, "ncat25k": 7, "term": "faster", "bg": 1.137122507703847e-06, "y": 0.5695415695415695, "cat25k": 15}, {"ncat": 13, "s": 0.8360528360528361, "cat": 2, "x": 0.3457653457653458, "os": 0.6393690247857154, "ncat25k": 7, "term": "specifically", "bg": 1.2198988528490525e-06, "y": 0.5765345765345765, "cat25k": 15}, {"ncat": 13, "s": 0.13830613830613833, "cat": 0, "x": 0.34654234654234656, "os": 0.08213270841683162, "ncat25k": 7, "term": "ready", "bg": 4.459654414371402e-07, "y": 0.07381507381507381, "cat25k": 0}, {"ncat": 13, "s": 0.8360528360528361, "cat": 2, "x": 0.3473193473193473, "os": 0.6393690247857154, "ncat25k": 7, "term": "aws", "bg": 6.172310330047151e-05, "y": 0.581973581973582, "cat25k": 15}, {"ncat": 13, "s": 0.5501165501165501, "cat": 1, "x": 0.3480963480963481, "os": 0.2480583452418879, "ncat25k": 7, "term": "history", "bg": 1.487554733588593e-07, "y": 0.34654234654234656, "cat25k": 7}, {"ncat": 13, "s": 0.13830613830613833, "cat": 0, "x": 0.34887334887334887, "os": 0.08213270841683162, "ncat25k": 7, "term": "visualizing", "bg": 5.5017605633802825e-05, "y": 0.08313908313908314, "cat25k": 0}, {"ncat": 13, "s": 0.9782439782439782, "cat": 4, "x": 0.34965034965034963, "os": 0.790730940617375, "ncat25k": 7, "term": "therefore", "bg": 6.430857724813019e-07, "y": 0.8414918414918415, "cat25k": 30}, {"ncat": 13, "s": 0.13830613830613833, "cat": 0, "x": 0.3504273504273504, "os": 0.08213270841683162, "ncat25k": 7, "term": "streaming", "bg": 6.715064092139073e-06, "y": 0.09090909090909091, "cat25k": 0}, {"ncat": 13, "s": 0.5501165501165501, "cat": 1, "x": 0.35120435120435123, "os": 0.2480583452418879, "ncat25k": 7, "term": "ease", "bg": 2.4126611347185275e-06, "y": 0.3706293706293706, "cat25k": 7}, {"ncat": 13, "s": 0.5501165501165501, "cat": 1, "x": 0.351981351981352, "os": 0.2480583452418879, "ncat25k": 7, "term": "aims", "bg": 2.6827668447576726e-06, "y": 0.37218337218337216, "cat25k": 7}, {"ncat": 13, "s": 0.5501165501165501, "cat": 1, "x": 0.35275835275835277, "os": 0.2480583452418879, "ncat25k": 7, "term": "once", "bg": 3.0357239349796507e-07, "y": 0.3745143745143745, "cat25k": 7}, {"ncat": 13, "s": 0.5501165501165501, "cat": 1, "x": 0.35353535353535354, "os": 0.2480583452418879, "ncat25k": 7, "term": "low", "bg": 2.487385911786859e-07, "y": 0.37995337995337997, "cat25k": 7}, {"ncat": 13, "s": 0.5501165501165501, "cat": 1, "x": 0.3543123543123543, "os": 0.2480583452418879, "ncat25k": 7, "term": "evaluated", "bg": 2.6907462832865735e-06, "y": 0.38073038073038074, "cat25k": 7}, {"ncat": 13, "s": 0.13830613830613833, "cat": 0, "x": 0.3550893550893551, "os": 0.08213270841683162, "ncat25k": 7, "term": "ca", "bg": 2.453166953826468e-07, "y": 0.10567210567210568, "cat25k": 0}, {"ncat": 13, "s": 0.5501165501165501, "cat": 1, "x": 0.35586635586635584, "os": 0.2480583452418879, "ncat25k": 7, "term": "distributions", "bg": 6.858148927995883e-06, "y": 0.3939393939393939, "cat25k": 7}, {"ncat": 13, "s": 0.13830613830613833, "cat": 0, "x": 0.35664335664335667, "os": 0.08213270841683162, "ncat25k": 7, "term": "simply", "bg": 4.184732909743938e-07, "y": 0.10878010878010878, "cat25k": 0}, {"ncat": 13, "s": 0.8360528360528361, "cat": 2, "x": 0.35742035742035744, "os": 0.6393690247857154, "ncat25k": 7, "term": "extraction", "bg": 8.659580449512636e-06, "y": 0.6068376068376068, "cat25k": 15}, {"ncat": 13, "s": 0.13830613830613833, "cat": 0, "x": 0.3581973581973582, "os": 0.08213270841683162, "ncat25k": 7, "term": "exploring", "bg": 3.842472092845654e-06, "y": 0.11188811188811189, "cat25k": 0}, {"ncat": 13, "s": 0.8360528360528361, "cat": 2, "x": 0.358974358974359, "os": 0.6393690247857154, "ncat25k": 7, "term": "keras", "bg": 0.0006800176804596921, "y": 0.60994560994561, "cat25k": 15}, {"ncat": 13, "s": 0.13830613830613833, "cat": 0, "x": 0.35975135975135974, "os": 0.08213270841683162, "ncat25k": 7, "term": "hope", "bg": 3.5010528406499167e-07, "y": 0.12121212121212122, "cat25k": 0}, {"ncat": 13, "s": 0.8360528360528361, "cat": 2, "x": 0.3605283605283605, "os": 0.6393690247857154, "ncat25k": 7, "term": "setting", "bg": 8.178211316716126e-07, "y": 0.6130536130536131, "cat25k": 15}, {"ncat": 13, "s": 0.13830613830613833, "cat": 0, "x": 0.3613053613053613, "os": 0.08213270841683162, "ncat25k": 7, "term": "preparation", "bg": 1.0241626761087112e-06, "y": 0.128982128982129, "cat25k": 0}, {"ncat": 13, "s": 0.13830613830613833, "cat": 0, "x": 0.3620823620823621, "os": 0.08213270841683162, "ncat25k": 7, "term": "recommendations", "bg": 1.8973185698883152e-06, "y": 0.13442113442113443, "cat25k": 0}, {"ncat": 13, "s": 0.5501165501165501, "cat": 1, "x": 0.3628593628593629, "os": 0.2480583452418879, "ncat25k": 7, "term": "evaluate", "bg": 2.2395055370841277e-06, "y": 0.41025641025641024, "cat25k": 7}, {"ncat": 13, "s": 0.5501165501165501, "cat": 1, "x": 0.36363636363636365, "os": 0.2480583452418879, "ncat25k": 7, "term": "performing", "bg": 1.4268692415124585e-06, "y": 0.41181041181041184, "cat25k": 7}, {"ncat": 13, "s": 0.5501165501165501, "cat": 1, "x": 0.3644133644133644, "os": 0.2480583452418879, "ncat25k": 7, "term": "target", "bg": 6.498783217526928e-07, "y": 0.41802641802641805, "cat25k": 7}, {"ncat": 13, "s": 0.9261849261849262, "cat": 3, "x": 0.3651903651903652, "os": 0.7282481546648807, "ncat25k": 7, "term": "4", "bg": 0.0, "y": 0.7801087801087802, "cat25k": 22}, {"ncat": 13, "s": 0.13830613830613833, "cat": 0, "x": 0.36596736596736595, "os": 0.08213270841683162, "ncat25k": 7, "term": "tests", "bg": 1.0364221595254863e-06, "y": 0.15151515151515152, "cat25k": 0}, {"ncat": 13, "s": 0.5501165501165501, "cat": 1, "x": 0.3667443667443667, "os": 0.2480583452418879, "ncat25k": 7, "term": "limitations", "bg": 2.160156302738191e-06, "y": 0.4281274281274281, "cat25k": 7}, {"ncat": 13, "s": 0.5501165501165501, "cat": 1, "x": 0.36752136752136755, "os": 0.2480583452418879, "ncat25k": 7, "term": "enable", "bg": 7.528112607315128e-07, "y": 0.4358974358974359, "cat25k": 7}, {"ncat": 13, "s": 0.13830613830613833, "cat": 0, "x": 0.3682983682983683, "os": 0.08213270841683162, "ncat25k": 7, "term": "novel", "bg": 1.458602424589229e-06, "y": 0.15695415695415696, "cat25k": 0}, {"ncat": 13, "s": 0.13830613830613833, "cat": 0, "x": 0.3690753690753691, "os": 0.08213270841683162, "ncat25k": 7, "term": "person", "bg": 1.770308407810832e-07, "y": 0.15773115773115773, "cat25k": 0}, {"ncat": 13, "s": 0.13830613830613833, "cat": 0, "x": 0.36985236985236986, "os": 0.08213270841683162, "ncat25k": 7, "term": "presents", "bg": 1.1706366858807167e-06, "y": 0.1623931623931624, "cat25k": 0}, {"ncat": 13, "s": 0.13830613830613833, "cat": 0, "x": 0.3706293706293706, "os": 0.08213270841683162, "ncat25k": 7, "term": "cost", "bg": 2.767210631935188e-07, "y": 0.16317016317016317, "cat25k": 0}, {"ncat": 13, "s": 0.13830613830613833, "cat": 0, "x": 0.3714063714063714, "os": 0.08213270841683162, "ncat25k": 7, "term": "foundation", "bg": 4.624553112090705e-07, "y": 0.16472416472416473, "cat25k": 0}, {"ncat": 13, "s": 0.13830613830613833, "cat": 0, "x": 0.37218337218337216, "os": 0.08213270841683162, "ncat25k": 7, "term": "groups", "bg": 2.8586881595971294e-07, "y": 0.16783216783216784, "cat25k": 0}, {"ncat": 13, "s": 0.8360528360528361, "cat": 2, "x": 0.372960372960373, "os": 0.6393690247857154, "ncat25k": 7, "term": "optimization", "bg": 7.043319068964659e-06, "y": 0.6394716394716394, "cat25k": 15}, {"ncat": 13, "s": 0.9782439782439782, "cat": 4, "x": 0.37373737373737376, "os": 0.790730940617375, "ncat25k": 7, "term": "integration", "bg": 1.2891232405808138e-06, "y": 0.8515928515928516, "cat25k": 30}, {"ncat": 13, "s": 0.9914529914529915, "cat": 5, "x": 0.3745143745143745, "os": 0.8370542079277241, "ncat25k": 7, "term": "critical", "bg": 1.0377550418139613e-06, "y": 0.891996891996892, "cat25k": 37}, {"ncat": 13, "s": 0.8360528360528361, "cat": 2, "x": 0.3752913752913753, "os": 0.6393690247857154, "ncat25k": 7, "term": "2015", "bg": 0.0, "y": 0.6542346542346542, "cat25k": 15}, {"ncat": 13, "s": 0.5501165501165501, "cat": 1, "x": 0.37606837606837606, "os": 0.2480583452418879, "ncat25k": 7, "term": "journey", "bg": 1.4467518586498635e-06, "y": 0.4677544677544678, "cat25k": 7}, {"ncat": 13, "s": 0.8360528360528361, "cat": 2, "x": 0.37684537684537683, "os": 0.6393690247857154, "ncat25k": 7, "term": "importance", "bg": 2.0323961691138787e-06, "y": 0.6596736596736597, "cat25k": 15}, {"ncat": 13, "s": 0.13830613830613833, "cat": 0, "x": 0.3776223776223776, "os": 0.08213270841683162, "ncat25k": 7, "term": "daily", "bg": 2.8313806370712136e-07, "y": 0.20435120435120435, "cat25k": 0}, {"ncat": 13, "s": 0.9782439782439782, "cat": 4, "x": 0.37839937839937837, "os": 0.790730940617375, "ncat25k": 7, "term": "vision", "bg": 1.286079053166554e-06, "y": 0.853923853923854, "cat25k": 30}, {"ncat": 13, "s": 0.13830613830613833, "cat": 0, "x": 0.3791763791763792, "os": 0.08213270841683162, "ncat25k": 7, "term": "behavior", "bg": 1.24046989619903e-06, "y": 0.20512820512820512, "cat25k": 0}, {"ncat": 13, "s": 0.13830613830613833, "cat": 0, "x": 0.37995337995337997, "os": 0.08213270841683162, "ncat25k": 7, "term": "combining", "bg": 4.384470730807966e-06, "y": 0.21212121212121213, "cat25k": 0}, {"ncat": 13, "s": 0.9914529914529915, "cat": 5, "x": 0.38073038073038074, "os": 0.8370542079277241, "ncat25k": 7, "term": "scaling", "bg": 1.077547778340212e-05, "y": 0.8935508935508936, "cat25k": 37}, {"ncat": 13, "s": 0.8360528360528361, "cat": 2, "x": 0.3815073815073815, "os": 0.6393690247857154, "ncat25k": 7, "term": "challenging", "bg": 3.9723988897586476e-06, "y": 0.6713286713286714, "cat25k": 15}, {"ncat": 13, "s": 0.5501165501165501, "cat": 1, "x": 0.3822843822843823, "os": 0.2480583452418879, "ncat25k": 7, "term": "unique", "bg": 5.376810008204379e-07, "y": 0.4864024864024864, "cat25k": 7}, {"ncat": 13, "s": 0.13830613830613833, "cat": 0, "x": 0.38306138306138304, "os": 0.08213270841683162, "ncat25k": 7, "term": "similarity", "bg": 1.1919673321486503e-05, "y": 0.22377622377622378, "cat25k": 0}, {"ncat": 13, "s": 0.5501165501165501, "cat": 1, "x": 0.3838383838383838, "os": 0.2480583452418879, "ncat25k": 7, "term": "offer", "bg": 2.5327158023452125e-07, "y": 0.4933954933954934, "cat25k": 7}, {"ncat": 13, "s": 0.13830613830613833, "cat": 0, "x": 0.38461538461538464, "os": 0.08213270841683162, "ncat25k": 7, "term": "hardware", "bg": 4.0418418121545457e-07, "y": 0.22688422688422688, "cat25k": 0}, {"ncat": 13, "s": 0.8360528360528361, "cat": 2, "x": 0.3853923853923854, "os": 0.6393690247857154, "ncat25k": 7, "term": "infrastructure", "bg": 1.3694925940921868e-06, "y": 0.6798756798756799, "cat25k": 15}, {"ncat": 13, "s": 0.13830613830613833, "cat": 0, "x": 0.3861693861693862, "os": 0.08213270841683162, "ncat25k": 7, "term": "metrics", "bg": 9.997960416075118e-06, "y": 0.22843822843822845, "cat25k": 0}, {"ncat": 13, "s": 0.8360528360528361, "cat": 2, "x": 0.38694638694638694, "os": 0.6393690247857154, "ncat25k": 7, "term": "area", "bg": 1.4615992865549252e-07, "y": 0.6876456876456877, "cat25k": 15}, {"ncat": 13, "s": 0.13830613830613833, "cat": 0, "x": 0.3877233877233877, "os": 0.08213270841683162, "ncat25k": 7, "term": "million", "bg": 2.3542028313560242e-07, "y": 0.2400932400932401, "cat25k": 0}, {"ncat": 14, "s": 0.9704739704739705, "cat": 4, "x": 0.3885003885003885, "os": 0.7747749367722871, "ncat25k": 8, "term": "memory", "bg": 3.5562519613618633e-07, "y": 0.7995337995337995, "cat25k": 30}, {"ncat": 14, "s": 0.1048951048951049, "cat": 0, "x": 0.38927738927738925, "os": 0.06775758005545379, "ncat25k": 8, "term": "visualisation", "bg": 6.333247570605815e-05, "y": 0.006216006216006216, "cat25k": 0}, {"ncat": 14, "s": 0.5361305361305362, "cat": 1, "x": 0.3900543900543901, "os": 0.22444038327419036, "ncat25k": 8, "term": "deploy", "bg": 1.1951837084508703e-05, "y": 0.27505827505827507, "cat25k": 7}, {"ncat": 14, "s": 0.1048951048951049, "cat": 0, "x": 0.39083139083139085, "os": 0.06775758005545379, "ncat25k": 8, "term": "please", "bg": 7.365243027793961e-08, "y": 0.021756021756021756, "cat25k": 0}, {"ncat": 14, "s": 0.1048951048951049, "cat": 0, "x": 0.3916083916083916, "os": 0.06775758005545379, "ncat25k": 8, "term": "am", "bg": 5.897107708715848e-08, "y": 0.024864024864024864, "cat25k": 0}, {"ncat": 14, "s": 0.5361305361305362, "cat": 1, "x": 0.3923853923853924, "os": 0.22444038327419036, "ncat25k": 8, "term": "2016", "bg": 0.0, "y": 0.28593628593628595, "cat25k": 7}, {"ncat": 14, "s": 0.1048951048951049, "cat": 0, "x": 0.39316239316239315, "os": 0.06775758005545379, "ncat25k": 8, "term": "install", "bg": 1.329397528856789e-06, "y": 0.028749028749028748, "cat25k": 0}, {"ncat": 14, "s": 0.1048951048951049, "cat": 0, "x": 0.3939393939393939, "os": 0.06775758005545379, "ncat25k": 8, "term": "additional", "bg": 2.5698134609444245e-07, "y": 0.03263403263403263, "cat25k": 0}, {"ncat": 14, "s": 0.1048951048951049, "cat": 0, "x": 0.3947163947163947, "os": 0.06775758005545379, "ncat25k": 8, "term": "store", "bg": 1.441186541232424e-07, "y": 0.03651903651903652, "cat25k": 0}, {"ncat": 14, "s": 0.5361305361305362, "cat": 1, "x": 0.3954933954933955, "os": 0.22444038327419036, "ncat25k": 8, "term": "efficiently", "bg": 4.649441416108269e-06, "y": 0.29681429681429683, "cat25k": 7}, {"ncat": 14, "s": 0.9704739704739705, "cat": 4, "x": 0.3962703962703963, "os": 0.7747749367722871, "ncat25k": 8, "term": "scalable", "bg": 1.8115398838615534e-05, "y": 0.8251748251748252, "cat25k": 30}, {"ncat": 14, "s": 0.5361305361305362, "cat": 1, "x": 0.39704739704739705, "os": 0.22444038327419036, "ncat25k": 8, "term": "focused", "bg": 1.8955473592531145e-06, "y": 0.30691530691530694, "cat25k": 7}, {"ncat": 14, "s": 0.1048951048951049, "cat": 0, "x": 0.3978243978243978, "os": 0.06775758005545379, "ncat25k": 8, "term": "principles", "bg": 1.1550322460939938e-06, "y": 0.042735042735042736, "cat25k": 0}, {"ncat": 14, "s": 0.1048951048951049, "cat": 0, "x": 0.3986013986013986, "os": 0.06775758005545379, "ncat25k": 8, "term": "installed", "bg": 8.617955683024696e-07, "y": 0.045066045066045064, "cat25k": 0}, {"ncat": 14, "s": 0.811965811965812, "cat": 2, "x": 0.39937839937839936, "os": 0.6252889000290294, "ncat25k": 8, "term": "latest", "bg": 2.422961320155927e-07, "y": 0.5555555555555556, "cat25k": 15}, {"ncat": 14, "s": 0.1048951048951049, "cat": 0, "x": 0.40015540015540013, "os": 0.06775758005545379, "ncat25k": 8, "term": "anaconda", "bg": 9.161352463227693e-05, "y": 0.0473970473970474, "cat25k": 0}, {"ncat": 14, "s": 0.1048951048951049, "cat": 0, "x": 0.40093240093240096, "os": 0.06775758005545379, "ncat25k": 8, "term": "analysing", "bg": 2.072920464480508e-05, "y": 0.05982905982905983, "cat25k": 0}, {"ncat": 14, "s": 0.811965811965812, "cat": 2, "x": 0.4017094017094017, "os": 0.6252889000290294, "ncat25k": 8, "term": "read", "bg": 1.1164542821213232e-07, "y": 0.578088578088578, "cat25k": 15}, {"ncat": 14, "s": 0.5361305361305362, "cat": 1, "x": 0.4024864024864025, "os": 0.22444038327419036, "ncat25k": 8, "term": "medical", "bg": 3.21808044256278e-07, "y": 0.34887334887334887, "cat25k": 7}, {"ncat": 14, "s": 0.5361305361305362, "cat": 1, "x": 0.40326340326340326, "os": 0.22444038327419036, "ncat25k": 8, "term": "throughout", "bg": 6.630493166135958e-07, "y": 0.35275835275835277, "cat25k": 7}, {"ncat": 14, "s": 0.5361305361305362, "cat": 1, "x": 0.40404040404040403, "os": 0.22444038327419036, "ncat25k": 8, "term": "briefly", "bg": 4.930907114327287e-06, "y": 0.35664335664335667, "cat25k": 7}, {"ncat": 14, "s": 0.1048951048951049, "cat": 0, "x": 0.4048174048174048, "os": 0.06775758005545379, "ncat25k": 8, "term": "handling", "bg": 9.45139801298849e-07, "y": 0.09168609168609168, "cat25k": 0}, {"ncat": 14, "s": 0.5361305361305362, "cat": 1, "x": 0.40559440559440557, "os": 0.22444038327419036, "ncat25k": 8, "term": "handle", "bg": 1.171527395489891e-06, "y": 0.372960372960373, "cat25k": 7}, {"ncat": 14, "s": 0.1048951048951049, "cat": 0, "x": 0.4063714063714064, "os": 0.06775758005545379, "ncat25k": 8, "term": "methodology", "bg": 3.7473143132149844e-06, "y": 0.09712509712509712, "cat25k": 0}, {"ncat": 14, "s": 0.5361305361305362, "cat": 1, "x": 0.40714840714840717, "os": 0.22444038327419036, "ncat25k": 8, "term": "detailed", "bg": 6.093557898206962e-07, "y": 0.37684537684537683, "cat25k": 7}, {"ncat": 14, "s": 0.9137529137529138, "cat": 3, "x": 0.40792540792540793, "os": 0.7122711015420973, "ncat25k": 8, "term": "accuracy", "bg": 2.2931403606259976e-06, "y": 0.7645687645687645, "cat25k": 22}, {"ncat": 14, "s": 0.811965811965812, "cat": 2, "x": 0.4087024087024087, "os": 0.6252889000290294, "ncat25k": 8, "term": "analytical", "bg": 6.202528293667767e-06, "y": 0.5928515928515928, "cat25k": 15}, {"ncat": 14, "s": 0.5361305361305362, "cat": 1, "x": 0.4094794094794095, "os": 0.22444038327419036, "ncat25k": 8, "term": "strong", "bg": 5.238445270696413e-07, "y": 0.3877233877233877, "cat25k": 7}, {"ncat": 14, "s": 0.811965811965812, "cat": 2, "x": 0.41025641025641024, "os": 0.6252889000290294, "ncat25k": 8, "term": "flexibility", "bg": 3.1206432894076783e-06, "y": 0.602952602952603, "cat25k": 15}, {"ncat": 14, "s": 0.1048951048951049, "cat": 0, "x": 0.411033411033411, "os": 0.06775758005545379, "ncat25k": 8, "term": "scripts", "bg": 1.833386635230197e-06, "y": 0.11421911421911422, "cat25k": 0}, {"ncat": 14, "s": 0.5361305361305362, "cat": 1, "x": 0.41181041181041184, "os": 0.22444038327419036, "ncat25k": 8, "term": "far", "bg": 3.6488742015740063e-07, "y": 0.40093240093240096, "cat25k": 7}, {"ncat": 14, "s": 0.1048951048951049, "cat": 0, "x": 0.4125874125874126, "os": 0.06775758005545379, "ncat25k": 8, "term": "website", "bg": 1.2345991929894222e-07, "y": 0.11577311577311578, "cat25k": 0}, {"ncat": 14, "s": 0.5361305361305362, "cat": 1, "x": 0.4133644133644134, "os": 0.22444038327419036, "ncat25k": 8, "term": "map", "bg": 1.2911797365609212e-07, "y": 0.40559440559440557, "cat25k": 7}, {"ncat": 14, "s": 0.811965811965812, "cat": 2, "x": 0.41414141414141414, "os": 0.6252889000290294, "ncat25k": 8, "term": "limited", "bg": 3.5757102828544917e-07, "y": 0.6177156177156177, "cat25k": 15}, {"ncat": 14, "s": 0.811965811965812, "cat": 2, "x": 0.4149184149184149, "os": 0.6252889000290294, "ncat25k": 8, "term": "higher", "bg": 4.840668196156462e-07, "y": 0.6184926184926185, "cat25k": 15}, {"ncat": 14, "s": 0.5361305361305362, "cat": 1, "x": 0.4156954156954157, "os": 0.22444038327419036, "ncat25k": 8, "term": "needed", "bg": 4.4359957571005287e-07, "y": 0.439005439005439, "cat25k": 7}, {"ncat": 14, "s": 0.811965811965812, "cat": 2, "x": 0.41647241647241645, "os": 0.6252889000290294, "ncat25k": 8, "term": "google", "bg": 5.431810899863787e-07, "y": 0.6371406371406372, "cat25k": 15}, {"ncat": 14, "s": 0.5361305361305362, "cat": 1, "x": 0.4172494172494173, "os": 0.22444038327419036, "ncat25k": 8, "term": "presented", "bg": 8.538384049342298e-07, "y": 0.4444444444444444, "cat25k": 7}, {"ncat": 14, "s": 0.5361305361305362, "cat": 1, "x": 0.41802641802641805, "os": 0.22444038327419036, "ncat25k": 8, "term": "represent", "bg": 1.1469379700008435e-06, "y": 0.445998445998446, "cat25k": 7}, {"ncat": 14, "s": 0.1048951048951049, "cat": 0, "x": 0.4188034188034188, "os": 0.06775758005545379, "ncat25k": 8, "term": "analyses", "bg": 4.161015034044533e-06, "y": 0.1794871794871795, "cat25k": 0}, {"ncat": 14, "s": 0.1048951048951049, "cat": 0, "x": 0.4195804195804196, "os": 0.06775758005545379, "ncat25k": 8, "term": "media", "bg": 2.2165767449199223e-07, "y": 0.1857031857031857, "cat25k": 0}, {"ncat": 14, "s": 0.1048951048951049, "cat": 0, "x": 0.42035742035742035, "os": 0.06775758005545379, "ncat25k": 8, "term": "generation", "bg": 1.0991795998261796e-06, "y": 0.1958041958041958, "cat25k": 0}, {"ncat": 14, "s": 0.1048951048951049, "cat": 0, "x": 0.4211344211344211, "os": 0.06775758005545379, "ncat25k": 8, "term": "humans", "bg": 2.490920982226189e-06, "y": 0.2066822066822067, "cat25k": 0}, {"ncat": 14, "s": 0.1048951048951049, "cat": 0, "x": 0.4219114219114219, "os": 0.06775758005545379, "ncat25k": 8, "term": "difference", "bg": 7.164532251757633e-07, "y": 0.20823620823620823, "cat25k": 0}, {"ncat": 14, "s": 0.5361305361305362, "cat": 1, "x": 0.4226884226884227, "os": 0.22444038327419036, "ncat25k": 8, "term": "structured", "bg": 4.153652849232786e-06, "y": 0.48717948717948717, "cat25k": 7}, {"ncat": 14, "s": 0.5361305361305362, "cat": 1, "x": 0.4234654234654235, "os": 0.22444038327419036, "ncat25k": 8, "term": "early", "bg": 3.195819420782897e-07, "y": 0.4926184926184926, "cat25k": 7}, {"ncat": 14, "s": 0.811965811965812, "cat": 2, "x": 0.42424242424242425, "os": 0.6252889000290294, "ncat25k": 8, "term": "growing", "bg": 1.029681946434555e-06, "y": 0.6891996891996892, "cat25k": 15}, {"ncat": 14, "s": 0.1048951048951049, "cat": 0, "x": 0.425019425019425, "os": 0.06775758005545379, "ncat25k": 8, "term": "external", "bg": 6.308855709214747e-07, "y": 0.24864024864024864, "cat25k": 0}, {"ncat": 14, "s": 0.5361305361305362, "cat": 1, "x": 0.4257964257964258, "os": 0.22444038327419036, "ncat25k": 8, "term": "involves", "bg": 2.7701461960044637e-06, "y": 0.5058275058275058, "cat25k": 7}, {"ncat": 15, "s": 0.905982905982906, "cat": 3, "x": 0.42657342657342656, "os": 0.6975663805657142, "ncat25k": 9, "term": "term", "bg": 3.7800541149110675e-07, "y": 0.6961926961926962, "cat25k": 22}, {"ncat": 15, "s": 0.5135975135975137, "cat": 1, "x": 0.42735042735042733, "os": 0.20381046305545375, "ncat25k": 9, "term": "layer", "bg": 3.7735953762357834e-06, "y": 0.2665112665112665, "cat25k": 7}, {"ncat": 15, "s": 0.5135975135975137, "cat": 1, "x": 0.4281274281274281, "os": 0.20381046305545375, "ncat25k": 9, "term": "familiar", "bg": 2.180715136452798e-06, "y": 0.26728826728826727, "cat25k": 7}, {"ncat": 15, "s": 0.08313908313908314, "cat": 0, "x": 0.4289044289044289, "os": 0.05548291646822645, "ncat25k": 9, "term": "certain", "bg": 4.916297642765017e-07, "y": 0.017094017094017096, "cat25k": 0}, {"ncat": 15, "s": 0.5135975135975137, "cat": 1, "x": 0.4296814296814297, "os": 0.20381046305545375, "ncat25k": 9, "term": "either", "bg": 3.425325762584304e-07, "y": 0.2766122766122766, "cat25k": 7}, {"ncat": 15, "s": 0.5135975135975137, "cat": 1, "x": 0.43045843045843046, "os": 0.20381046305545375, "ncat25k": 9, "term": "query", "bg": 1.5285679315173543e-06, "y": 0.2836052836052836, "cat25k": 7}, {"ncat": 15, "s": 0.08313908313908314, "cat": 0, "x": 0.43123543123543123, "os": 0.05548291646822645, "ncat25k": 9, "term": "mobile", "bg": 3.15917454230566e-07, "y": 0.029526029526029528, "cat25k": 0}, {"ncat": 15, "s": 0.5135975135975137, "cat": 1, "x": 0.432012432012432, "os": 0.20381046305545375, "ncat25k": 9, "term": "document", "bg": 4.857247519262022e-07, "y": 0.3045843045843046, "cat25k": 7}, {"ncat": 15, "s": 0.5135975135975137, "cat": 1, "x": 0.43278943278943277, "os": 0.20381046305545375, "ncat25k": 9, "term": "purpose", "bg": 6.208039858595751e-07, "y": 0.31546231546231546, "cat25k": 7}, {"ncat": 15, "s": 0.5135975135975137, "cat": 1, "x": 0.43356643356643354, "os": 0.20381046305545375, "ncat25k": 9, "term": "showing", "bg": 7.52148182217675e-07, "y": 0.3247863247863248, "cat25k": 7}, {"ncat": 15, "s": 0.5135975135975137, "cat": 1, "x": 0.43434343434343436, "os": 0.20381046305545375, "ncat25k": 9, "term": "due", "bg": 3.6181540039258684e-07, "y": 0.32556332556332557, "cat25k": 7}, {"ncat": 15, "s": 0.7940947940947941, "cat": 2, "x": 0.43512043512043513, "os": 0.6125955325352423, "ncat25k": 9, "term": "major", "bg": 3.25972132837947e-07, "y": 0.5788655788655789, "cat25k": 15}, {"ncat": 15, "s": 0.08313908313908314, "cat": 0, "x": 0.4358974358974359, "os": 0.05548291646822645, "ncat25k": 9, "term": "self", "bg": 4.2470432880948907e-07, "y": 0.08080808080808081, "cat25k": 0}, {"ncat": 15, "s": 0.08313908313908314, "cat": 0, "x": 0.4366744366744367, "os": 0.05548291646822645, "ncat25k": 9, "term": "relationships", "bg": 1.6684490708090119e-06, "y": 0.08624708624708624, "cat25k": 0}, {"ncat": 15, "s": 0.5135975135975137, "cat": 1, "x": 0.43745143745143744, "os": 0.20381046305545375, "ncat25k": 9, "term": "though", "bg": 4.129379367103934e-07, "y": 0.3543123543123543, "cat25k": 7}, {"ncat": 15, "s": 0.905982905982906, "cat": 3, "x": 0.4382284382284382, "os": 0.6975663805657142, "ncat25k": 9, "term": "setup", "bg": 2.029385845270491e-06, "y": 0.7544677544677545, "cat25k": 22}, {"ncat": 15, "s": 0.5135975135975137, "cat": 1, "x": 0.439005439005439, "os": 0.20381046305545375, "ncat25k": 9, "term": "practice", "bg": 4.990765005254859e-07, "y": 0.3667443667443667, "cat25k": 7}, {"ncat": 15, "s": 0.5135975135975137, "cat": 1, "x": 0.4397824397824398, "os": 0.20381046305545375, "ncat25k": 9, "term": "tips", "bg": 4.846261519063202e-07, "y": 0.36985236985236986, "cat25k": 7}, {"ncat": 15, "s": 0.5135975135975137, "cat": 1, "x": 0.4405594405594406, "os": 0.20381046305545375, "ncat25k": 9, "term": "mining", "bg": 3.1554049193814495e-06, "y": 0.37373737373737376, "cat25k": 7}, {"ncat": 15, "s": 0.08313908313908314, "cat": 0, "x": 0.44133644133644134, "os": 0.05548291646822645, "ncat25k": 9, "term": "seaborn", "bg": 0.00021665212288989857, "y": 0.10334110334110334, "cat25k": 0}, {"ncat": 15, "s": 0.905982905982906, "cat": 3, "x": 0.4421134421134421, "os": 0.6975663805657142, "ncat25k": 9, "term": "aim", "bg": 1.5824490166747814e-06, "y": 0.7668997668997669, "cat25k": 22}, {"ncat": 15, "s": 0.5135975135975137, "cat": 1, "x": 0.4428904428904429, "os": 0.20381046305545375, "ncat25k": 9, "term": "actually", "bg": 5.462730820862422e-07, "y": 0.3885003885003885, "cat25k": 7}, {"ncat": 15, "s": 0.5135975135975137, "cat": 1, "x": 0.44366744366744365, "os": 0.20381046305545375, "ncat25k": 9, "term": "implementations", "bg": 1.1532021146581686e-05, "y": 0.3923853923853924, "cat25k": 7}, {"ncat": 15, "s": 0.08313908313908314, "cat": 0, "x": 0.4444444444444444, "os": 0.05548291646822645, "ncat25k": 9, "term": "strategies", "bg": 1.2439663575248012e-06, "y": 0.1282051282051282, "cat25k": 0}, {"ncat": 15, "s": 0.7940947940947941, "cat": 2, "x": 0.44522144522144524, "os": 0.6125955325352423, "ncat25k": 9, "term": "advantages", "bg": 3.1528763533327633e-06, "y": 0.6309246309246309, "cat25k": 15}, {"ncat": 15, "s": 0.7940947940947941, "cat": 2, "x": 0.445998445998446, "os": 0.6125955325352423, "ncat25k": 9, "term": "rich", "bg": 1.1790374220582575e-06, "y": 0.6332556332556333, "cat25k": 15}, {"ncat": 15, "s": 0.08313908313908314, "cat": 0, "x": 0.4467754467754468, "os": 0.05548291646822645, "ncat25k": 9, "term": "requirements", "bg": 3.4925076927361167e-07, "y": 0.1585081585081585, "cat25k": 0}, {"ncat": 15, "s": 0.08313908313908314, "cat": 0, "x": 0.44755244755244755, "os": 0.05548291646822645, "ncat25k": 9, "term": "edge", "bg": 8.097737018321298e-07, "y": 0.16705516705516704, "cat25k": 0}, {"ncat": 15, "s": 0.9937839937839937, "cat": 6, "x": 0.4483294483294483, "os": 0.8448009783667603, "ncat25k": 9, "term": "point", "bg": 3.415971510296595e-07, "y": 0.9261849261849262, "cat25k": 45}, {"ncat": 15, "s": 0.08313908313908314, "cat": 0, "x": 0.4491064491064491, "os": 0.05548291646822645, "ncat25k": 9, "term": "rapidly", "bg": 2.9809327123167257e-06, "y": 0.19813519813519814, "cat25k": 0}, {"ncat": 15, "s": 0.5135975135975137, "cat": 1, "x": 0.44988344988344986, "os": 0.20381046305545375, "ncat25k": 9, "term": "fun", "bg": 4.2393477885078227e-07, "y": 0.46853146853146854, "cat25k": 7}, {"ncat": 15, "s": 0.7940947940947941, "cat": 2, "x": 0.4506604506604507, "os": 0.6125955325352423, "ncat25k": 9, "term": "idea", "bg": 6.09971391426785e-07, "y": 0.6604506604506605, "cat25k": 15}, {"ncat": 15, "s": 0.08313908313908314, "cat": 0, "x": 0.45143745143745145, "os": 0.05548291646822645, "ncat25k": 9, "term": "thinking", "bg": 1.0614943712934094e-06, "y": 0.20590520590520592, "cat25k": 0}, {"ncat": 15, "s": 0.905982905982906, "cat": 3, "x": 0.4522144522144522, "os": 0.6975663805657142, "ncat25k": 9, "term": "representation", "bg": 3.7297903309022025e-06, "y": 0.7940947940947941, "cat25k": 22}, {"ncat": 15, "s": 0.5135975135975137, "cat": 1, "x": 0.452991452991453, "os": 0.20381046305545375, "ncat25k": 9, "term": "researchers", "bg": 2.9050945944528293e-06, "y": 0.48562548562548563, "cat25k": 7}, {"ncat": 15, "s": 0.08313908313908314, "cat": 0, "x": 0.45376845376845376, "os": 0.05548291646822645, "ncat25k": 9, "term": "experiences", "bg": 1.4847742586336528e-06, "y": 0.2222222222222222, "cat25k": 0}, {"ncat": 15, "s": 0.08313908313908314, "cat": 0, "x": 0.45454545454545453, "os": 0.05548291646822645, "ncat25k": 9, "term": "towards", "bg": 1.1918858147384702e-06, "y": 0.22999222999222999, "cat25k": 0}, {"ncat": 15, "s": 0.5135975135975137, "cat": 1, "x": 0.4553224553224553, "os": 0.20381046305545375, "ncat25k": 9, "term": "points", "bg": 6.451765980165024e-07, "y": 0.4965034965034965, "cat25k": 7}, {"ncat": 15, "s": 0.7940947940947941, "cat": 2, "x": 0.4560994560994561, "os": 0.6125955325352423, "ncat25k": 9, "term": "risk", "bg": 1.113491046604076e-06, "y": 0.6884226884226884, "cat25k": 15}, {"ncat": 15, "s": 0.08313908313908314, "cat": 0, "x": 0.4568764568764569, "os": 0.05548291646822645, "ncat25k": 9, "term": "discussed", "bg": 1.2666312141867447e-06, "y": 0.24087024087024086, "cat25k": 0}, {"ncat": 16, "s": 0.7762237762237763, "cat": 2, "x": 0.45765345765345766, "os": 0.6011033184767522, "ncat25k": 9, "term": "toolkit", "bg": 1.0466022059384209e-05, "y": 0.508935508935509, "cat25k": 15}, {"ncat": 16, "s": 0.4825174825174825, "cat": 1, "x": 0.45843045843045843, "os": 0.18580311830553375, "ncat25k": 9, "term": "load", "bg": 1.3153099739718572e-06, "y": 0.2595182595182595, "cat25k": 7}, {"ncat": 16, "s": 0.4825174825174825, "cat": 1, "x": 0.4592074592074592, "os": 0.18580311830553375, "ncat25k": 9, "term": "workshop", "bg": 2.3665580868652074e-06, "y": 0.2618492618492618, "cat25k": 7}, {"ncat": 16, "s": 0.4825174825174825, "cat": 1, "x": 0.45998445998445997, "os": 0.18580311830553375, "ncat25k": 9, "term": "sql", "bg": 2.216716357421358e-06, "y": 0.26495726495726496, "cat25k": 7}, {"ncat": 16, "s": 0.06604506604506605, "cat": 0, "x": 0.46076146076146074, "os": 0.045082088186235825, "ncat25k": 9, "term": "individual", "bg": 4.002835482250296e-07, "y": 0.03108003108003108, "cat25k": 0}, {"ncat": 16, "s": 0.06604506604506605, "cat": 0, "x": 0.46153846153846156, "os": 0.045082088186235825, "ncat25k": 9, "term": "play", "bg": 2.6513602402694463e-07, "y": 0.03418803418803419, "cat25k": 0}, {"ncat": 16, "s": 0.4825174825174825, "cat": 1, "x": 0.46231546231546233, "os": 0.18580311830553375, "ncat25k": 9, "term": "another", "bg": 1.9724544350438132e-07, "y": 0.29292929292929293, "cat25k": 7}, {"ncat": 16, "s": 0.8912198912198913, "cat": 3, "x": 0.4630924630924631, "os": 0.6840093356884034, "ncat25k": 9, "term": "especially", "bg": 7.400678770073122e-07, "y": 0.7257187257187258, "cat25k": 22}, {"ncat": 16, "s": 0.4825174825174825, "cat": 1, "x": 0.46386946386946387, "os": 0.18580311830553375, "ncat25k": 9, "term": "accessible", "bg": 2.1261862857051123e-06, "y": 0.2999222999222999, "cat25k": 7}, {"ncat": 16, "s": 0.7762237762237763, "cat": 2, "x": 0.46464646464646464, "os": 0.6011033184767522, "ncat25k": 9, "term": "creation", "bg": 1.2878012887366391e-06, "y": 0.5586635586635587, "cat25k": 15}, {"ncat": 16, "s": 0.06604506604506605, "cat": 0, "x": 0.4654234654234654, "os": 0.045082088186235825, "ncat25k": 9, "term": "internal", "bg": 6.459842031022974e-07, "y": 0.050505050505050504, "cat25k": 0}, {"ncat": 16, "s": 0.06604506604506605, "cat": 0, "x": 0.4662004662004662, "os": 0.045082088186235825, "ncat25k": 9, "term": "effectively", "bg": 1.9362629127080706e-06, "y": 0.057498057498057496, "cat25k": 0}, {"ncat": 16, "s": 0.4825174825174825, "cat": 1, "x": 0.466977466977467, "os": 0.18580311830553375, "ncat25k": 9, "term": "account", "bg": 2.2579016938660703e-07, "y": 0.317016317016317, "cat25k": 7}, {"ncat": 16, "s": 0.06604506604506605, "cat": 0, "x": 0.4677544677544678, "os": 0.045082088186235825, "ncat25k": 9, "term": "analyse", "bg": 1.289540914642063e-05, "y": 0.061383061383061384, "cat25k": 0}, {"ncat": 16, "s": 0.06604506604506605, "cat": 0, "x": 0.46853146853146854, "os": 0.045082088186235825, "ncat25k": 9, "term": "cleaning", "bg": 1.5006775729773535e-06, "y": 0.06604506604506605, "cat25k": 0}, {"ncat": 16, "s": 0.06604506604506605, "cat": 0, "x": 0.4693084693084693, "os": 0.045082088186235825, "ncat25k": 9, "term": "taking", "bg": 5.134418432767551e-07, "y": 0.0675990675990676, "cat25k": 0}, {"ncat": 16, "s": 0.06604506604506605, "cat": 0, "x": 0.4700854700854701, "os": 0.045082088186235825, "ncat25k": 9, "term": "try", "bg": 3.318724221973359e-07, "y": 0.06993006993006994, "cat25k": 0}, {"ncat": 16, "s": 0.4825174825174825, "cat": 1, "x": 0.47086247086247085, "os": 0.18580311830553375, "ncat25k": 9, "term": "format", "bg": 5.924741615958733e-07, "y": 0.3278943278943279, "cat25k": 7}, {"ncat": 16, "s": 0.4825174825174825, "cat": 1, "x": 0.4716394716394716, "os": 0.18580311830553375, "ncat25k": 9, "term": "pitfalls", "bg": 2.966182819329805e-05, "y": 0.3418803418803419, "cat25k": 7}, {"ncat": 16, "s": 0.8912198912198913, "cat": 3, "x": 0.47241647241647244, "os": 0.6840093356884034, "ncat25k": 9, "term": "theory", "bg": 8.731149819605515e-07, "y": 0.752913752913753, "cat25k": 22}, {"ncat": 16, "s": 0.06604506604506605, "cat": 0, "x": 0.4731934731934732, "os": 0.045082088186235825, "ncat25k": 9, "term": "bokeh", "bg": 0.0005823890612141546, "y": 0.09324009324009325, "cat25k": 0}, {"ncat": 16, "s": 0.4825174825174825, "cat": 1, "x": 0.473970473970474, "os": 0.18580311830553375, "ncat25k": 9, "term": "sophisticated", "bg": 4.339497082664599e-06, "y": 0.36752136752136755, "cat25k": 7}, {"ncat": 16, "s": 0.06604506604506605, "cat": 0, "x": 0.47474747474747475, "os": 0.045082088186235825, "ncat25k": 9, "term": "news", "bg": 9.529574878795422e-08, "y": 0.09401709401709402, "cat25k": 0}, {"ncat": 16, "s": 0.06604506604506605, "cat": 0, "x": 0.4755244755244755, "os": 0.045082088186235825, "ncat25k": 9, "term": "extracting", "bg": 2.0718271391744703e-05, "y": 0.0979020979020979, "cat25k": 0}, {"ncat": 16, "s": 0.8912198912198913, "cat": 3, "x": 0.4763014763014763, "os": 0.6840093356884034, "ncat25k": 9, "term": "traditional", "bg": 1.1258156064979242e-06, "y": 0.7614607614607615, "cat25k": 22}, {"ncat": 16, "s": 0.4825174825174825, "cat": 1, "x": 0.47707847707847706, "os": 0.18580311830553375, "ncat25k": 9, "term": "flexible", "bg": 2.5192649449976444e-06, "y": 0.3962703962703963, "cat25k": 7}, {"ncat": 16, "s": 0.06604506604506605, "cat": 0, "x": 0.47785547785547783, "os": 0.045082088186235825, "ncat25k": 9, "term": "lead", "bg": 4.96763294919894e-07, "y": 0.1320901320901321, "cat25k": 0}, {"ncat": 16, "s": 0.8912198912198913, "cat": 3, "x": 0.47863247863247865, "os": 0.6840093356884034, "ncat25k": 9, "term": "pyspark", "bg": 0.0008159657294393636, "y": 0.7762237762237763, "cat25k": 22}, {"ncat": 16, "s": 0.7762237762237763, "cat": 2, "x": 0.4794094794094794, "os": 0.6011033184767522, "ncat25k": 9, "term": "objects", "bg": 1.884414657083201e-06, "y": 0.6216006216006216, "cat25k": 15}, {"ncat": 16, "s": 0.7762237762237763, "cat": 2, "x": 0.4801864801864802, "os": 0.6011033184767522, "ncat25k": 9, "term": "effective", "bg": 6.046390351622442e-07, "y": 0.6301476301476302, "cat25k": 15}, {"ncat": 16, "s": 0.7762237762237763, "cat": 2, "x": 0.48096348096348096, "os": 0.6011033184767522, "ncat25k": 9, "term": "itself", "bg": 7.032027643369468e-07, "y": 0.6348096348096348, "cat25k": 15}, {"ncat": 16, "s": 0.4825174825174825, "cat": 1, "x": 0.48174048174048173, "os": 0.18580311830553375, "ncat25k": 9, "term": "per", "bg": 2.0085520402663922e-07, "y": 0.43745143745143744, "cat25k": 7}, {"ncat": 16, "s": 0.06604506604506605, "cat": 0, "x": 0.4825174825174825, "os": 0.045082088186235825, "ncat25k": 9, "term": "necessary", "bg": 4.950357063067014e-07, "y": 0.1825951825951826, "cat25k": 0}, {"ncat": 16, "s": 0.7762237762237763, "cat": 2, "x": 0.48329448329448327, "os": 0.6011033184767522, "ncat25k": 9, "term": "parts", "bg": 3.970021218473156e-07, "y": 0.6495726495726496, "cat25k": 15}, {"ncat": 16, "s": 0.06604506604506605, "cat": 0, "x": 0.4840714840714841, "os": 0.045082088186235825, "ncat25k": 9, "term": "detect", "bg": 5.380627332019447e-06, "y": 0.19502719502719504, "cat25k": 0}, {"ncat": 16, "s": 0.4825174825174825, "cat": 1, "x": 0.48484848484848486, "os": 0.18580311830553375, "ncat25k": 9, "term": "had", "bg": 9.15998072040567e-08, "y": 0.4654234654234654, "cat25k": 7}, {"ncat": 16, "s": 0.4825174825174825, "cat": 1, "x": 0.48562548562548563, "os": 0.18580311830553375, "ncat25k": 9, "term": "further", "bg": 3.100895044169605e-07, "y": 0.47086247086247085, "cat25k": 7}, {"ncat": 16, "s": 0.7762237762237763, "cat": 2, "x": 0.4864024864024864, "os": 0.6011033184767522, "ncat25k": 9, "term": "widely", "bg": 3.1543521385042976e-06, "y": 0.6674436674436675, "cat25k": 15}, {"ncat": 16, "s": 0.06604506604506605, "cat": 0, "x": 0.48717948717948717, "os": 0.045082088186235825, "ncat25k": 9, "term": "little", "bg": 1.8156560173457096e-07, "y": 0.2097902097902098, "cat25k": 0}, {"ncat": 16, "s": 0.4825174825174825, "cat": 1, "x": 0.48795648795648794, "os": 0.18580311830553375, "ncat25k": 9, "term": "down", "bg": 1.688637341965375e-07, "y": 0.4825174825174825, "cat25k": 7}, {"ncat": 16, "s": 0.4825174825174825, "cat": 1, "x": 0.4887334887334887, "os": 0.18580311830553375, "ncat25k": 9, "term": "changing", "bg": 1.1803353276755307e-06, "y": 0.48329448329448327, "cat25k": 7}, {"ncat": 16, "s": 0.7762237762237763, "cat": 2, "x": 0.48951048951048953, "os": 0.6011033184767522, "ncat25k": 9, "term": "domain", "bg": 5.667969014800428e-07, "y": 0.6767676767676768, "cat25k": 15}, {"ncat": 16, "s": 0.06604506604506605, "cat": 0, "x": 0.4902874902874903, "os": 0.045082088186235825, "ncat25k": 9, "term": "impact", "bg": 7.524312320791531e-07, "y": 0.2261072261072261, "cat25k": 0}, {"ncat": 16, "s": 0.06604506604506605, "cat": 0, "x": 0.49106449106449107, "os": 0.045082088186235825, "ncat25k": 9, "term": "experiments", "bg": 4.34525388558034e-06, "y": 0.23620823620823622, "cat25k": 0}, {"ncat": 16, "s": 0.06604506604506605, "cat": 0, "x": 0.49184149184149184, "os": 0.045082088186235825, "ncat25k": 9, "term": "monitoring", "bg": 9.57737968494587e-07, "y": 0.236985236985237, "cat25k": 0}, {"ncat": 16, "s": 0.06604506604506605, "cat": 0, "x": 0.4926184926184926, "os": 0.045082088186235825, "ncat25k": 9, "term": "teams", "bg": 1.5072062720186172e-06, "y": 0.24553224553224554, "cat25k": 0}, {"ncat": 17, "s": 0.752913752913753, "cat": 2, "x": 0.4933954933954934, "os": 0.5906563765367487, "ncat25k": 10, "term": "deployment", "bg": 5.054610006510338e-06, "y": 0.5120435120435121, "cat25k": 15}, {"ncat": 17, "s": 0.752913752913753, "cat": 2, "x": 0.49417249417249415, "os": 0.5906563765367487, "ncat25k": 10, "term": "initial", "bg": 9.64076090573117e-07, "y": 0.515928515928516, "cat25k": 15}, {"ncat": 17, "s": 0.752913752913753, "cat": 2, "x": 0.494949494949495, "os": 0.5906563765367487, "ncat25k": 10, "term": "products", "bg": 1.6888009320617796e-07, "y": 0.5392385392385393, "cat25k": 15}, {"ncat": 17, "s": 0.752913752913753, "cat": 2, "x": 0.49572649572649574, "os": 0.5906563765367487, "ncat25k": 10, "term": "environments", "bg": 3.271571758989554e-06, "y": 0.5485625485625486, "cat25k": 15}, {"ncat": 17, "s": 0.05516705516705517, "cat": 0, "x": 0.4965034965034965, "os": 0.036339294419592494, "ncat25k": 10, "term": "components", "bg": 9.440407843047265e-07, "y": 0.03885003885003885, "cat25k": 0}, {"ncat": 17, "s": 0.05516705516705517, "cat": 0, "x": 0.4972804972804973, "os": 0.036339294419592494, "ncat25k": 10, "term": "participants", "bg": 1.564772230613132e-06, "y": 0.052059052059052056, "cat25k": 0}, {"ncat": 17, "s": 0.05516705516705517, "cat": 0, "x": 0.49805749805749805, "os": 0.036339294419592494, "ncat25k": 10, "term": "associated", "bg": 5.203857203036034e-07, "y": 0.05827505827505827, "cat25k": 0}, {"ncat": 17, "s": 0.8764568764568765, "cat": 3, "x": 0.4988344988344988, "os": 0.671486360791416, "ncat25k": 10, "term": "pre", "bg": 1.099369267890399e-06, "y": 0.7420357420357421, "cat25k": 22}, {"ncat": 17, "s": 0.752913752913753, "cat": 2, "x": 0.4996114996114996, "os": 0.5906563765367487, "ncat25k": 10, "term": "integrated", "bg": 1.1665606245383801e-06, "y": 0.5710955710955711, "cat25k": 15}, {"ncat": 17, "s": 0.4405594405594406, "cat": 1, "x": 0.5003885003885004, "os": 0.17009741521989252, "ncat25k": 10, "term": "julia", "bg": 1.902525041660086e-05, "y": 0.33644133644133645, "cat25k": 7}, {"ncat": 17, "s": 0.05516705516705517, "cat": 0, "x": 0.5011655011655012, "os": 0.036339294419592494, "ncat25k": 10, "term": "background", "bg": 6.879188231329367e-07, "y": 0.08702408702408702, "cat25k": 0}, {"ncat": 17, "s": 0.4405594405594406, "cat": 1, "x": 0.5019425019425019, "os": 0.17009741521989252, "ncat25k": 10, "term": "larger", "bg": 6.842242742506432e-07, "y": 0.3682983682983683, "cat25k": 7}, {"ncat": 17, "s": 0.9277389277389277, "cat": 4, "x": 0.5027195027195027, "os": 0.7325748836160971, "ncat25k": 10, "term": "step", "bg": 6.723475396890811e-07, "y": 0.8430458430458431, "cat25k": 30}, {"ncat": 17, "s": 0.752913752913753, "cat": 2, "x": 0.5034965034965035, "os": 0.5906563765367487, "ncat25k": 10, "term": "sample", "bg": 6.948963583073245e-07, "y": 0.5982905982905983, "cat25k": 15}, {"ncat": 17, "s": 0.4405594405594406, "cat": 1, "x": 0.5042735042735043, "os": 0.17009741521989252, "ncat25k": 10, "term": "manage", "bg": 1.1488888876731334e-06, "y": 0.38927738927738925, "cat25k": 7}, {"ncat": 17, "s": 0.4405594405594406, "cat": 1, "x": 0.5050505050505051, "os": 0.17009741521989252, "ncat25k": 10, "term": "'re", "bg": 0.0, "y": 0.3916083916083916, "cat25k": 7}, {"ncat": 17, "s": 0.8764568764568765, "cat": 3, "x": 0.5058275058275058, "os": 0.671486360791416, "ncat25k": 10, "term": "predictions", "bg": 8.276340098953992e-06, "y": 0.7692307692307693, "cat25k": 22}, {"ncat": 17, "s": 0.05516705516705517, "cat": 0, "x": 0.5066045066045066, "os": 0.036339294419592494, "ncat25k": 10, "term": "let", "bg": 2.3746323501654654e-07, "y": 0.11033411033411034, "cat25k": 0}, {"ncat": 17, "s": 0.05516705516705517, "cat": 0, "x": 0.5073815073815073, "os": 0.036339294419592494, "ncat25k": 10, "term": "away", "bg": 3.3641687433399974e-07, "y": 0.12665112665112666, "cat25k": 0}, {"ncat": 17, "s": 0.9277389277389277, "cat": 4, "x": 0.5081585081585082, "os": 0.7325748836160971, "ncat25k": 10, "term": "require", "bg": 7.833931398038921e-07, "y": 0.8453768453768454, "cat25k": 30}, {"ncat": 17, "s": 0.9735819735819736, "cat": 5, "x": 0.508935508935509, "os": 0.7810375681080519, "ncat25k": 10, "term": "multi", "bg": 1.3031885784643417e-06, "y": 0.885003885003885, "cat25k": 37}, {"ncat": 17, "s": 0.4405594405594406, "cat": 1, "x": 0.5097125097125097, "os": 0.17009741521989252, "ncat25k": 10, "term": "terms", "bg": 1.3677747017924505e-07, "y": 0.4094794094794095, "cat25k": 7}, {"ncat": 17, "s": 0.05516705516705517, "cat": 0, "x": 0.5104895104895105, "os": 0.036339294419592494, "ncat25k": 10, "term": "benefits", "bg": 5.475981718356805e-07, "y": 0.14219114219114218, "cat25k": 0}, {"ncat": 17, "s": 0.752913752913753, "cat": 2, "x": 0.5112665112665112, "os": 0.5906563765367487, "ncat25k": 10, "term": "ml", "bg": 3.7125391008129953e-06, "y": 0.6239316239316239, "cat25k": 15}, {"ncat": 17, "s": 0.752913752913753, "cat": 2, "x": 0.5120435120435121, "os": 0.5906563765367487, "ncat25k": 10, "term": "stored", "bg": 2.1610552346268476e-06, "y": 0.6247086247086248, "cat25k": 15}, {"ncat": 17, "s": 0.05516705516705517, "cat": 0, "x": 0.5128205128205128, "os": 0.036339294419592494, "ncat25k": 10, "term": "keep", "bg": 3.174069204414626e-07, "y": 0.14685314685314685, "cat25k": 0}, {"ncat": 17, "s": 0.9735819735819736, "cat": 5, "x": 0.5135975135975136, "os": 0.7810375681080519, "ncat25k": 10, "term": "train", "bg": 2.12935126276323e-06, "y": 0.8888888888888888, "cat25k": 37}, {"ncat": 17, "s": 0.4405594405594406, "cat": 1, "x": 0.5143745143745144, "os": 0.17009741521989252, "ncat25k": 10, "term": "demo", "bg": 2.3297112690976333e-06, "y": 0.43123543123543123, "cat25k": 7}, {"ncat": 17, "s": 0.05516705516705517, "cat": 0, "x": 0.5151515151515151, "os": 0.036339294419592494, "ncat25k": 10, "term": "levels", "bg": 6.816989181882755e-07, "y": 0.16083916083916083, "cat25k": 0}, {"ncat": 17, "s": 0.05516705516705517, "cat": 0, "x": 0.515928515928516, "os": 0.036339294419592494, "ncat25k": 10, "term": "advantage", "bg": 1.0882349678249727e-06, "y": 0.17482517482517482, "cat25k": 0}, {"ncat": 17, "s": 0.752913752913753, "cat": 2, "x": 0.5167055167055167, "os": 0.5906563765367487, "ncat25k": 10, "term": "selection", "bg": 8.497716232123775e-07, "y": 0.6503496503496503, "cat25k": 15}, {"ncat": 17, "s": 0.05516705516705517, "cat": 0, "x": 0.5174825174825175, "os": 0.036339294419592494, "ncat25k": 10, "term": "intelligence", "bg": 1.7061094420551366e-06, "y": 0.1888111888111888, "cat25k": 0}, {"ncat": 17, "s": 0.4405594405594406, "cat": 1, "x": 0.5182595182595182, "os": 0.17009741521989252, "ncat25k": 10, "term": "context", "bg": 1.0322952588595879e-06, "y": 0.46153846153846156, "cat25k": 7}, {"ncat": 17, "s": 0.752913752913753, "cat": 2, "x": 0.519036519036519, "os": 0.5906563765367487, "ncat25k": 10, "term": "visual", "bg": 2.2760482794562422e-06, "y": 0.6557886557886557, "cat25k": 15}, {"ncat": 17, "s": 0.752913752913753, "cat": 2, "x": 0.5198135198135199, "os": 0.5906563765367487, "ncat25k": 10, "term": "technologies", "bg": 8.913259721692377e-07, "y": 0.6581196581196581, "cat25k": 15}, {"ncat": 17, "s": 0.752913752913753, "cat": 2, "x": 0.5205905205905206, "os": 0.5906563765367487, "ncat25k": 10, "term": "file", "bg": 2.562175253068323e-07, "y": 0.6697746697746698, "cat25k": 15}, {"ncat": 17, "s": 0.4405594405594406, "cat": 1, "x": 0.5213675213675214, "os": 0.17009741521989252, "ncat25k": 10, "term": "address", "bg": 1.4504343323699814e-07, "y": 0.5050505050505051, "cat25k": 7}, {"ncat": 18, "s": 0.8679098679098679, "cat": 3, "x": 0.5221445221445221, "os": 0.659895152221076, "ncat25k": 10, "term": "processes", "bg": 1.3141909625614794e-06, "y": 0.7016317016317016, "cat25k": 22}, {"ncat": 18, "s": 0.37373737373737376, "cat": 1, "x": 0.5229215229215229, "os": 0.1564084045203158, "ncat25k": 10, "term": "ideas", "bg": 8.601858387012142e-07, "y": 0.2571872571872572, "cat25k": 7}, {"ncat": 18, "s": 0.7404817404817405, "cat": 2, "x": 0.5236985236985237, "os": 0.5811233392216532, "ncat25k": 10, "term": "formats", "bg": 4.397669067863782e-06, "y": 0.5221445221445221, "cat25k": 15}, {"ncat": 18, "s": 0.37373737373737376, "cat": 1, "x": 0.5244755244755245, "os": 0.1564084045203158, "ncat25k": 10, "term": "soon", "bg": 7.776476182876228e-07, "y": 0.2773892773892774, "cat25k": 7}, {"ncat": 18, "s": 0.9238539238539238, "cat": 4, "x": 0.5252525252525253, "os": 0.7202078522944554, "ncat25k": 10, "term": "highly", "bg": 1.1634697852248738e-06, "y": 0.8205128205128205, "cat25k": 30}, {"ncat": 18, "s": 0.8679098679098679, "cat": 3, "x": 0.526029526029526, "os": 0.659895152221076, "ncat25k": 10, "term": "enough", "bg": 5.734065235791212e-07, "y": 0.7233877233877234, "cat25k": 22}, {"ncat": 18, "s": 0.046620046620046623, "cat": 0, "x": 0.5268065268065268, "os": 0.02905115852299195, "ncat25k": 10, "term": "databases", "bg": 3.3125618833717816e-06, "y": 0.03341103341103341, "cat25k": 0}, {"ncat": 18, "s": 0.37373737373737376, "cat": 1, "x": 0.5275835275835276, "os": 0.1564084045203158, "ncat25k": 10, "term": "comes", "bg": 6.213647540505486e-07, "y": 0.2937062937062937, "cat25k": 7}, {"ncat": 18, "s": 0.7404817404817405, "cat": 2, "x": 0.5283605283605284, "os": 0.5811233392216532, "ncat25k": 10, "term": "storage", "bg": 7.613143655379215e-07, "y": 0.5508935508935509, "cat25k": 15}, {"ncat": 18, "s": 0.046620046620046623, "cat": 0, "x": 0.5291375291375291, "os": 0.02905115852299195, "ncat25k": 10, "term": "version", "bg": 2.4939415923866946e-07, "y": 0.04428904428904429, "cat25k": 0}, {"ncat": 18, "s": 0.8679098679098679, "cat": 3, "x": 0.5299145299145299, "os": 0.659895152221076, "ncat25k": 10, "term": "instead", "bg": 9.184443820020014e-07, "y": 0.7404817404817405, "cat25k": 22}, {"ncat": 18, "s": 0.37373737373737376, "cat": 1, "x": 0.5306915306915307, "os": 0.1564084045203158, "ncat25k": 10, "term": "dynamic", "bg": 1.7705642870101053e-06, "y": 0.32245532245532244, "cat25k": 7}, {"ncat": 18, "s": 0.968919968919969, "cat": 5, "x": 0.5314685314685315, "os": 0.7686706849340951, "ncat25k": 10, "term": "related", "bg": 2.3351845855088226e-07, "y": 0.8725718725718725, "cat25k": 37}, {"ncat": 18, "s": 0.7404817404817405, "cat": 2, "x": 0.5322455322455323, "os": 0.5811233392216532, "ncat25k": 10, "term": "currently", "bg": 5.605782180449479e-07, "y": 0.5726495726495726, "cat25k": 15}, {"ncat": 18, "s": 0.37373737373737376, "cat": 1, "x": 0.533022533022533, "os": 0.1564084045203158, "ncat25k": 10, "term": "question", "bg": 4.894382813203448e-07, "y": 0.3356643356643357, "cat25k": 7}, {"ncat": 18, "s": 0.37373737373737376, "cat": 1, "x": 0.5337995337995338, "os": 0.1564084045203158, "ncat25k": 10, "term": "past", "bg": 4.350159754415703e-07, "y": 0.3411033411033411, "cat25k": 7}, {"ncat": 18, "s": 0.37373737373737376, "cat": 1, "x": 0.5345765345765345, "os": 0.1564084045203158, "ncat25k": 10, "term": "'", "bg": 0.0, "y": 0.3550893550893551, "cat25k": 7}, {"ncat": 18, "s": 0.7404817404817405, "cat": 2, "x": 0.5353535353535354, "os": 0.5811233392216532, "ncat25k": 10, "term": "automatically", "bg": 1.4146846416119979e-06, "y": 0.5843045843045843, "cat25k": 15}, {"ncat": 18, "s": 0.8679098679098679, "cat": 3, "x": 0.5361305361305362, "os": 0.659895152221076, "ncat25k": 10, "term": "difficult", "bg": 1.1749793321135481e-06, "y": 0.756021756021756, "cat25k": 22}, {"ncat": 18, "s": 0.8679098679098679, "cat": 3, "x": 0.5369075369075369, "os": 0.659895152221076, "ncat25k": 10, "term": "modern", "bg": 1.2248973046099815e-06, "y": 0.7591297591297591, "cat25k": 22}, {"ncat": 18, "s": 0.37373737373737376, "cat": 1, "x": 0.5376845376845377, "os": 0.1564084045203158, "ncat25k": 10, "term": "kind", "bg": 5.954242412604587e-07, "y": 0.3714063714063714, "cat25k": 7}, {"ncat": 18, "s": 0.046620046620046623, "cat": 0, "x": 0.5384615384615384, "os": 0.02905115852299195, "ncat25k": 10, "term": "concept", "bg": 1.2579021016933972e-06, "y": 0.10101010101010101, "cat25k": 0}, {"ncat": 18, "s": 0.37373737373737376, "cat": 1, "x": 0.5392385392385393, "os": 0.1564084045203158, "ncat25k": 10, "term": "whether", "bg": 3.9949554746042296e-07, "y": 0.4024864024864025, "cat25k": 7}, {"ncat": 18, "s": 0.8679098679098679, "cat": 3, "x": 0.54001554001554, "os": 0.659895152221076, "ncat25k": 10, "term": "scipy", "bg": 9.346231132295901e-05, "y": 0.7738927738927739, "cat25k": 22}, {"ncat": 18, "s": 0.37373737373737376, "cat": 1, "x": 0.5407925407925408, "os": 0.1564084045203158, "ncat25k": 10, "term": "job", "bg": 3.261641662303096e-07, "y": 0.40714840714840717, "cat25k": 7}, {"ncat": 18, "s": 0.046620046620046623, "cat": 0, "x": 0.5415695415695416, "os": 0.02905115852299195, "ncat25k": 10, "term": "everything", "bg": 4.152746496486241e-07, "y": 0.14063714063714064, "cat25k": 0}, {"ncat": 18, "s": 0.37373737373737376, "cat": 1, "x": 0.5423465423465423, "os": 0.1564084045203158, "ncat25k": 10, "term": "complete", "bg": 2.944525218496654e-07, "y": 0.4156954156954157, "cat25k": 7}, {"ncat": 18, "s": 0.8679098679098679, "cat": 3, "x": 0.5431235431235432, "os": 0.659895152221076, "ncat25k": 10, "term": "written", "bg": 6.722724204303711e-07, "y": 0.7793317793317793, "cat25k": 22}, {"ncat": 18, "s": 0.968919968919969, "cat": 5, "x": 0.5439005439005439, "os": 0.7686706849340951, "ncat25k": 10, "term": "variables", "bg": 2.4856619767334584e-06, "y": 0.8881118881118881, "cat25k": 37}, {"ncat": 18, "s": 0.37373737373737376, "cat": 1, "x": 0.5446775446775447, "os": 0.1564084045203158, "ncat25k": 10, "term": "app", "bg": 5.3770278218010615e-06, "y": 0.43356643356643354, "cat25k": 7}, {"ncat": 18, "s": 0.046620046620046623, "cat": 0, "x": 0.5454545454545454, "os": 0.02905115852299195, "ncat25k": 10, "term": "ai", "bg": 8.73327133788865e-06, "y": 0.16006216006216006, "cat25k": 0}, {"ncat": 18, "s": 0.046620046620046623, "cat": 0, "x": 0.5462315462315462, "os": 0.02905115852299195, "ncat25k": 10, "term": "think", "bg": 1.7433258021811603e-07, "y": 0.16161616161616163, "cat25k": 0}, {"ncat": 18, "s": 0.37373737373737376, "cat": 1, "x": 0.5470085470085471, "os": 0.1564084045203158, "ncat25k": 10, "term": "providing", "bg": 8.953895702267816e-07, "y": 0.452991452991453, "cat25k": 7}, {"ncat": 18, "s": 0.046620046620046623, "cat": 0, "x": 0.5477855477855478, "os": 0.02905115852299195, "ncat25k": 10, "term": "highlight", "bg": 3.6415527619309007e-06, "y": 0.17871017871017872, "cat25k": 0}, {"ncat": 18, "s": 0.7404817404817405, "cat": 2, "x": 0.5485625485625486, "os": 0.5811233392216532, "ncat25k": 10, "term": "solve", "bg": 4.126816480913253e-06, "y": 0.6519036519036518, "cat25k": 15}, {"ncat": 18, "s": 0.9238539238539238, "cat": 4, "x": 0.5493395493395493, "os": 0.7202078522944554, "ncat25k": 10, "term": "generate", "bg": 2.7389574684276265e-06, "y": 0.8531468531468531, "cat25k": 30}, {"ncat": 18, "s": 0.7404817404817405, "cat": 2, "x": 0.5501165501165501, "os": 0.5811233392216532, "ncat25k": 10, "term": "predicting", "bg": 1.8043664848767433e-05, "y": 0.6643356643356644, "cat25k": 15}, {"ncat": 18, "s": 0.8679098679098679, "cat": 3, "x": 0.5508935508935509, "os": 0.659895152221076, "ncat25k": 10, "term": "computer", "bg": 2.407547093025542e-07, "y": 0.7933177933177933, "cat25k": 22}, {"ncat": 18, "s": 0.37373737373737376, "cat": 1, "x": 0.5516705516705517, "os": 0.1564084045203158, "ncat25k": 10, "term": "top", "bg": 9.910569518950391e-08, "y": 0.48174048174048173, "cat25k": 7}, {"ncat": 18, "s": 0.046620046620046623, "cat": 0, "x": 0.5524475524475524, "os": 0.02905115852299195, "ncat25k": 10, "term": "mean", "bg": 4.973281294907176e-07, "y": 0.2198912198912199, "cat25k": 0}, {"ncat": 18, "s": 0.37373737373737376, "cat": 1, "x": 0.5532245532245532, "os": 0.1564084045203158, "ncat25k": 10, "term": "capabilities", "bg": 2.4320735671300394e-06, "y": 0.49184149184149184, "cat25k": 7}, {"ncat": 18, "s": 0.046620046620046623, "cat": 0, "x": 0.554001554001554, "os": 0.02905115852299195, "ncat25k": 10, "term": "life", "bg": 1.6303807283989373e-07, "y": 0.22533022533022534, "cat25k": 0}, {"ncat": 18, "s": 0.046620046620046623, "cat": 0, "x": 0.5547785547785548, "os": 0.02905115852299195, "ncat25k": 10, "term": "today", "bg": 1.5038295897737516e-07, "y": 0.2331002331002331, "cat25k": 0}, {"ncat": 19, "s": 0.03496503496503497, "cat": 0, "x": 0.5555555555555556, "os": 0.02302796696846998, "ncat25k": 11, "term": "transform", "bg": 7.099140149515781e-06, "y": 0.003885003885003885, "cat25k": 0}, {"ncat": 19, "s": 0.03496503496503497, "cat": 0, "x": 0.5563325563325563, "os": 0.02302796696846998, "ncat25k": 11, "term": "pydata", "bg": 0.00044206409929439767, "y": 0.009324009324009324, "cat25k": 0}, {"ncat": 19, "s": 0.9121989121989122, "cat": 4, "x": 0.5571095571095571, "os": 0.7085936769079131, "ncat25k": 11, "term": "machines", "bg": 1.7412461750625689e-06, "y": 0.8111888111888111, "cat25k": 30}, {"ncat": 19, "s": 0.03496503496503497, "cat": 0, "x": 0.5578865578865578, "os": 0.02302796696846998, "ncat25k": 11, "term": "laptop", "bg": 1.2512172388707277e-06, "y": 0.013986013986013986, "cat25k": 0}, {"ncat": 19, "s": 0.9121989121989122, "cat": 4, "x": 0.5586635586635587, "os": 0.7085936769079131, "ncat25k": 11, "term": "predictive", "bg": 3.468338405778252e-05, "y": 0.8158508158508159, "cat25k": 30}, {"ncat": 19, "s": 0.03496503496503497, "cat": 0, "x": 0.5594405594405595, "os": 0.02302796696846998, "ncat25k": 11, "term": "clustering", "bg": 3.221845544724586e-05, "y": 0.018648018648018648, "cat25k": 0}, {"ncat": 19, "s": 0.8523698523698524, "cat": 3, "x": 0.5602175602175602, "os": 0.6491442325582681, "ncat25k": 11, "term": "leverage", "bg": 1.2016437525219498e-05, "y": 0.717948717948718, "cat25k": 22}, {"ncat": 19, "s": 0.35508935508935513, "cat": 1, "x": 0.560994560994561, "os": 0.14448154143663833, "ncat25k": 11, "term": "designed", "bg": 5.019293746888561e-07, "y": 0.28438228438228436, "cat25k": 7}, {"ncat": 19, "s": 0.03496503496503497, "cat": 0, "x": 0.5617715617715617, "os": 0.02302796696846998, "ncat25k": 11, "term": "back", "bg": 9.013775476501717e-08, "y": 0.03496503496503497, "cat25k": 0}, {"ncat": 19, "s": 0.7249417249417249, "cat": 2, "x": 0.5625485625485626, "os": 0.5723930411431304, "ncat25k": 11, "term": "form", "bg": 2.4812317028701615e-07, "y": 0.5524475524475524, "cat25k": 15}, {"ncat": 19, "s": 0.7249417249417249, "cat": 2, "x": 0.5633255633255633, "os": 0.5723930411431304, "ncat25k": 11, "term": "ability", "bg": 8.441746624236659e-07, "y": 0.5656565656565656, "cat25k": 15}, {"ncat": 19, "s": 0.35508935508935513, "cat": 1, "x": 0.5641025641025641, "os": 0.14448154143663833, "ncat25k": 11, "term": "topic", "bg": 6.169554706755968e-07, "y": 0.3201243201243201, "cat25k": 7}, {"ncat": 19, "s": 0.9541569541569542, "cat": 5, "x": 0.5648795648795649, "os": 0.7569186885995417, "ncat25k": 11, "term": "tensorflow", "bg": 0.0007479940160478717, "y": 0.8756798756798757, "cat25k": 37}, {"ncat": 19, "s": 0.8523698523698524, "cat": 3, "x": 0.5656565656565656, "os": 0.6491442325582681, "ncat25k": 11, "term": "function", "bg": 7.234309991778952e-07, "y": 0.7575757575757576, "cat25k": 22}, {"ncat": 19, "s": 0.8523698523698524, "cat": 3, "x": 0.5664335664335665, "os": 0.6491442325582681, "ncat25k": 11, "term": "millions", "bg": 2.8673092434117125e-06, "y": 0.7653457653457654, "cat25k": 22}, {"ncat": 19, "s": 0.35508935508935513, "cat": 1, "x": 0.5672105672105672, "os": 0.14448154143663833, "ncat25k": 11, "term": "speed", "bg": 6.005762312731623e-07, "y": 0.3838383838383838, "cat25k": 7}, {"ncat": 19, "s": 0.7249417249417249, "cat": 2, "x": 0.567987567987568, "os": 0.5723930411431304, "ncat25k": 11, "term": "apache", "bg": 7.0781456673980545e-06, "y": 0.5998445998445998, "cat25k": 15}, {"ncat": 19, "s": 0.7249417249417249, "cat": 2, "x": 0.5687645687645687, "os": 0.5723930411431304, "ncat25k": 11, "term": "among", "bg": 6.223876177419728e-07, "y": 0.6045066045066045, "cat25k": 15}, {"ncat": 19, "s": 0.03496503496503497, "cat": 0, "x": 0.5695415695415695, "os": 0.02302796696846998, "ncat25k": 11, "term": "skills", "bg": 5.594309982562256e-07, "y": 0.12198912198912198, "cat25k": 0}, {"ncat": 19, "s": 0.7249417249417249, "cat": 2, "x": 0.5703185703185704, "os": 0.5723930411431304, "ncat25k": 11, "term": "usage", "bg": 2.1128383216598365e-06, "y": 0.6122766122766122, "cat25k": 15}, {"ncat": 19, "s": 0.03496503496503497, "cat": 0, "x": 0.5710955710955711, "os": 0.02302796696846998, "ncat25k": 11, "term": "visualizations", "bg": 0.00016241600772172107, "y": 0.12276612276612277, "cat25k": 0}, {"ncat": 19, "s": 0.8523698523698524, "cat": 3, "x": 0.5718725718725719, "os": 0.6491442325582681, "ncat25k": 11, "term": "engine", "bg": 1.023074119558718e-06, "y": 0.7754467754467754, "cat25k": 22}, {"ncat": 19, "s": 0.03496503496503497, "cat": 0, "x": 0.5726495726495726, "os": 0.02302796696846998, "ncat25k": 11, "term": "collection", "bg": 3.916466981245998e-07, "y": 0.13675213675213677, "cat25k": 0}, {"ncat": 19, "s": 0.35508935508935513, "cat": 1, "x": 0.5734265734265734, "os": 0.14448154143663833, "ncat25k": 11, "term": "against", "bg": 3.1212372975480667e-07, "y": 0.41647241647241645, "cat25k": 7}, {"ncat": 19, "s": 0.03496503496503497, "cat": 0, "x": 0.5742035742035742, "os": 0.02302796696846998, "ncat25k": 11, "term": "words", "bg": 5.932255357579106e-07, "y": 0.14607614607614608, "cat25k": 0}, {"ncat": 19, "s": 0.7249417249417249, "cat": 2, "x": 0.574980574980575, "os": 0.5723930411431304, "ncat25k": 11, "term": "year", "bg": 1.4627328255109114e-07, "y": 0.627039627039627, "cat25k": 15}, {"ncat": 19, "s": 0.03496503496503497, "cat": 0, "x": 0.5757575757575758, "os": 0.02302796696846998, "ncat25k": 11, "term": "requires", "bg": 9.838775470924276e-07, "y": 0.14996114996114995, "cat25k": 0}, {"ncat": 19, "s": 0.03496503496503497, "cat": 0, "x": 0.5765345765345765, "os": 0.02302796696846998, "ncat25k": 11, "term": "industry", "bg": 3.4797683008704615e-07, "y": 0.1554001554001554, "cat25k": 0}, {"ncat": 19, "s": 0.03496503496503497, "cat": 0, "x": 0.5773115773115773, "os": 0.02302796696846998, "ncat25k": 11, "term": "required", "bg": 2.756950997163486e-07, "y": 0.1592851592851593, "cat25k": 0}, {"ncat": 19, "s": 0.9541569541569542, "cat": 5, "x": 0.578088578088578, "os": 0.7569186885995417, "ncat25k": 11, "term": "parameters", "bg": 2.6631102604545136e-06, "y": 0.8912198912198912, "cat25k": 37}, {"ncat": 19, "s": 0.8523698523698524, "cat": 3, "x": 0.5788655788655789, "os": 0.6491442325582681, "ncat25k": 11, "term": "range", "bg": 3.737370257970114e-07, "y": 0.7863247863247863, "cat25k": 22}, {"ncat": 19, "s": 0.03496503496503497, "cat": 0, "x": 0.5796425796425796, "os": 0.02302796696846998, "ncat25k": 11, "term": "really", "bg": 2.960203432086995e-07, "y": 0.18725718725718726, "cat25k": 0}, {"ncat": 19, "s": 0.7249417249417249, "cat": 2, "x": 0.5804195804195804, "os": 0.5723930411431304, "ncat25k": 11, "term": "must", "bg": 1.6388274047887028e-07, "y": 0.6565656565656566, "cat25k": 15}, {"ncat": 19, "s": 0.03496503496503497, "cat": 0, "x": 0.5811965811965812, "os": 0.02302796696846998, "ncat25k": 11, "term": "directly", "bg": 7.552103550049927e-07, "y": 0.20357420357420358, "cat25k": 0}, {"ncat": 19, "s": 0.03496503496503497, "cat": 0, "x": 0.581973581973582, "os": 0.02302796696846998, "ncat25k": 11, "term": "increase", "bg": 5.846479558776906e-07, "y": 0.20901320901320902, "cat25k": 0}, {"ncat": 19, "s": 0.35508935508935513, "cat": 1, "x": 0.5827505827505828, "os": 0.14448154143663833, "ncat25k": 11, "term": "insights", "bg": 9.0940636680594e-06, "y": 0.49106449106449107, "cat25k": 7}, {"ncat": 19, "s": 0.7249417249417249, "cat": 2, "x": 0.5835275835275835, "os": 0.5723930411431304, "ncat25k": 11, "term": "result", "bg": 3.449825823390511e-07, "y": 0.6837606837606838, "cat25k": 15}, {"ncat": 19, "s": 0.8523698523698524, "cat": 3, "x": 0.5843045843045843, "os": 0.6491442325582681, "ncat25k": 11, "term": "customer", "bg": 3.68514698149123e-07, "y": 0.7964257964257965, "cat25k": 22}, {"ncat": 20, "s": 0.2983682983682984, "cat": 1, "x": 0.585081585081585, "os": 0.13408894311198782, "ncat25k": 11, "term": "steps", "bg": 1.3551575550476728e-06, "y": 0.26262626262626265, "cat25k": 7}, {"ncat": 20, "s": 0.0303030303030303, "cat": 0, "x": 0.5858585858585859, "os": 0.018094590247545117, "ncat25k": 11, "term": "bring", "bg": 7.968720899552176e-07, "y": 0.012432012432012432, "cat25k": 0}, {"ncat": 20, "s": 0.0303030303030303, "cat": 0, "x": 0.5866355866355867, "os": 0.018094590247545117, "ncat25k": 11, "term": "exploratory", "bg": 4.214525596609023e-05, "y": 0.01320901320901321, "cat25k": 0}, {"ncat": 20, "s": 0.9090909090909091, "cat": 4, "x": 0.5874125874125874, "os": 0.6976770712065212, "ncat25k": 11, "term": "looking", "bg": 3.715306468199948e-07, "y": 0.815073815073815, "cat25k": 30}, {"ncat": 20, "s": 0.9090909090909091, "cat": 4, "x": 0.5881895881895882, "os": 0.6976770712065212, "ncat25k": 11, "term": "fields", "bg": 1.4210264779863644e-06, "y": 0.8236208236208237, "cat25k": 30}, {"ncat": 20, "s": 0.8306138306138307, "cat": 3, "x": 0.5889665889665889, "os": 0.6391521292349586, "ncat25k": 11, "term": "bayesian", "bg": 9.187906976166694e-05, "y": 0.7342657342657343, "cat25k": 22}, {"ncat": 20, "s": 0.2983682983682984, "cat": 1, "x": 0.5897435897435898, "os": 0.13408894311198782, "ncat25k": 11, "term": "always", "bg": 3.444162490169558e-07, "y": 0.31313131313131315, "cat25k": 7}, {"ncat": 20, "s": 0.2983682983682984, "cat": 1, "x": 0.5905205905205905, "os": 0.13408894311198782, "ncat25k": 11, "term": "moving", "bg": 1.0407931553124742e-06, "y": 0.3185703185703186, "cat25k": 7}, {"ncat": 20, "s": 0.7171717171717172, "cat": 2, "x": 0.5912975912975913, "os": 0.5643709911939453, "ncat25k": 11, "term": "details", "bg": 2.0644575324249574e-07, "y": 0.5687645687645687, "cat25k": 15}, {"ncat": 20, "s": 0.2983682983682984, "cat": 1, "x": 0.5920745920745921, "os": 0.13408894311198782, "ncat25k": 11, "term": "answer", "bg": 8.95515138212494e-07, "y": 0.3348873348873349, "cat25k": 7}, {"ncat": 20, "s": 0.2983682983682984, "cat": 1, "x": 0.5928515928515928, "os": 0.13408894311198782, "ncat25k": 11, "term": "graph", "bg": 7.2869492592464516e-06, "y": 0.34498834498834496, "cat25k": 7}, {"ncat": 20, "s": 0.9090909090909091, "cat": 4, "x": 0.5936285936285937, "os": 0.6976770712065212, "ncat25k": 11, "term": "insight", "bg": 4.088050979405386e-06, "y": 0.8383838383838383, "cat25k": 30}, {"ncat": 20, "s": 0.0303030303030303, "cat": 0, "x": 0.5944055944055944, "os": 0.018094590247545117, "ncat25k": 11, "term": "dataframes", "bg": 0.0005440511408072359, "y": 0.10023310023310024, "cat25k": 0}, {"ncat": 20, "s": 0.2983682983682984, "cat": 1, "x": 0.5951825951825952, "os": 0.13408894311198782, "ncat25k": 11, "term": "includes", "bg": 3.7305353986240506e-07, "y": 0.3954933954933955, "cat25k": 7}, {"ncat": 20, "s": 0.2983682983682984, "cat": 1, "x": 0.5959595959595959, "os": 0.13408894311198782, "ncat25k": 11, "term": "deal", "bg": 7.193584086140604e-07, "y": 0.4211344211344211, "cat25k": 7}, {"ncat": 20, "s": 0.2983682983682984, "cat": 1, "x": 0.5967365967365967, "os": 0.13408894311198782, "ncat25k": 11, "term": "addition", "bg": 6.25406947882351e-07, "y": 0.4234654234654235, "cat25k": 7}, {"ncat": 20, "s": 0.8306138306138307, "cat": 3, "x": 0.5975135975135976, "os": 0.6391521292349586, "ncat25k": 11, "term": "linear", "bg": 2.6016321078865613e-06, "y": 0.7808857808857809, "cat25k": 22}, {"ncat": 20, "s": 0.0303030303030303, "cat": 0, "x": 0.5982905982905983, "os": 0.018094590247545117, "ncat25k": 11, "term": "company", "bg": 2.0962428496848127e-07, "y": 0.1561771561771562, "cat25k": 0}, {"ncat": 20, "s": 0.8306138306138307, "cat": 3, "x": 0.5990675990675991, "os": 0.6391521292349586, "ncat25k": 11, "term": "known", "bg": 6.473001549775279e-07, "y": 0.7871017871017871, "cat25k": 22}, {"ncat": 20, "s": 0.0303030303030303, "cat": 0, "x": 0.5998445998445998, "os": 0.018094590247545117, "ncat25k": 11, "term": "keynote", "bg": 1.0191960726978e-05, "y": 0.216006216006216, "cat25k": 0}, {"ncat": 20, "s": 0.0303030303030303, "cat": 0, "x": 0.6006216006216006, "os": 0.018094590247545117, "ncat25k": 11, "term": "automated", "bg": 4.793350169846951e-06, "y": 0.23232323232323232, "cat25k": 0}, {"ncat": 20, "s": 0.8306138306138307, "cat": 3, "x": 0.6013986013986014, "os": 0.6391521292349586, "ncat25k": 11, "term": "reduce", "bg": 1.5019381761193732e-06, "y": 0.797979797979798, "cat25k": 22}, {"ncat": 21, "s": 0.7055167055167055, "cat": 2, "x": 0.6021756021756022, "os": 0.5569765007453507, "ncat25k": 12, "term": "files", "bg": 6.048926904448922e-07, "y": 0.5167055167055167, "cat25k": 15}, {"ncat": 21, "s": 0.7055167055167055, "cat": 2, "x": 0.602952602952603, "os": 0.5569765007453507, "ncat25k": 12, "term": "analyzing", "bg": 1.2983832442299177e-05, "y": 0.5182595182595182, "cat25k": 15}, {"ncat": 21, "s": 0.029526029526029524, "cat": 0, "x": 0.6037296037296037, "os": 0.014091100589488148, "ncat25k": 12, "term": "supervised", "bg": 2.025001403694155e-05, "y": 0.014763014763014764, "cat25k": 0}, {"ncat": 21, "s": 0.7055167055167055, "cat": 2, "x": 0.6045066045066045, "os": 0.5569765007453507, "ncat25k": 12, "term": "live", "bg": 3.611788958620293e-07, "y": 0.5361305361305362, "cat25k": 15}, {"ncat": 21, "s": 0.8951048951048951, "cat": 4, "x": 0.6052836052836053, "os": 0.6874059609086651, "ncat25k": 12, "term": "having", "bg": 4.5671391143723525e-07, "y": 0.8267288267288267, "cat25k": 30}, {"ncat": 21, "s": 0.7055167055167055, "cat": 2, "x": 0.6060606060606061, "os": 0.5569765007453507, "ncat25k": 12, "term": "compare", "bg": 2.945660484612645e-07, "y": 0.5571095571095571, "cat25k": 15}, {"ncat": 21, "s": 0.27738927738927743, "cat": 1, "x": 0.6068376068376068, "os": 0.12502677408550122, "ncat25k": 12, "term": "changes", "bg": 3.978945836607451e-07, "y": 0.3162393162393162, "cat25k": 7}, {"ncat": 21, "s": 0.7055167055167055, "cat": 2, "x": 0.6076146076146076, "os": 0.5569765007453507, "ncat25k": 12, "term": "finding", "bg": 1.6120303027306936e-06, "y": 0.567987567987568, "cat25k": 15}, {"ncat": 21, "s": 0.27738927738927743, "cat": 1, "x": 0.6083916083916084, "os": 0.12502677408550122, "ncat25k": 12, "term": "works", "bg": 4.863255094568731e-07, "y": 0.32400932400932403, "cat25k": 7}, {"ncat": 21, "s": 0.27738927738927743, "cat": 1, "x": 0.6091686091686092, "os": 0.12502677408550122, "ncat25k": 12, "term": "follow", "bg": 6.656958987387392e-07, "y": 0.331002331002331, "cat25k": 7}, {"ncat": 21, "s": 0.27738927738927743, "cat": 1, "x": 0.60994560994561, "os": 0.12502677408550122, "ncat25k": 12, "term": "ever", "bg": 5.180197968039454e-07, "y": 0.34032634032634035, "cat25k": 7}, {"ncat": 21, "s": 0.9300699300699301, "cat": 5, "x": 0.6107226107226107, "os": 0.735144295788878, "ncat25k": 12, "term": "prediction", "bg": 1.901056091762049e-05, "y": 0.8772338772338772, "cat25k": 37}, {"ncat": 21, "s": 0.8212898212898213, "cat": 3, "x": 0.6114996114996115, "os": 0.629846422345214, "ncat25k": 12, "term": "input", "bg": 1.1167469802594557e-06, "y": 0.7567987567987567, "cat25k": 22}, {"ncat": 21, "s": 0.9906759906759907, "cat": 8, "x": 0.6122766122766122, "os": 0.835142589349342, "ncat25k": 12, "term": "art", "bg": 3.817143181985732e-07, "y": 0.9456099456099456, "cat25k": 59}, {"ncat": 21, "s": 0.27738927738927743, "cat": 1, "x": 0.6130536130536131, "os": 0.12502677408550122, "ncat25k": 12, "term": "starting", "bg": 7.918139691145923e-07, "y": 0.4125874125874126, "cat25k": 7}, {"ncat": 21, "s": 0.27738927738927743, "cat": 1, "x": 0.6138306138306139, "os": 0.12502677408550122, "ncat25k": 12, "term": "values", "bg": 7.992701978005339e-07, "y": 0.4149184149184149, "cat25k": 7}, {"ncat": 21, "s": 0.8212898212898213, "cat": 3, "x": 0.6146076146076146, "os": 0.629846422345214, "ncat25k": 12, "term": "distribution", "bg": 1.0877483250111017e-06, "y": 0.7785547785547785, "cat25k": 22}, {"ncat": 21, "s": 0.27738927738927743, "cat": 1, "x": 0.6153846153846154, "os": 0.12502677408550122, "ncat25k": 12, "term": "day", "bg": 1.5234552082414732e-07, "y": 0.4188034188034188, "cat25k": 7}, {"ncat": 21, "s": 0.27738927738927743, "cat": 1, "x": 0.6161616161616161, "os": 0.12502677408550122, "ncat25k": 12, "term": "decision", "bg": 8.717157021277707e-07, "y": 0.42657342657342656, "cat25k": 7}, {"ncat": 21, "s": 0.8212898212898213, "cat": 3, "x": 0.616938616938617, "os": 0.629846422345214, "ncat25k": 12, "term": "technical", "bg": 5.635965560519028e-07, "y": 0.7839937839937839, "cat25k": 22}, {"ncat": 21, "s": 0.7055167055167055, "cat": 2, "x": 0.6177156177156177, "os": 0.5569765007453507, "ncat25k": 12, "term": "computational", "bg": 8.797199820961819e-06, "y": 0.679098679098679, "cat25k": 15}, {"ncat": 22, "s": 0.6907536907536908, "cat": 2, "x": 0.6184926184926185, "os": 0.5501403488368969, "ncat25k": 13, "term": "short", "bg": 5.052732137181341e-07, "y": 0.5104895104895105, "cat25k": 15}, {"ncat": 22, "s": 0.024087024087024088, "cat": 0, "x": 0.6192696192696193, "os": 0.010873089367411215, "ncat25k": 13, "term": "graphs", "bg": 1.2688395002129732e-05, "y": 0.00777000777000777, "cat25k": 0}, {"ncat": 22, "s": 0.22921522921522924, "cat": 1, "x": 0.62004662004662, "os": 0.11711330166386402, "ncat25k": 13, "term": "add", "bg": 1.6522551507924855e-07, "y": 0.2703962703962704, "cat25k": 7}, {"ncat": 22, "s": 0.6907536907536908, "cat": 2, "x": 0.6208236208236209, "os": 0.5501403488368969, "ncat25k": 13, "term": "brief", "bg": 2.017280698893622e-06, "y": 0.533022533022533, "cat25k": 15}, {"ncat": 22, "s": 0.024087024087024088, "cat": 0, "x": 0.6216006216006216, "os": 0.010873089367411215, "ncat25k": 13, "term": "public", "bg": 2.2323743356561304e-07, "y": 0.027195027195027196, "cat25k": 0}, {"ncat": 22, "s": 0.22921522921522924, "cat": 1, "x": 0.6223776223776224, "os": 0.11711330166386402, "ncat25k": 13, "term": "allowing", "bg": 2.77816977982027e-06, "y": 0.3014763014763015, "cat25k": 7}, {"ncat": 22, "s": 0.024087024087024088, "cat": 0, "x": 0.6231546231546231, "os": 0.010873089367411215, "ncat25k": 13, "term": "attendees", "bg": 1.5206732912089357e-05, "y": 0.048174048174048176, "cat25k": 0}, {"ncat": 22, "s": 0.22921522921522924, "cat": 1, "x": 0.6239316239316239, "os": 0.11711330166386402, "ncat25k": 13, "term": "word", "bg": 1.0729939785399989e-06, "y": 0.31934731934731936, "cat25k": 7}, {"ncat": 22, "s": 0.8065268065268066, "cat": 3, "x": 0.6247086247086248, "os": 0.6211627783592036, "ncat25k": 13, "term": "modelling", "bg": 1.5931682540484134e-05, "y": 0.7412587412587412, "cat25k": 22}, {"ncat": 22, "s": 0.6907536907536908, "cat": 2, "x": 0.6254856254856255, "os": 0.5501403488368969, "ncat25k": 13, "term": "nlp", "bg": 7.180033762114313e-05, "y": 0.5672105672105672, "cat25k": 15}, {"ncat": 22, "s": 0.8065268065268066, "cat": 3, "x": 0.6262626262626263, "os": 0.6211627783592036, "ncat25k": 13, "term": "type", "bg": 2.789456577926015e-07, "y": 0.7435897435897436, "cat25k": 22}, {"ncat": 22, "s": 0.8065268065268066, "cat": 3, "x": 0.627039627039627, "os": 0.6211627783592036, "ncat25k": 13, "term": "here", "bg": 8.43975725263858e-08, "y": 0.749028749028749, "cat25k": 22}, {"ncat": 22, "s": 0.024087024087024088, "cat": 0, "x": 0.6278166278166278, "os": 0.010873089367411215, "ncat25k": 13, "term": "operations", "bg": 1.0279239062624625e-06, "y": 0.08780108780108781, "cat25k": 0}, {"ncat": 22, "s": 0.22921522921522924, "cat": 1, "x": 0.6285936285936286, "os": 0.11711330166386402, "ncat25k": 13, "term": "product", "bg": 1.6030699149562142e-07, "y": 0.38306138306138304, "cat25k": 7}, {"ncat": 22, "s": 0.22921522921522924, "cat": 1, "x": 0.6293706293706294, "os": 0.11711330166386402, "ncat25k": 13, "term": "apis", "bg": 3.388904918947077e-05, "y": 0.39704739704739705, "cat25k": 7}, {"ncat": 22, "s": 0.22921522921522924, "cat": 1, "x": 0.6301476301476302, "os": 0.11711330166386402, "ncat25k": 13, "term": "exploration", "bg": 6.047360176099128e-06, "y": 0.40015540015540013, "cat25k": 7}, {"ncat": 22, "s": 0.6907536907536908, "cat": 2, "x": 0.6309246309246309, "os": 0.5501403488368969, "ncat25k": 13, "term": "class", "bg": 4.079382396567495e-07, "y": 0.6076146076146076, "cat25k": 15}, {"ncat": 22, "s": 0.024087024087024088, "cat": 0, "x": 0.6317016317016317, "os": 0.010873089367411215, "ncat25k": 13, "term": "group", "bg": 1.8014625683240012e-07, "y": 0.11499611499611499, "cat25k": 0}, {"ncat": 22, "s": 0.024087024087024088, "cat": 0, "x": 0.6324786324786325, "os": 0.010873089367411215, "ncat25k": 13, "term": "resources", "bg": 2.4840576057926896e-07, "y": 0.11965811965811966, "cat25k": 0}, {"ncat": 22, "s": 0.9254079254079255, "cat": 5, "x": 0.6332556332556333, "os": 0.7250617290340426, "ncat25k": 13, "term": "recently", "bg": 1.101160363927023e-06, "y": 0.8842268842268842, "cat25k": 37}, {"ncat": 22, "s": 0.22921522921522924, "cat": 1, "x": 0.634032634032634, "os": 0.11711330166386402, "ncat25k": 13, "term": "combine", "bg": 3.922540683913072e-06, "y": 0.4087024087024087, "cat25k": 7}, {"ncat": 22, "s": 0.6907536907536908, "cat": 2, "x": 0.6348096348096348, "os": 0.5501403488368969, "ncat25k": 13, "term": "practices", "bg": 2.0457205427716232e-06, "y": 0.6278166278166278, "cat25k": 15}, {"ncat": 22, "s": 0.6907536907536908, "cat": 2, "x": 0.6355866355866356, "os": 0.5501403488368969, "ncat25k": 13, "term": "too", "bg": 3.0645098736975876e-07, "y": 0.6285936285936286, "cat25k": 15}, {"ncat": 22, "s": 0.024087024087024088, "cat": 0, "x": 0.6363636363636364, "os": 0.010873089367411215, "ncat25k": 13, "term": "services", "bg": 1.2448329584705606e-07, "y": 0.15384615384615385, "cat25k": 0}, {"ncat": 22, "s": 0.22921522921522924, "cat": 1, "x": 0.6371406371406372, "os": 0.11711330166386402, "ncat25k": 13, "term": "companies", "bg": 5.582636181198734e-07, "y": 0.43434343434343436, "cat25k": 7}, {"ncat": 22, "s": 0.6907536907536908, "cat": 2, "x": 0.6379176379176379, "os": 0.5501403488368969, "ncat25k": 13, "term": "control", "bg": 3.245577673229787e-07, "y": 0.6487956487956488, "cat25k": 15}, {"ncat": 22, "s": 0.6907536907536908, "cat": 2, "x": 0.6386946386946387, "os": 0.5501403488368969, "ncat25k": 13, "term": "because", "bg": 2.431462031156511e-07, "y": 0.6534576534576535, "cat25k": 15}, {"ncat": 22, "s": 0.22921522921522924, "cat": 1, "x": 0.6394716394716394, "os": 0.11711330166386402, "ncat25k": 13, "term": "yet", "bg": 3.893633808948004e-07, "y": 0.46076146076146074, "cat25k": 7}, {"ncat": 22, "s": 0.6907536907536908, "cat": 2, "x": 0.6402486402486403, "os": 0.5501403488368969, "ncat25k": 13, "term": "output", "bg": 1.2871546117248056e-06, "y": 0.6573426573426573, "cat25k": 15}, {"ncat": 23, "s": 0.21756021756021757, "cat": 1, "x": 0.6410256410256411, "os": 0.11018732100230122, "ncat25k": 13, "term": "review", "bg": 1.6510144112625598e-07, "y": 0.25252525252525254, "cat25k": 7}, {"ncat": 23, "s": 0.8756798756798758, "cat": 4, "x": 0.6418026418026418, "os": 0.668609648383863, "ncat25k": 13, "term": "image", "bg": 8.384178408851631e-07, "y": 0.8003108003108003, "cat25k": 30}, {"ncat": 23, "s": 0.6511266511266511, "cat": 2, "x": 0.6425796425796426, "os": 0.5438028819885607, "ncat25k": 13, "term": "topics", "bg": 5.669293183706968e-07, "y": 0.5097125097125097, "cat25k": 15}, {"ncat": 23, "s": 0.6511266511266511, "cat": 2, "x": 0.6433566433566433, "os": 0.5438028819885607, "ncat25k": 13, "term": "structures", "bg": 3.4398193250535056e-06, "y": 0.5275835275835276, "cat25k": 15}, {"ncat": 23, "s": 0.21756021756021757, "cat": 1, "x": 0.6441336441336442, "os": 0.11018732100230122, "ncat25k": 13, "term": "object", "bg": 8.293190715618567e-07, "y": 0.2913752913752914, "cat25k": 7}, {"ncat": 23, "s": 0.797979797979798, "cat": 3, "x": 0.6449106449106449, "os": 0.6130440302496115, "ncat25k": 13, "term": "efficient", "bg": 3.032754966742809e-06, "y": 0.7249417249417249, "cat25k": 22}, {"ncat": 23, "s": 0.022533022533022532, "cat": 0, "x": 0.6456876456876457, "os": 0.008311685204090535, "ncat25k": 13, "term": "running", "bg": 9.041459654342531e-07, "y": 0.041181041181041184, "cat25k": 0}, {"ncat": 23, "s": 0.797979797979798, "cat": 3, "x": 0.6464646464646465, "os": 0.6130440302496115, "ncat25k": 13, "term": "interest", "bg": 5.316030363304824e-07, "y": 0.7381507381507382, "cat25k": 22}, {"ncat": 23, "s": 0.797979797979798, "cat": 3, "x": 0.6472416472416472, "os": 0.6130440302496115, "ncat25k": 13, "term": "aspects", "bg": 2.245980878631229e-06, "y": 0.7389277389277389, "cat25k": 22}, {"ncat": 23, "s": 0.21756021756021757, "cat": 1, "x": 0.6480186480186481, "os": 0.11018732100230122, "ncat25k": 13, "term": "doing", "bg": 7.412936909235258e-07, "y": 0.3139083139083139, "cat25k": 7}, {"ncat": 23, "s": 0.6511266511266511, "cat": 2, "x": 0.6487956487956488, "os": 0.5438028819885607, "ncat25k": 13, "term": "last", "bg": 1.3406133489927187e-07, "y": 0.5718725718725719, "cat25k": 15}, {"ncat": 23, "s": 0.21756021756021757, "cat": 1, "x": 0.6495726495726496, "os": 0.11018732100230122, "ncat25k": 13, "term": "languages", "bg": 1.8669824107319697e-06, "y": 0.32634032634032634, "cat25k": 7}, {"ncat": 23, "s": 0.21756021756021757, "cat": 1, "x": 0.6503496503496503, "os": 0.11018732100230122, "ncat25k": 13, "term": "move", "bg": 7.973712719546924e-07, "y": 0.33255633255633255, "cat25k": 7}, {"ncat": 23, "s": 0.022533022533022532, "cat": 0, "x": 0.6511266511266511, "os": 0.008311685204090535, "ncat25k": 13, "term": "options", "bg": 4.499377815875736e-07, "y": 0.07692307692307693, "cat25k": 0}, {"ncat": 23, "s": 0.6511266511266511, "cat": 2, "x": 0.6519036519036518, "os": 0.5438028819885607, "ncat25k": 13, "term": "amount", "bg": 6.727626632000147e-07, "y": 0.585081585081585, "cat25k": 15}, {"ncat": 23, "s": 0.6511266511266511, "cat": 2, "x": 0.6526806526806527, "os": 0.5438028819885607, "ncat25k": 13, "term": "extract", "bg": 5.7763083090119165e-06, "y": 0.5866355866355867, "cat25k": 15}, {"ncat": 23, "s": 0.6511266511266511, "cat": 2, "x": 0.6534576534576535, "os": 0.5438028819885607, "ncat25k": 13, "term": "local", "bg": 2.510516729948733e-07, "y": 0.5951825951825952, "cat25k": 15}, {"ncat": 23, "s": 0.6511266511266511, "cat": 2, "x": 0.6542346542346542, "os": 0.5438028819885607, "ncat25k": 13, "term": "second", "bg": 3.260984851665544e-07, "y": 0.5975135975135976, "cat25k": 15}, {"ncat": 23, "s": 0.6511266511266511, "cat": 2, "x": 0.655011655011655, "os": 0.5438028819885607, "ncat25k": 13, "term": "wide", "bg": 6.607494803888871e-07, "y": 0.6021756021756022, "cat25k": 15}, {"ncat": 23, "s": 0.9230769230769231, "cat": 5, "x": 0.6557886557886557, "os": 0.7154755888830898, "ncat25k": 13, "term": "workflow", "bg": 1.787525751542858e-05, "y": 0.8826728826728827, "cat25k": 37}, {"ncat": 23, "s": 0.797979797979798, "cat": 3, "x": 0.6565656565656566, "os": 0.6130440302496115, "ncat25k": 13, "term": "frameworks", "bg": 2.6743048129090558e-05, "y": 0.7746697746697747, "cat25k": 22}, {"ncat": 23, "s": 0.797979797979798, "cat": 3, "x": 0.6573426573426573, "os": 0.6130440302496115, "ncat25k": 13, "term": "developing", "bg": 1.4456626924777458e-06, "y": 0.7902097902097902, "cat25k": 22}, {"ncat": 23, "s": 0.21756021756021757, "cat": 1, "x": 0.6581196581196581, "os": 0.11018732100230122, "ncat25k": 13, "term": "others", "bg": 5.739120219772434e-07, "y": 0.4560994560994561, "cat25k": 7}, {"ncat": 24, "s": 0.8741258741258742, "cat": 4, "x": 0.6588966588966589, "os": 0.6599980965950667, "ncat25k": 14, "term": "2", "bg": 0.0, "y": 0.8166278166278166, "cat25k": 30}, {"ncat": 24, "s": 0.2105672105672106, "cat": 1, "x": 0.6596736596736597, "os": 0.10410675516745482, "ncat25k": 14, "term": "cloud", "bg": 1.0126686595287494e-05, "y": 0.29215229215229216, "cat25k": 7}, {"ncat": 24, "s": 0.5268065268065268, "cat": 2, "x": 0.6604506604506605, "os": 0.20413242013802935, "ncat25k": 14, "term": "three", "bg": 2.863085933332239e-07, "y": 0.5547785547785548, "cat25k": 15}, {"ncat": 24, "s": 0.5268065268065268, "cat": 2, "x": 0.6612276612276612, "os": 0.20413242013802935, "ncat25k": 14, "term": "relevant", "bg": 1.2786666029390988e-06, "y": 0.5742035742035742, "cat25k": 15}, {"ncat": 24, "s": 0.2105672105672106, "cat": 1, "x": 0.662004662004662, "os": 0.10410675516745482, "ncat25k": 14, "term": "still", "bg": 3.14876132373947e-07, "y": 0.3341103341103341, "cat25k": 7}, {"ncat": 24, "s": 0.5268065268065268, "cat": 2, "x": 0.6627816627816627, "os": 0.20413242013802935, "ncat25k": 14, "term": "means", "bg": 4.90268675908382e-07, "y": 0.5811965811965812, "cat25k": 15}, {"ncat": 24, "s": 0.2105672105672106, "cat": 1, "x": 0.6635586635586636, "os": 0.10410675516745482, "ncat25k": 14, "term": "ipython", "bg": 0.000294079980268182, "y": 0.3473193473193473, "cat25k": 7}, {"ncat": 24, "s": 0.021756021756021753, "cat": 0, "x": 0.6643356643356644, "os": 0.006293279743645852, "ncat25k": 14, "term": "social", "bg": 7.588555290271116e-07, "y": 0.08236208236208237, "cat25k": 0}, {"ncat": 24, "s": 0.2105672105672106, "cat": 1, "x": 0.6651126651126651, "os": 0.10410675516745482, "ncat25k": 14, "term": "beyond", "bg": 1.13845094016441e-06, "y": 0.3504273504273504, "cat25k": 7}, {"ncat": 24, "s": 0.9728049728049728, "cat": 7, "x": 0.6658896658896659, "os": 0.7792322217079312, "ncat25k": 14, "term": "method", "bg": 1.1831914777462863e-06, "y": 0.9378399378399378, "cat25k": 52}, {"ncat": 24, "s": 0.7824397824397824, "cat": 3, "x": 0.6666666666666666, "os": 0.6054393324307953, "ncat25k": 14, "term": "under", "bg": 1.9144004247033529e-07, "y": 0.7684537684537684, "cat25k": 22}, {"ncat": 24, "s": 0.5268065268065268, "cat": 2, "x": 0.6674436674436675, "os": 0.20413242013802935, "ncat25k": 14, "term": "testing", "bg": 1.6583848573513803e-06, "y": 0.6091686091686092, "cat25k": 15}, {"ncat": 24, "s": 0.7824397824397824, "cat": 3, "x": 0.6682206682206682, "os": 0.6054393324307953, "ncat25k": 14, "term": "called", "bg": 7.096450624315846e-07, "y": 0.7894327894327894, "cat25k": 22}, {"ncat": 24, "s": 0.5268065268065268, "cat": 2, "x": 0.668997668997669, "os": 0.20413242013802935, "ncat25k": 14, "term": "robust", "bg": 9.615860124951234e-06, "y": 0.6635586635586636, "cat25k": 15}, {"ncat": 24, "s": 0.5268065268065268, "cat": 2, "x": 0.6697746697746698, "os": 0.20413242013802935, "ncat25k": 14, "term": "predict", "bg": 1.3521096290487233e-05, "y": 0.6682206682206682, "cat25k": 15}, {"ncat": 24, "s": 0.5268065268065268, "cat": 2, "x": 0.6705516705516705, "os": 0.20413242013802935, "ncat25k": 14, "term": "lessons", "bg": 3.406607718377312e-06, "y": 0.6775446775446775, "cat25k": 15}, {"ncat": 25, "s": 0.1794871794871795, "cat": 1, "x": 0.6713286713286714, "os": 0.09874730972297435, "ncat25k": 14, "term": "existing", "bg": 1.1880371582721432e-06, "y": 0.2556332556332556, "cat25k": 7}, {"ncat": 25, "s": 0.1794871794871795, "cat": 1, "x": 0.6721056721056721, "os": 0.09874730972297435, "ncat25k": 14, "term": "writing", "bg": 6.964093494155865e-07, "y": 0.2641802641802642, "cat25k": 7}, {"ncat": 25, "s": 0.1794871794871795, "cat": 1, "x": 0.6728826728826729, "os": 0.09874730972297435, "ncat25k": 14, "term": "architecture", "bg": 1.4906690250012362e-06, "y": 0.2851592851592852, "cat25k": 7}, {"ncat": 25, "s": 0.77000777000777, "cat": 3, "x": 0.6736596736596736, "os": 0.5983034008486467, "ncat25k": 14, "term": "'ve", "bg": 0.0, "y": 0.7288267288267288, "cat25k": 22}, {"ncat": 25, "s": 0.0202020202020202, "cat": 0, "x": 0.6744366744366744, "os": 0.00471897889610251, "ncat25k": 14, "term": "rather", "bg": 8.132002348630705e-07, "y": 0.055944055944055944, "cat25k": 0}, {"ncat": 25, "s": 0.5042735042735043, "cat": 2, "x": 0.6752136752136753, "os": 0.1948896128369308, "ncat25k": 14, "term": "created", "bg": 9.45907480352774e-07, "y": 0.6107226107226107, "cat25k": 15}, {"ncat": 25, "s": 0.77000777000777, "cat": 3, "x": 0.675990675990676, "os": 0.5983034008486467, "ncat25k": 14, "term": "human", "bg": 5.849721925689378e-07, "y": 0.7777777777777778, "cat25k": 22}, {"ncat": 25, "s": 0.0202020202020202, "cat": 0, "x": 0.6767676767676768, "os": 0.00471897889610251, "ncat25k": 14, "term": "needs", "bg": 4.6882549328181554e-07, "y": 0.1686091686091686, "cat25k": 0}, {"ncat": 25, "s": 0.5042735042735043, "cat": 2, "x": 0.6775446775446775, "os": 0.1948896128369308, "ncat25k": 14, "term": "search", "bg": 1.718396235243797e-07, "y": 0.6402486402486403, "cat25k": 15}, {"ncat": 26, "s": 0.49572649572649574, "cat": 2, "x": 0.6783216783216783, "os": 0.18651880040023716, "ncat25k": 15, "term": "content", "bg": 4.021203658742943e-07, "y": 0.5128205128205128, "cat25k": 15}, {"ncat": 26, "s": 0.7622377622377622, "cat": 3, "x": 0.679098679098679, "os": 0.5915958386898541, "ncat25k": 15, "term": "parallel", "bg": 5.872753739666264e-06, "y": 0.7125097125097125, "cat25k": 22}, {"ncat": 26, "s": 0.7622377622377622, "cat": 3, "x": 0.6798756798756799, "os": 0.5915958386898541, "ncat25k": 15, "term": "done", "bg": 6.606414482597374e-07, "y": 0.7171717171717171, "cat25k": 22}, {"ncat": 26, "s": 0.016317016317016316, "cat": 0, "x": 0.6806526806526807, "os": 0.0035038100271824746, "ncat25k": 15, "term": "session", "bg": 1.7372059756266382e-06, "y": 0.017871017871017872, "cat25k": 0}, {"ncat": 26, "s": 0.016317016317016316, "cat": 0, "x": 0.6814296814296814, "os": 0.0035038100271824746, "ncat25k": 15, "term": "before", "bg": 2.088858213245715e-07, "y": 0.022533022533022532, "cat25k": 0}, {"ncat": 26, "s": 0.8966588966588968, "cat": 5, "x": 0.6822066822066822, "os": 0.6894114847756881, "ncat25k": 15, "term": "1", "bg": 0.0, "y": 0.864024864024864, "cat25k": 37}, {"ncat": 26, "s": 0.49572649572649574, "cat": 2, "x": 0.682983682983683, "os": 0.18651880040023716, "ncat25k": 15, "term": "space", "bg": 5.591052021145984e-07, "y": 0.5431235431235432, "cat25k": 15}, {"ncat": 26, "s": 0.49572649572649574, "cat": 2, "x": 0.6837606837606838, "os": 0.18651880040023716, "ncat25k": 15, "term": "detection", "bg": 6.448934423897171e-06, "y": 0.5462315462315462, "cat25k": 15}, {"ncat": 26, "s": 0.7622377622377622, "cat": 3, "x": 0.6845376845376845, "os": 0.5915958386898541, "ncat25k": 15, "term": "started", "bg": 8.75916395325801e-07, "y": 0.7272727272727273, "cat25k": 22}, {"ncat": 26, "s": 0.7622377622377622, "cat": 3, "x": 0.6853146853146853, "os": 0.5915958386898541, "ncat25k": 15, "term": "going", "bg": 4.504454648249711e-07, "y": 0.7296037296037297, "cat25k": 22}, {"ncat": 26, "s": 0.016317016317016316, "cat": 0, "x": 0.6860916860916861, "os": 0.0035038100271824746, "ncat25k": 15, "term": "study", "bg": 5.225479506002502e-07, "y": 0.04584304584304584, "cat25k": 0}, {"ncat": 26, "s": 0.8484848484848485, "cat": 4, "x": 0.6868686868686869, "os": 0.6441578275406832, "ncat25k": 15, "term": "apply", "bg": 1.0463107332046005e-06, "y": 0.8298368298368298, "cat25k": 30}, {"ncat": 26, "s": 0.016317016317016316, "cat": 0, "x": 0.6876456876456877, "os": 0.0035038100271824746, "ncat25k": 15, "term": "right", "bg": 2.7033148583803306e-07, "y": 0.0644910644910645, "cat25k": 0}, {"ncat": 26, "s": 0.8484848484848485, "cat": 4, "x": 0.6884226884226884, "os": 0.6441578275406832, "ncat25k": 15, "term": "write", "bg": 5.364350258970714e-07, "y": 0.8344988344988346, "cat25k": 30}, {"ncat": 26, "s": 0.7622377622377622, "cat": 3, "x": 0.6891996891996892, "os": 0.5915958386898541, "ncat25k": 15, "term": "custom", "bg": 1.2591369497605257e-06, "y": 0.7583527583527584, "cat25k": 22}, {"ncat": 26, "s": 0.49572649572649574, "cat": 2, "x": 0.6899766899766899, "os": 0.18651880040023716, "ncat25k": 15, "term": "scientist", "bg": 1.0323108714722496e-05, "y": 0.5889665889665889, "cat25k": 15}, {"ncat": 26, "s": 0.016317016317016316, "cat": 0, "x": 0.6907536907536908, "os": 0.0035038100271824746, "ncat25k": 15, "term": "\u2019s", "bg": 0.0, "y": 0.10256410256410256, "cat25k": 0}, {"ncat": 26, "s": 0.49572649572649574, "cat": 2, "x": 0.6915306915306916, "os": 0.18651880040023716, "ncat25k": 15, "term": "service", "bg": 2.0783024302590982e-07, "y": 0.6363636363636364, "cat25k": 15}, {"ncat": 26, "s": 0.1724941724941725, "cat": 1, "x": 0.6923076923076923, "os": 0.0940011155163572, "ncat25k": 15, "term": "potential", "bg": 9.400618961012056e-07, "y": 0.48795648795648794, "cat25k": 7}, {"ncat": 27, "s": 0.46464646464646464, "cat": 2, "x": 0.6930846930846931, "os": 0.1789062818989126, "ncat25k": 16, "term": "stack", "bg": 8.029815239304555e-06, "y": 0.5135975135975136, "cat25k": 15}, {"ncat": 27, "s": 0.46464646464646464, "cat": 2, "x": 0.6938616938616938, "os": 0.1789062818989126, "ncat25k": 16, "term": "provided", "bg": 4.12748975341288e-07, "y": 0.5213675213675214, "cat25k": 15}, {"ncat": 27, "s": 0.46464646464646464, "cat": 2, "x": 0.6946386946386947, "os": 0.1789062818989126, "ncat25k": 16, "term": "core", "bg": 1.5186270116551118e-06, "y": 0.526029526029526, "cat25k": 15}, {"ncat": 27, "s": 0.7466977466977467, "cat": 3, "x": 0.6954156954156954, "os": 0.5852805430716669, "ncat25k": 16, "term": "computing", "bg": 2.575431935553268e-06, "y": 0.7117327117327117, "cat25k": 22}, {"ncat": 27, "s": 0.8298368298368298, "cat": 4, "x": 0.6961926961926962, "os": 0.6368623239647113, "ncat25k": 16, "term": "regression", "bg": 1.6480209678018683e-05, "y": 0.8212898212898213, "cat25k": 30}, {"ncat": 27, "s": 0.1693861693861694, "cat": 1, "x": 0.696969696969697, "os": 0.08977533295782264, "ncat25k": 16, "term": "basics", "bg": 4.825336102754567e-06, "y": 0.2804972804972805, "cat25k": 7}, {"ncat": 27, "s": 0.8896658896658898, "cat": 5, "x": 0.6977466977466977, "os": 0.6815333381856084, "ncat25k": 16, "term": "power", "bg": 3.616891559323243e-07, "y": 0.8655788655788655, "cat25k": 37}, {"ncat": 27, "s": 0.7466977466977467, "cat": 3, "x": 0.6985236985236986, "os": 0.5852805430716669, "ncat25k": 16, "term": "implemented", "bg": 4.252721171062494e-06, "y": 0.7241647241647242, "cat25k": 22}, {"ncat": 27, "s": 0.1693861693861694, "cat": 1, "x": 0.6993006993006993, "os": 0.08977533295782264, "ncat25k": 16, "term": "patterns", "bg": 3.8569131504153705e-06, "y": 0.29448329448329447, "cat25k": 7}, {"ncat": 27, "s": 0.014763014763014762, "cat": 0, "x": 0.7000777000777001, "os": 0.002575727298108399, "ncat25k": 16, "term": "identify", "bg": 2.2795657171552664e-06, "y": 0.06682206682206682, "cat25k": 0}, {"ncat": 27, "s": 0.014763014763014762, "cat": 0, "x": 0.7008547008547008, "os": 0.002575727298108399, "ncat25k": 16, "term": "goal", "bg": 1.5743486011113928e-06, "y": 0.09867909867909867, "cat25k": 0}, {"ncat": 27, "s": 0.46464646464646464, "cat": 2, "x": 0.7016317016317016, "os": 0.1789062818989126, "ncat25k": 16, "term": "now", "bg": 1.0793036809506496e-07, "y": 0.5905205905205905, "cat25k": 15}, {"ncat": 27, "s": 0.8896658896658898, "cat": 5, "x": 0.7024087024087025, "os": 0.6815333381856084, "ncat25k": 16, "term": "fit", "bg": 1.7899012078159145e-06, "y": 0.8803418803418803, "cat25k": 37}, {"ncat": 27, "s": 0.46464646464646464, "cat": 2, "x": 0.7031857031857032, "os": 0.1789062818989126, "ncat25k": 16, "term": "general", "bg": 2.4368703485736216e-07, "y": 0.6052836052836053, "cat25k": 15}, {"ncat": 27, "s": 0.7466977466977467, "cat": 3, "x": 0.703962703962704, "os": 0.5852805430716669, "ncat25k": 16, "term": "behind", "bg": 1.3718292345485776e-06, "y": 0.7723387723387724, "cat25k": 22}, {"ncat": 27, "s": 0.46464646464646464, "cat": 2, "x": 0.7047397047397047, "os": 0.1789062818989126, "ncat25k": 16, "term": "functions", "bg": 1.7329661986675915e-06, "y": 0.6208236208236209, "cat25k": 15}, {"ncat": 27, "s": 0.46464646464646464, "cat": 2, "x": 0.7055167055167055, "os": 0.1789062818989126, "ncat25k": 16, "term": "technology", "bg": 3.0151771959334504e-07, "y": 0.668997668997669, "cat25k": 15}, {"ncat": 28, "s": 0.1662781662781663, "cat": 1, "x": 0.7062937062937062, "os": 0.085990719088538, "ncat25k": 16, "term": "structure", "bg": 1.7828177110043873e-06, "y": 0.2742812742812743, "cat25k": 7}, {"ncat": 28, "s": 0.8236208236208237, "cat": 4, "x": 0.7070707070707071, "os": 0.6299433893140561, "ncat25k": 16, "term": "modeling", "bg": 6.798283574493892e-06, "y": 0.8197358197358198, "cat25k": 30}, {"ncat": 28, "s": 0.01320901320901321, "cat": 0, "x": 0.7078477078477079, "os": 0.0018744660588241313, "ncat25k": 16, "term": "decisions", "bg": 2.353778433047075e-06, "y": 0.031857031857031856, "cat25k": 0}, {"ncat": 28, "s": 0.4506604506604507, "cat": 2, "x": 0.7086247086247086, "os": 0.17195372413203386, "ncat25k": 16, "term": "found", "bg": 3.360279660050157e-07, "y": 0.5501165501165501, "cat25k": 15}, {"ncat": 28, "s": 0.4506604506604507, "cat": 2, "x": 0.7094017094017094, "os": 0.17195372413203386, "ncat25k": 16, "term": "change", "bg": 3.9863535727622715e-07, "y": 0.560994560994561, "cat25k": 15}, {"ncat": 28, "s": 0.7381507381507382, "cat": 3, "x": 0.7101787101787101, "os": 0.5793251857072753, "ncat25k": 16, "term": "presentation", "bg": 2.506342045244966e-06, "y": 0.7521367521367521, "cat25k": 22}, {"ncat": 28, "s": 0.9821289821289821, "cat": 9, "x": 0.710955710955711, "os": 0.8000363791733194, "ncat25k": 16, "term": "state", "bg": 2.4711962604146e-07, "y": 0.9557109557109557, "cat25k": 67}, {"ncat": 28, "s": 0.4506604506604507, "cat": 2, "x": 0.7117327117327117, "os": 0.17195372413203386, "ncat25k": 16, "term": "quality", "bg": 4.324274656452463e-07, "y": 0.5920745920745921, "cat25k": 15}, {"ncat": 28, "s": 0.1662781662781663, "cat": 1, "x": 0.7125097125097125, "os": 0.085990719088538, "ncat25k": 16, "term": "sources", "bg": 1.1487753233359778e-06, "y": 0.40326340326340326, "cat25k": 7}, {"ncat": 28, "s": 0.4506604506604507, "cat": 2, "x": 0.7132867132867133, "os": 0.17195372413203386, "ncat25k": 16, "term": "cluster", "bg": 7.707496974131341e-06, "y": 0.616938616938617, "cat25k": 15}, {"ncat": 28, "s": 0.01320901320901321, "cat": 0, "x": 0.714063714063714, "os": 0.0018744660588241313, "ncat25k": 16, "term": "developers", "bg": 3.308784222393314e-06, "y": 0.17560217560217561, "cat25k": 0}, {"ncat": 29, "s": 0.8096348096348096, "cat": 4, "x": 0.7148407148407149, "os": 0.6233742958031785, "ncat25k": 17, "term": "natural", "bg": 9.884106287710538e-07, "y": 0.8026418026418026, "cat25k": 30}, {"ncat": 29, "s": 0.4312354312354313, "cat": 2, "x": 0.7156177156177156, "os": 0.16557651728089323, "ncat25k": 17, "term": "perform", "bg": 2.4265011967733437e-06, "y": 0.519036519036519, "cat25k": 15}, {"ncat": 29, "s": 0.010878010878010878, "cat": 0, "x": 0.7163947163947164, "os": 0.0013503016207921381, "ncat25k": 17, "term": "during", "bg": 3.4869844793110067e-07, "y": 0.023310023310023312, "cat25k": 0}, {"ncat": 29, "s": 0.9114219114219115, "cat": 6, "x": 0.7171717171717171, "os": 0.7048805191763442, "ncat25k": 17, "term": "since", "bg": 3.8242599400595754e-07, "y": 0.9075369075369075, "cat25k": 45}, {"ncat": 29, "s": 0.16239316239316243, "cat": 1, "x": 0.717948717948718, "os": 0.08258017693037423, "ncat25k": 17, "term": "management", "bg": 2.563100915723665e-07, "y": 0.30225330225330227, "cat25k": 7}, {"ncat": 29, "s": 0.7311577311577312, "cat": 3, "x": 0.7187257187257188, "os": 0.5737007597261946, "ncat25k": 17, "term": "advanced", "bg": 4.609893390164868e-07, "y": 0.7319347319347319, "cat25k": 22}, {"ncat": 29, "s": 0.010878010878010878, "cat": 0, "x": 0.7195027195027195, "os": 0.0013503016207921381, "ncat25k": 17, "term": "ecosystem", "bg": 1.7341545330482267e-05, "y": 0.07303807303807304, "cat25k": 0}, {"ncat": 29, "s": 0.8096348096348096, "cat": 4, "x": 0.7202797202797203, "os": 0.6233742958031785, "ncat25k": 17, "term": "images", "bg": 1.755837179743419e-06, "y": 0.8376068376068376, "cat25k": 30}, {"ncat": 29, "s": 0.16239316239316243, "cat": 1, "x": 0.721056721056721, "os": 0.08258017693037423, "ncat25k": 17, "term": "pipelines", "bg": 5.4372255900210046e-05, "y": 0.35742035742035744, "cat25k": 7}, {"ncat": 29, "s": 0.4312354312354313, "cat": 2, "x": 0.7218337218337219, "os": 0.16557651728089323, "ncat25k": 17, "term": "coming", "bg": 1.5141391672448843e-06, "y": 0.5835275835275835, "cat25k": 15}, {"ncat": 29, "s": 0.7311577311577312, "cat": 3, "x": 0.7226107226107226, "os": 0.5737007597261946, "ncat25k": 17, "term": "could", "bg": 2.57911711226547e-07, "y": 0.7622377622377622, "cat25k": 22}, {"ncat": 29, "s": 0.4312354312354313, "cat": 2, "x": 0.7233877233877234, "os": 0.16557651728089323, "ncat25k": 17, "term": "access", "bg": 3.851362531827041e-07, "y": 0.5912975912975913, "cat25k": 15}, {"ncat": 29, "s": 0.8096348096348096, "cat": 4, "x": 0.7241647241647242, "os": 0.6233742958031785, "ncat25k": 17, "term": "improve", "bg": 1.4001210388357596e-06, "y": 0.8438228438228438, "cat25k": 30}, {"ncat": 29, "s": 0.010878010878010878, "cat": 0, "x": 0.7249417249417249, "os": 0.0013503016207921381, "ncat25k": 17, "term": "full", "bg": 2.3222299210236374e-07, "y": 0.13986013986013987, "cat25k": 0}, {"ncat": 30, "s": 0.010101010101010102, "cat": 0, "x": 0.7257187257187258, "os": 0.0009627675310867057, "ncat25k": 17, "term": "interface", "bg": 1.245255722938679e-06, "y": 0.000777000777000777, "cat25k": 0}, {"ncat": 30, "s": 0.13364413364413366, "cat": 1, "x": 0.7264957264957265, "os": 0.07948731648413004, "ncat25k": 17, "term": "hands", "bg": 1.5851928356600102e-06, "y": 0.26961926961926963, "cat25k": 7}, {"ncat": 30, "s": 0.38850038850038854, "cat": 2, "x": 0.7272727272727273, "os": 0.15970218888964394, "ncat25k": 17, "term": "sets", "bg": 1.911304808201245e-06, "y": 0.5376845376845377, "cat25k": 15}, {"ncat": 30, "s": 0.13364413364413366, "cat": 1, "x": 0.728049728049728, "os": 0.07948731648413004, "ncat25k": 17, "term": "issues", "bg": 4.925979994596732e-07, "y": 0.3076923076923077, "cat25k": 7}, {"ncat": 30, "s": 0.13364413364413366, "cat": 1, "x": 0.7288267288267288, "os": 0.07948731648413004, "ncat25k": 17, "term": "were", "bg": 1.3314244910629883e-07, "y": 0.3123543123543124, "cat25k": 7}, {"ncat": 30, "s": 0.13364413364413366, "cat": 1, "x": 0.7296037296037297, "os": 0.07948731648413004, "ncat25k": 17, "term": "already", "bg": 7.561631213528386e-07, "y": 0.32323232323232326, "cat25k": 7}, {"ncat": 30, "s": 0.38850038850038854, "cat": 2, "x": 0.7303807303807304, "os": 0.15970218888964394, "ncat25k": 17, "term": "explain", "bg": 3.266729239138922e-06, "y": 0.5703185703185704, "cat25k": 15}, {"ncat": 30, "s": 0.13364413364413366, "cat": 1, "x": 0.7311577311577312, "os": 0.07948731648413004, "ncat25k": 17, "term": "walk", "bg": 1.987412645570333e-06, "y": 0.358974358974359, "cat25k": 7}, {"ncat": 30, "s": 0.7202797202797203, "cat": 3, "x": 0.7319347319347319, "os": 0.5683811848617681, "ncat25k": 17, "term": "variety", "bg": 1.9614811536487433e-06, "y": 0.763014763014763, "cat25k": 22}, {"ncat": 30, "s": 0.8041958041958042, "cat": 4, "x": 0.7327117327117327, "os": 0.6171305820478379, "ncat25k": 17, "term": "applied", "bg": 1.8190751010257742e-06, "y": 0.85003885003885, "cat25k": 30}, {"ncat": 30, "s": 0.38850038850038854, "cat": 2, "x": 0.7334887334887334, "os": 0.15970218888964394, "ncat25k": 17, "term": "driven", "bg": 6.277546237210762e-06, "y": 0.6480186480186481, "cat25k": 15}, {"ncat": 30, "s": 0.8749028749028749, "cat": 5, "x": 0.7342657342657343, "os": 0.6599986249676055, "ncat25k": 17, "term": "approaches", "bg": 6.167522837144291e-06, "y": 0.8927738927738927, "cat25k": 37}, {"ncat": 31, "s": 0.00777000777000777, "cat": 0, "x": 0.7350427350427351, "os": 0.0006793840559164432, "ncat25k": 18, "term": "concepts", "bg": 3.833087173636583e-06, "y": 0.010101010101010102, "cat25k": 0}, {"ncat": 31, "s": 0.128982128982129, "cat": 1, "x": 0.7358197358197358, "os": 0.07666505984081057, "ncat25k": 18, "term": "github", "bg": 0.0007310003655001827, "y": 0.27117327117327117, "cat25k": 7}, {"ncat": 31, "s": 0.128982128982129, "cat": 1, "x": 0.7365967365967366, "os": 0.07666505984081057, "ncat25k": 18, "term": "easily", "bg": 1.9968177855989642e-06, "y": 0.27583527583527584, "cat25k": 7}, {"ncat": 31, "s": 0.7917637917637919, "cat": 4, "x": 0.7373737373737373, "os": 0.6111898498046111, "ncat25k": 18, "term": "implement", "bg": 4.318342654635892e-06, "y": 0.8181818181818182, "cat25k": 30}, {"ncat": 31, "s": 0.7163947163947164, "cat": 3, "x": 0.7381507381507382, "os": 0.5633429636878199, "ncat25k": 18, "term": "main", "bg": 4.005319103823068e-07, "y": 0.7202797202797203, "cat25k": 22}, {"ncat": 31, "s": 0.128982128982129, "cat": 1, "x": 0.7389277389277389, "os": 0.07666505984081057, "ncat25k": 18, "term": "solutions", "bg": 8.661157354682961e-07, "y": 0.29914529914529914, "cat25k": 7}, {"ncat": 31, "s": 0.00777000777000777, "cat": 0, "x": 0.7397047397047397, "os": 0.0006793840559164432, "ncat25k": 18, "term": "analyze", "bg": 1.0380971498704241e-05, "y": 0.05439005439005439, "cat25k": 0}, {"ncat": 31, "s": 0.128982128982129, "cat": 1, "x": 0.7404817404817405, "os": 0.07666505984081057, "ncat25k": 18, "term": "scientific", "bg": 2.0219547609722664e-06, "y": 0.32867132867132864, "cat25k": 7}, {"ncat": 31, "s": 0.128982128982129, "cat": 1, "x": 0.7412587412587412, "os": 0.07666505984081057, "ncat25k": 18, "term": "areas", "bg": 6.551793471155823e-07, "y": 0.35586635586635584, "cat25k": 7}, {"ncat": 31, "s": 0.00777000777000777, "cat": 0, "x": 0.7420357420357421, "os": 0.0006793840559164432, "ncat25k": 18, "term": "next", "bg": 1.8778417423098287e-07, "y": 0.08935508935508936, "cat25k": 0}, {"ncat": 31, "s": 0.7917637917637919, "cat": 4, "x": 0.7428127428127428, "os": 0.6111898498046111, "ncat25k": 18, "term": "given", "bg": 7.00298981509035e-07, "y": 0.8407148407148407, "cat25k": 30}, {"ncat": 31, "s": 0.8578088578088578, "cat": 5, "x": 0.7435897435897436, "os": 0.6534513418801298, "ncat25k": 18, "term": "implementation", "bg": 2.3388176694102913e-06, "y": 0.8811188811188811, "cat25k": 37}, {"ncat": 31, "s": 0.7917637917637919, "cat": 4, "x": 0.7443667443667443, "os": 0.6111898498046111, "ncat25k": 18, "term": "spark", "bg": 4.4353613619946386e-05, "y": 0.8461538461538461, "cat25k": 30}, {"ncat": 31, "s": 0.37140637140637145, "cat": 2, "x": 0.7451437451437452, "os": 0.15426889252743364, "ncat25k": 18, "term": "lot", "bg": 7.322372548966044e-07, "y": 0.6223776223776224, "cat25k": 15}, {"ncat": 31, "s": 0.128982128982129, "cat": 1, "x": 0.745920745920746, "os": 0.07666505984081057, "ncat25k": 18, "term": "team", "bg": 6.519000251941898e-07, "y": 0.42424242424242425, "cat25k": 7}, {"ncat": 32, "s": 0.85003885003885, "cat": 5, "x": 0.7466977466977467, "os": 0.6471884381516964, "ncat25k": 18, "term": "distributed", "bg": 4.536984272933915e-06, "y": 0.8601398601398601, "cat25k": 37}, {"ncat": 32, "s": 0.12587412587412591, "cat": 1, "x": 0.7474747474747475, "os": 0.07407432085150151, "ncat25k": 18, "term": "quickly", "bg": 2.3833635267424557e-06, "y": 0.3333333333333333, "cat25k": 7}, {"ncat": 32, "s": 0.85003885003885, "cat": 5, "x": 0.7482517482517482, "os": 0.6471884381516964, "ncat25k": 18, "term": "package", "bg": 1.7061622212164353e-06, "y": 0.8749028749028749, "cat25k": 37}, {"ncat": 32, "s": 0.8943278943278944, "cat": 6, "x": 0.749028749028749, "os": 0.6841177605741742, "ncat25k": 18, "term": "knowledge", "bg": 1.2685412887934391e-06, "y": 0.9191919191919192, "cat25k": 45}, {"ncat": 32, "s": 0.12587412587412591, "cat": 1, "x": 0.7498057498057498, "os": 0.07407432085150151, "ncat25k": 18, "term": "hard", "bg": 5.992055870604965e-07, "y": 0.36363636363636365, "cat25k": 7}, {"ncat": 32, "s": 0.12587412587412591, "cat": 1, "x": 0.7505827505827506, "os": 0.07407432085150151, "ncat25k": 18, "term": "develop", "bg": 1.3637502759351298e-06, "y": 0.36596736596736595, "cat25k": 7}, {"ncat": 32, "s": 0.7839937839937839, "cat": 4, "x": 0.7513597513597513, "os": 0.6055315758349641, "ncat25k": 18, "term": "learned", "bg": 4.905550087731891e-06, "y": 0.8477078477078477, "cat25k": 30}, {"ncat": 32, "s": 0.006993006993006993, "cat": 0, "x": 0.7521367521367521, "os": 0.00047443992174667216, "ncat25k": 18, "term": "database", "bg": 1.1940378071657444e-06, "y": 0.1445221445221445, "cat25k": 0}, {"ncat": 32, "s": 0.365967365967366, "cat": 2, "x": 0.752913752913753, "os": 0.14922399025290156, "ncat25k": 18, "term": "future", "bg": 8.363258904866011e-07, "y": 0.6355866355866356, "cat25k": 15}, {"ncat": 32, "s": 0.85003885003885, "cat": 5, "x": 0.7536907536907537, "os": 0.6471884381516964, "ncat25k": 18, "term": "practical", "bg": 3.5547571024217808e-06, "y": 0.8904428904428905, "cat25k": 37}, {"ncat": 32, "s": 0.365967365967366, "cat": 2, "x": 0.7544677544677545, "os": 0.14922399025290156, "ncat25k": 18, "term": "non", "bg": 3.873061647945229e-07, "y": 0.655011655011655, "cat25k": 15}, {"ncat": 33, "s": 0.7731157731157732, "cat": 4, "x": 0.7552447552447552, "os": 0.6001369392609306, "ncat25k": 19, "term": "environment", "bg": 1.0384322705720315e-06, "y": 0.811965811965812, "cat25k": 30}, {"ncat": 33, "s": 0.7731157731157732, "cat": 4, "x": 0.756021756021756, "os": 0.6001369392609306, "ncat25k": 19, "term": "standard", "bg": 6.110192805708273e-07, "y": 0.822066822066822, "cat25k": 30}, {"ncat": 33, "s": 0.8477078477078478, "cat": 5, "x": 0.7567987567987567, "os": 0.6411931356330538, "ncat25k": 19, "term": "describe", "bg": 4.204702547414995e-06, "y": 0.8694638694638694, "cat25k": 37}, {"ncat": 33, "s": 0.7016317016317017, "cat": 3, "x": 0.7575757575757576, "os": 0.5540277492136181, "ncat25k": 19, "term": "solution", "bg": 1.295227715941073e-06, "y": 0.7303807303807304, "cat25k": 22}, {"ncat": 33, "s": 0.7731157731157732, "cat": 4, "x": 0.7583527583527584, "os": 0.6001369392609306, "ncat25k": 19, "term": "fast", "bg": 1.169477102653649e-06, "y": 0.8360528360528361, "cat25k": 30}, {"ncat": 33, "s": 0.12354312354312354, "cat": 1, "x": 0.7591297591297591, "os": 0.07168278435572467, "ncat25k": 19, "term": "audience", "bg": 3.253315067053671e-06, "y": 0.37839937839937837, "cat25k": 7}, {"ncat": 33, "s": 0.358974358974359, "cat": 2, "x": 0.7599067599067599, "os": 0.14452274625402695, "ncat25k": 19, "term": "following", "bg": 3.532167686599709e-07, "y": 0.6161616161616161, "cat25k": 15}, {"ncat": 33, "s": 0.12354312354312354, "cat": 1, "x": 0.7606837606837606, "os": 0.07168278435572467, "ncat25k": 19, "term": "possible", "bg": 6.643484363012106e-07, "y": 0.4195804195804196, "cat25k": 7}, {"ncat": 34, "s": 0.31313131313131315, "cat": 2, "x": 0.7614607614607615, "os": 0.14012714453477387, "ncat25k": 20, "term": "outline", "bg": 6.152331733727082e-06, "y": 0.5493395493395493, "cat25k": 15}, {"ncat": 34, "s": 0.121989121989122, "cat": 1, "x": 0.7622377622377622, "os": 0.06946380267330377, "ncat25k": 20, "term": "online", "bg": 2.1282442508511816e-07, "y": 0.29836829836829837, "cat25k": 7}, {"ncat": 34, "s": 0.8780108780108781, "cat": 6, "x": 0.763014763014763, "os": 0.6715921998168707, "ncat25k": 20, "term": "production", "bg": 1.476902837758586e-06, "y": 0.9168609168609169, "cat25k": 45}, {"ncat": 34, "s": 0.31313131313131315, "cat": 2, "x": 0.7637917637917638, "os": 0.14012714453477387, "ncat25k": 20, "term": "similar", "bg": 1.039671142627009e-06, "y": 0.5804195804195804, "cat25k": 15}, {"ncat": 34, "s": 0.6891996891996892, "cat": 3, "x": 0.7645687645687645, "os": 0.5497141682202341, "ncat25k": 20, "term": "getting", "bg": 8.760593534058046e-07, "y": 0.7505827505827506, "cat25k": 22}, {"ncat": 34, "s": 0.7668997668997669, "cat": 4, "x": 0.7653457653457654, "os": 0.5949886640987, "ncat25k": 20, "term": "after", "bg": 2.466053275585657e-07, "y": 0.83993783993784, "cat25k": 30}, {"ncat": 34, "s": 0.121989121989122, "cat": 1, "x": 0.7661227661227661, "os": 0.06946380267330377, "ncat25k": 20, "term": "come", "bg": 5.17987896992642e-07, "y": 0.3690753690753691, "cat25k": 7}, {"ncat": 34, "s": 0.6891996891996892, "cat": 3, "x": 0.7668997668997669, "os": 0.5497141682202341, "ncat25k": 20, "term": "current", "bg": 4.0533422605125907e-07, "y": 0.7661227661227661, "cat25k": 22}, {"ncat": 34, "s": 0.8290598290598291, "cat": 5, "x": 0.7676767676767676, "os": 0.6354498044018917, "ncat25k": 20, "term": "uses", "bg": 1.679771715449888e-06, "y": 0.8857808857808858, "cat25k": 37}, {"ncat": 34, "s": 0.31313131313131315, "cat": 2, "x": 0.7684537684537684, "os": 0.14012714453477387, "ncat25k": 20, "term": "tasks", "bg": 5.296446266007834e-06, "y": 0.6441336441336442, "cat25k": 15}, {"ncat": 35, "s": 0.8243978243978244, "cat": 5, "x": 0.7692307692307693, "os": 0.6299438870054632, "ncat25k": 20, "term": "classification", "bg": 6.783615871765718e-06, "y": 0.8547008547008547, "cat25k": 37}, {"ncat": 35, "s": 0.30613830613830617, "cat": 2, "x": 0.77000777000777, "os": 0.1360048377505299, "ncat25k": 20, "term": "interactive", "bg": 4.5640705042214625e-06, "y": 0.5143745143745144, "cat25k": 15}, {"ncat": 35, "s": 0.006216006216006216, "cat": 0, "x": 0.7707847707847708, "os": 0.00015167402810351005, "ncat25k": 20, "term": "matplotlib", "bg": 0.0007291337066131051, "y": 0.003108003108003108, "cat25k": 0}, {"ncat": 35, "s": 0.30613830613830617, "cat": 2, "x": 0.7715617715617715, "os": 0.1360048377505299, "ncat25k": 20, "term": "design", "bg": 3.552988802529002e-07, "y": 0.5384615384615384, "cat25k": 15}, {"ncat": 35, "s": 0.5345765345765346, "cat": 3, "x": 0.7723387723387724, "os": 0.20523838441607417, "ncat25k": 20, "term": "3", "bg": 0.0, "y": 0.7358197358197358, "cat25k": 22}, {"ncat": 35, "s": 0.10411810411810411, "cat": 1, "x": 0.7731157731157731, "os": 0.06739541925542258, "ncat25k": 20, "term": "run", "bg": 8.352423308177027e-07, "y": 0.3986013986013986, "cat25k": 7}, {"ncat": 35, "s": 0.8243978243978244, "cat": 5, "x": 0.7738927738927739, "os": 0.6299438870054632, "ncat25k": 20, "term": "task", "bg": 2.4029813188477613e-06, "y": 0.8818958818958819, "cat25k": 37}, {"ncat": 36, "s": 0.749028749028749, "cat": 4, "x": 0.7746697746697747, "os": 0.5853689740375433, "ncat25k": 21, "term": "long", "bg": 3.562420214404102e-07, "y": 0.8018648018648019, "cat25k": 30}, {"ncat": 36, "s": 0.10178710178710179, "cat": 1, "x": 0.7754467754467754, "os": 0.06545952207578015, "ncat25k": 21, "term": "statistical", "bg": 7.924157347278692e-06, "y": 0.2634032634032634, "cat25k": 7}, {"ncat": 36, "s": 0.5104895104895105, "cat": 3, "x": 0.7762237762237763, "os": 0.19942405653958545, "ncat25k": 21, "term": "interesting", "bg": 1.9957157926616893e-06, "y": 0.7156177156177156, "cat25k": 22}, {"ncat": 36, "s": 0.749028749028749, "cat": 4, "x": 0.777000777000777, "os": 0.5853689740375433, "ncat25k": 21, "term": "popular", "bg": 8.575345666824238e-07, "y": 0.8174048174048174, "cat25k": 30}, {"ncat": 36, "s": 0.29603729603729606, "cat": 2, "x": 0.7777777777777778, "os": 0.13212822840147792, "ncat25k": 21, "term": "single", "bg": 9.342200119268755e-07, "y": 0.5369075369075369, "cat25k": 15}, {"ncat": 36, "s": 0.29603729603729606, "cat": 2, "x": 0.7785547785547785, "os": 0.13212822840147792, "ncat25k": 21, "term": "value", "bg": 6.147802829885239e-07, "y": 0.5477855477855478, "cat25k": 15}, {"ncat": 36, "s": 0.10178710178710179, "cat": 1, "x": 0.7793317793317793, "os": 0.06545952207578015, "ncat25k": 21, "term": "statistics", "bg": 1.4716045038080726e-06, "y": 0.3084693084693085, "cat25k": 7}, {"ncat": 36, "s": 0.005439005439005439, "cat": 0, "x": 0.7801087801087802, "os": 0.00010152087558684686, "ncat25k": 21, "term": "become", "bg": 7.793541652877695e-07, "y": 0.07925407925407925, "cat25k": 0}, {"ncat": 36, "s": 0.749028749028749, "cat": 4, "x": 0.7808857808857809, "os": 0.5853689740375433, "ncat25k": 21, "term": "e.g.", "bg": 0.0, "y": 0.846930846930847, "cat25k": 30}, {"ncat": 36, "s": 0.749028749028749, "cat": 4, "x": 0.7816627816627817, "os": 0.5853689740375433, "ncat25k": 21, "term": "years", "bg": 2.840581581738263e-07, "y": 0.8484848484848485, "cat25k": 30}, {"ncat": 37, "s": 0.2882672882672883, "cat": 2, "x": 0.7824397824397824, "os": 0.12847367836034324, "ncat25k": 21, "term": "things", "bg": 7.428038245915497e-07, "y": 0.5205905205905206, "cat25k": 15}, {"ncat": 37, "s": 0.2882672882672883, "cat": 2, "x": 0.7832167832167832, "os": 0.12847367836034324, "ncat25k": 21, "term": "within", "bg": 3.4415699827367406e-07, "y": 0.5236985236985237, "cat25k": 15}, {"ncat": 37, "s": 0.10023310023310024, "cat": 1, "x": 0.7839937839937839, "os": 0.06364112318120374, "ncat25k": 21, "term": "questions", "bg": 7.014350104106665e-07, "y": 0.2789432789432789, "cat25k": 7}, {"ncat": 37, "s": 0.2882672882672883, "cat": 2, "x": 0.7847707847707848, "os": 0.12847367836034324, "ncat25k": 21, "term": "r", "bg": 5.808709033913465e-07, "y": 0.54001554001554, "cat25k": 15}, {"ncat": 37, "s": 0.10023310023310024, "cat": 1, "x": 0.7855477855477856, "os": 0.06364112318120374, "ncat25k": 21, "term": "projects", "bg": 1.4667635478370046e-06, "y": 0.32944832944832947, "cat25k": 7}, {"ncat": 37, "s": 0.8585858585858586, "cat": 6, "x": 0.7863247863247863, "os": 0.6545223371696185, "ncat25k": 21, "term": "recent", "bg": 7.611884822005271e-07, "y": 0.9199689199689199, "cat25k": 45}, {"ncat": 37, "s": 0.7397047397047397, "cat": 4, "x": 0.7871017871017871, "os": 0.5808695111764461, "ncat25k": 21, "term": "test", "bg": 8.122887477108124e-07, "y": 0.8492618492618492, "cat25k": 30}, {"ncat": 38, "s": 0.9292929292929293, "cat": 9, "x": 0.7878787878787878, "os": 0.7339985721419788, "ncat25k": 22, "term": "neural", "bg": 5.0643074633475374e-05, "y": 0.9471639471639471, "cat25k": 67}, {"ncat": 38, "s": 0.5011655011655012, "cat": 3, "x": 0.7886557886557887, "os": 0.18873878184397758, "ncat25k": 22, "term": "share", "bg": 9.714274463911693e-07, "y": 0.7148407148407149, "cat25k": 22}, {"ncat": 38, "s": 0.8034188034188035, "cat": 5, "x": 0.7894327894327894, "os": 0.6147196032422555, "ncat25k": 22, "term": "small", "bg": 5.467900238390378e-07, "y": 0.8648018648018648, "cat25k": 37}, {"ncat": 38, "s": 0.09867909867909869, "cat": 1, "x": 0.7902097902097902, "os": 0.06192775614615598, "ncat25k": 22, "term": "every", "bg": 6.017623687807669e-07, "y": 0.3271173271173271, "cat25k": 7}, {"ncat": 38, "s": 0.8570318570318571, "cat": 6, "x": 0.790986790986791, "os": 0.6492455356892446, "ncat25k": 22, "term": "training", "bg": 8.397314057820908e-07, "y": 0.9184149184149184, "cat25k": 45}, {"ncat": 38, "s": 0.09867909867909869, "cat": 1, "x": 0.7917637917637917, "os": 0.06192775614615598, "ncat25k": 22, "term": "who", "bg": 1.743140520593385e-07, "y": 0.35120435120435123, "cat25k": 7}, {"ncat": 39, "s": 0.48018648018648025, "cat": 3, "x": 0.7925407925407926, "os": 0.18381580349378024, "ncat25k": 22, "term": "include", "bg": 4.816720421722703e-07, "y": 0.6977466977466977, "cat25k": 22}, {"ncat": 39, "s": 0.7902097902097902, "cat": 5, "x": 0.7933177933177933, "os": 0.6100367019635886, "ncat25k": 22, "term": "across", "bg": 1.668517265223555e-06, "y": 0.8585858585858586, "cat25k": 37}, {"ncat": 39, "s": 0.0979020979020979, "cat": 1, "x": 0.7940947940947941, "os": 0.06030898005503649, "ncat25k": 22, "term": "business", "bg": 2.2910878812445163e-07, "y": 0.27972027972027974, "cat25k": 7}, {"ncat": 39, "s": 0.2727272727272727, "cat": 2, "x": 0.7948717948717948, "os": 0.12175208803131599, "ncat25k": 22, "term": "allow", "bg": 1.4292048087560947e-06, "y": 0.5415695415695416, "cat25k": 15}, {"ncat": 39, "s": 0.2727272727272727, "cat": 2, "x": 0.7956487956487956, "os": 0.12175208803131599, "ncat25k": 22, "term": "know", "bg": 3.135016694698669e-07, "y": 0.5439005439005439, "cat25k": 15}, {"ncat": 39, "s": 0.48018648018648025, "cat": 3, "x": 0.7964257964257965, "os": 0.18381580349378024, "ncat25k": 22, "term": "makes", "bg": 1.2734905276978618e-06, "y": 0.7264957264957265, "cat25k": 22}, {"ncat": 39, "s": 0.004662004662004663, "cat": 0, "x": 0.7972027972027972, "os": 2.8507032052815795e-05, "ncat25k": 22, "term": "creating", "bg": 2.462709321955069e-06, "y": 0.06216006216006216, "cat25k": 0}, {"ncat": 39, "s": 0.7902097902097902, "cat": 5, "x": 0.797979797979798, "os": 0.6100367019635886, "ncat25k": 22, "term": "programming", "bg": 3.1134185264481755e-06, "y": 0.8741258741258742, "cat25k": 37}, {"ncat": 39, "s": 0.48018648018648025, "cat": 3, "x": 0.7987567987567987, "os": 0.18381580349378024, "ncat25k": 22, "term": "along", "bg": 1.2206445311875935e-06, "y": 0.7482517482517482, "cat25k": 22}, {"ncat": 39, "s": 0.2727272727272727, "cat": 2, "x": 0.7995337995337995, "os": 0.12175208803131599, "ncat25k": 22, "term": "made", "bg": 3.3394710840642863e-07, "y": 0.5967365967365967, "cat25k": 15}, {"ncat": 40, "s": 0.2680652680652681, "cat": 2, "x": 0.8003108003108003, "os": 0.11865206913548776, "ncat25k": 23, "term": "around", "bg": 5.477073011495828e-07, "y": 0.5244755244755245, "cat25k": 15}, {"ncat": 40, "s": 0.8337218337218338, "cat": 6, "x": 0.8010878010878011, "os": 0.6392514359219752, "ncat25k": 23, "term": "numpy", "bg": 0.0008748384531265533, "y": 0.8997668997668997, "cat25k": 45}, {"ncat": 40, "s": 0.0023310023310023314, "cat": 0, "x": 0.8018648018648019, "os": 1.8247937962290184e-05, "ncat25k": 23, "term": "visualization", "bg": 3.8278316916582334e-05, "y": 0.010878010878010878, "cat25k": 0}, {"ncat": 40, "s": 0.4708624708624709, "cat": 3, "x": 0.8026418026418026, "os": 0.17914296117024553, "ncat25k": 23, "term": "able", "bg": 9.131862005291877e-07, "y": 0.714063714063714, "cat25k": 22}, {"ncat": 40, "s": 0.7847707847707849, "cat": 5, "x": 0.8034188034188035, "os": 0.6055320492811147, "ncat25k": 23, "term": "various", "bg": 1.4461979996912368e-06, "y": 0.8671328671328671, "cat25k": 37}, {"ncat": 40, "s": 0.09557109557109557, "cat": 1, "x": 0.8041958041958042, "os": 0.0587759769473406, "ncat25k": 23, "term": "analytics", "bg": 3.774947685516658e-05, "y": 0.289044289044289, "cat25k": 7}, {"ncat": 40, "s": 0.0023310023310023314, "cat": 0, "x": 0.804972804972805, "os": 1.8247937962290184e-05, "ncat25k": 23, "term": "case", "bg": 6.279683224604436e-07, "y": 0.04972804972804973, "cat25k": 0}, {"ncat": 40, "s": 0.4708624708624709, "cat": 3, "x": 0.8057498057498057, "os": 0.17914296117024553, "ncat25k": 23, "term": "important", "bg": 7.487826226052627e-07, "y": 0.7443667443667443, "cat25k": 22}, {"ncat": 40, "s": 0.0023310023310023314, "cat": 0, "x": 0.8065268065268065, "os": 1.8247937962290184e-05, "ncat25k": 23, "term": "community", "bg": 5.497089743345576e-07, "y": 0.08547008547008547, "cat25k": 0}, {"ncat": 40, "s": 0.09557109557109557, "cat": 1, "x": 0.8073038073038074, "os": 0.0587759769473406, "ncat25k": 23, "term": "being", "bg": 3.9522316323256364e-07, "y": 0.35975135975135974, "cat25k": 7}, {"ncat": 40, "s": 0.09557109557109557, "cat": 1, "x": 0.8080808080808081, "os": 0.0587759769473406, "ncat25k": 23, "term": "just", "bg": 2.4192477302093986e-07, "y": 0.3853923853923854, "cat25k": 7}, {"ncat": 40, "s": 0.8337218337218338, "cat": 6, "x": 0.8088578088578089, "os": 0.6392514359219752, "ncat25k": 23, "term": "particular", "bg": 1.6903457082572765e-06, "y": 0.9246309246309247, "cat25k": 45}, {"ncat": 40, "s": 0.8337218337218338, "cat": 6, "x": 0.8096348096348096, "os": 0.6392514359219752, "ncat25k": 23, "term": "support", "bg": 3.479375777931549e-07, "y": 0.9254079254079254, "cat25k": 45}, {"ncat": 41, "s": 0.8267288267288269, "cat": 6, "x": 0.8104118104118104, "os": 0.6345160431762192, "ncat25k": 24, "term": "great", "bg": 3.514529580862142e-07, "y": 0.8974358974358975, "cat25k": 45}, {"ncat": 41, "s": 0.4568764568764569, "cat": 3, "x": 0.8111888111888111, "os": 0.17470113450475833, "ncat25k": 24, "term": "&", "bg": 0.0, "y": 0.7094017094017094, "cat25k": 22}, {"ncat": 41, "s": 0.8267288267288269, "cat": 6, "x": 0.811965811965812, "os": 0.6345160431762192, "ncat25k": 24, "term": "would", "bg": 2.0601932241257497e-07, "y": 0.9083139083139083, "cat25k": 45}, {"ncat": 41, "s": 0.22610722610722614, "cat": 2, "x": 0.8127428127428128, "os": 0.11570730678604174, "ncat25k": 24, "term": "same", "bg": 4.852297634863292e-07, "y": 0.554001554001554, "cat25k": 15}, {"ncat": 41, "s": 0.22610722610722614, "cat": 2, "x": 0.8135198135198135, "os": 0.11570730678604174, "ncat25k": 24, "term": "might", "bg": 7.947214020598238e-07, "y": 0.5633255633255633, "cat25k": 15}, {"ncat": 41, "s": 0.8267288267288269, "cat": 6, "x": 0.8142968142968143, "os": 0.6345160431762192, "ncat25k": 24, "term": "developed", "bg": 1.9883831711580664e-06, "y": 0.9176379176379177, "cat25k": 45}, {"ncat": 41, "s": 0.717948717948718, "cat": 4, "x": 0.815073815073815, "os": 0.5646619379771477, "ncat25k": 24, "term": "types", "bg": 1.580548270170889e-06, "y": 0.8391608391608392, "cat25k": 30}, {"ncat": 41, "s": 0.22610722610722614, "cat": 2, "x": 0.8158508158508159, "os": 0.11570730678604174, "ncat25k": 24, "term": "useful", "bg": 1.9909977380549323e-06, "y": 0.6006216006216006, "cat25k": 15}, {"ncat": 41, "s": 0.4568764568764569, "cat": 3, "x": 0.8166278166278166, "os": 0.17470113450475833, "ncat25k": 24, "term": "pipeline", "bg": 9.632821248802699e-06, "y": 0.7707847707847708, "cat25k": 22}, {"ncat": 41, "s": 0.0947940947940948, "cat": 1, "x": 0.8174048174048174, "os": 0.05732122916587307, "ncat25k": 24, "term": "platform", "bg": 3.04876967031367e-06, "y": 0.4568764568764569, "cat25k": 7}, {"ncat": 42, "s": 0.0015540015540015542, "cat": 0, "x": 0.8181818181818182, "os": 7.202132607297962e-06, "ncat25k": 24, "term": "packages", "bg": 3.0661955033848713e-06, "y": 0.0, "cat25k": 0}, {"ncat": 42, "s": 0.4483294483294484, "cat": 3, "x": 0.818958818958819, "os": 0.17047324969043853, "ncat25k": 24, "term": "dataset", "bg": 5.216252964387133e-05, "y": 0.7062937062937062, "cat25k": 22}, {"ncat": 42, "s": 0.222999222999223, "cat": 2, "x": 0.8197358197358198, "os": 0.11290589791280414, "ncat25k": 24, "term": "few", "bg": 9.168184654336889e-07, "y": 0.5470085470085471, "cat25k": 15}, {"ncat": 42, "s": 0.09324009324009325, "cat": 1, "x": 0.8205128205128205, "os": 0.0559382634597696, "ncat25k": 24, "term": "key", "bg": 7.884334304447277e-07, "y": 0.29603729603729606, "cat25k": 7}, {"ncat": 42, "s": 0.7684537684537684, "cat": 5, "x": 0.8212898212898213, "os": 0.5970199217042453, "ncat25k": 24, "term": "no", "bg": 1.32304772416874e-07, "y": 0.8733488733488733, "cat25k": 37}, {"ncat": 42, "s": 0.8958818958818958, "cat": 8, "x": 0.822066822066822, "os": 0.6874065187282342, "ncat25k": 24, "term": "networks", "bg": 5.066098945455551e-06, "y": 0.9448329448329449, "cat25k": 59}, {"ncat": 42, "s": 0.222999222999223, "cat": 2, "x": 0.8228438228438228, "os": 0.11290589791280414, "ncat25k": 24, "term": "together", "bg": 1.1248901666792208e-06, "y": 0.5827505827505828, "cat25k": 15}, {"ncat": 42, "s": 0.09324009324009325, "cat": 1, "x": 0.8236208236208237, "os": 0.0559382634597696, "ncat25k": 24, "term": "algorithm", "bg": 8.206140171976562e-06, "y": 0.3620823620823621, "cat25k": 7}, {"ncat": 42, "s": 0.8259518259518259, "cat": 6, "x": 0.8243978243978244, "os": 0.6299438876019591, "ncat25k": 24, "term": "feature", "bg": 3.0431656275808684e-06, "y": 0.9238539238539238, "cat25k": 45}, {"ncat": 43, "s": 0.7101787101787103, "cat": 4, "x": 0.8251748251748252, "os": 0.557492011048389, "ncat25k": 25, "term": "application", "bg": 1.0987984423197825e-06, "y": 0.8127428127428128, "cat25k": 30}, {"ncat": 43, "s": 0.000777000777000777, "cat": 0, "x": 0.8259518259518259, "os": 4.428962058378172e-06, "ncat25k": 25, "term": "cases", "bg": 1.319851795925812e-06, "y": 0.035742035742035744, "cat25k": 0}, {"ncat": 43, "s": 0.8181818181818182, "cat": 6, "x": 0.8267288267288267, "os": 0.6255271705559153, "ncat25k": 25, "term": "framework", "bg": 5.531732398757389e-06, "y": 0.9114219114219114, "cat25k": 45}, {"ncat": 43, "s": 0.8181818181818182, "cat": 6, "x": 0.8275058275058275, "os": 0.6255271705559153, "ncat25k": 25, "term": "allows", "bg": 2.5060478508891055e-06, "y": 0.9121989121989122, "cat25k": 45}, {"ncat": 43, "s": 0.43434343434343436, "cat": 3, "x": 0.8282828282828283, "os": 0.16644398643590652, "ncat25k": 25, "term": "engineering", "bg": 1.6495843111428271e-06, "y": 0.7311577311577312, "cat25k": 22}, {"ncat": 43, "s": 0.22222222222222224, "cat": 2, "x": 0.8290598290598291, "os": 0.11023725980268534, "ncat25k": 25, "term": "specific", "bg": 1.1121053848114929e-06, "y": 0.5757575757575758, "cat25k": 15}, {"ncat": 43, "s": 0.43434343434343436, "cat": 3, "x": 0.8298368298368298, "os": 0.16644398643590652, "ncat25k": 25, "term": "see", "bg": 1.7607492612585728e-07, "y": 0.7552447552447552, "cat25k": 22}, {"ncat": 43, "s": 0.08236208236208235, "cat": 1, "x": 0.8306138306138307, "os": 0.054621449723786186, "ncat25k": 25, "term": "find", "bg": 2.469329021810516e-07, "y": 0.3613053613053613, "cat25k": 7}, {"ncat": 44, "s": 0.2167832167832168, "cat": 2, "x": 0.8313908313908314, "os": 0.1076919264722126, "ncat25k": 25, "term": "via", "bg": 1.2746082393924788e-06, "y": 0.5291375291375291, "cat25k": 15}, {"ncat": 44, "s": 0.39083139083139085, "cat": 3, "x": 0.8321678321678322, "os": 0.1625995355393225, "ncat25k": 25, "term": "understand", "bg": 1.9405658686838804e-06, "y": 0.7397047397047397, "cat25k": 22}, {"ncat": 45, "s": 0.8888888888888888, "cat": 8, "x": 0.8329448329448329, "os": 0.6731049352665247, "ncat25k": 26, "term": "network", "bg": 1.2248335916008865e-06, "y": 0.9386169386169386, "cat25k": 59}, {"ncat": 45, "s": 0.3877233877233877, "cat": 3, "x": 0.8337218337218337, "os": 0.15892739808760425, "ncat25k": 26, "term": "want", "bg": 5.100305722757464e-07, "y": 0.7055167055167055, "cat25k": 22}, {"ncat": 45, "s": 0.6977466977466977, "cat": 4, "x": 0.8344988344988346, "os": 0.5508545866467703, "ncat25k": 26, "term": "working", "bg": 8.93137371957086e-07, "y": 0.8057498057498057, "cat25k": 30}, {"ncat": 45, "s": 0.6977466977466977, "cat": 4, "x": 0.8352758352758353, "os": 0.5508545866467703, "ncat25k": 26, "term": "ways", "bg": 2.0950633267235802e-06, "y": 0.8445998445998446, "cat25k": 30}, {"ncat": 46, "s": 0.20823620823620825, "cat": 2, "x": 0.8360528360528361, "os": 0.10293793611195051, "ncat25k": 26, "term": "series", "bg": 1.3363337407893276e-06, "y": 0.5252525252525253, "cat25k": 15}, {"ncat": 46, "s": 0.3721833721833722, "cat": 3, "x": 0.8368298368298368, "os": 0.15541621859189797, "ncat25k": 26, "term": "introduce", "bg": 9.529357871782794e-06, "y": 0.710955710955711, "cat25k": 22}, {"ncat": 46, "s": 0.20823620823620825, "cat": 2, "x": 0.8376068376068376, "os": 0.10293793611195051, "ncat25k": 26, "term": "even", "bg": 4.556265301906824e-07, "y": 0.5299145299145299, "cat25k": 15}, {"ncat": 46, "s": 0.08158508158508158, "cat": 1, "x": 0.8383838383838383, "os": 0.0510211975248005, "ncat25k": 26, "term": "finally", "bg": 2.1174040699397294e-06, "y": 0.30536130536130535, "cat25k": 7}, {"ncat": 46, "s": 0.8018648018648019, "cat": 6, "x": 0.8391608391608392, "os": 0.6131382142715753, "ncat25k": 26, "term": "complex", "bg": 3.107463582655211e-06, "y": 0.9215229215229215, "cat25k": 45}, {"ncat": 46, "s": 0.7451437451437452, "cat": 5, "x": 0.83993783993784, "os": 0.5817542683628221, "ncat25k": 26, "term": "much", "bg": 4.86710407385974e-07, "y": 0.8865578865578866, "cat25k": 37}, {"ncat": 46, "s": 0.20823620823620825, "cat": 2, "x": 0.8407148407148407, "os": 0.10293793611195051, "ncat25k": 26, "term": "software", "bg": 4.4248417479631513e-07, "y": 0.6472416472416472, "cat25k": 15}, {"ncat": 47, "s": 0.20590520590520592, "cat": 2, "x": 0.8414918414918415, "os": 0.10071459639284758, "ncat25k": 27, "term": "notebooks", "bg": 1.2020427032882598e-05, "y": 0.5151515151515151, "cat25k": 15}, {"ncat": 47, "s": 0.3690753690753691, "cat": 3, "x": 0.8422688422688422, "os": 0.152055645631623, "ncat25k": 27, "term": "powerful", "bg": 4.15965887944851e-06, "y": 0.7218337218337219, "cat25k": 22}, {"ncat": 47, "s": 0.20590520590520592, "cat": 2, "x": 0.8430458430458431, "os": 0.10071459639284758, "ncat25k": 27, "term": "datasets", "bg": 7.5089892240931e-05, "y": 0.5602175602175602, "cat25k": 15}, {"ncat": 47, "s": 0.7365967365967366, "cat": 5, "x": 0.8438228438228438, "os": 0.5782622537570583, "ncat25k": 27, "term": "those", "bg": 4.96054126346289e-07, "y": 0.878010878010878, "cat25k": 37}, {"ncat": 47, "s": 0.7365967365967366, "cat": 5, "x": 0.8445998445998446, "os": 0.5782622537570583, "ncat25k": 27, "term": "however", "bg": 7.923213335962621e-07, "y": 0.8873348873348873, "cat25k": 37}, {"ncat": 48, "s": 0.7855477855477856, "cat": 6, "x": 0.8453768453768454, "os": 0.6055320498485519, "ncat25k": 28, "term": "without", "bg": 5.745350420139237e-07, "y": 0.8958818958818959, "cat25k": 45}, {"ncat": 48, "s": 0.36519036519036524, "cat": 3, "x": 0.8461538461538461, "os": 0.14883621473171277, "ncat25k": 28, "term": "explore", "bg": 3.955067048274136e-06, "y": 0.7078477078477079, "cat25k": 22}, {"ncat": 48, "s": 0.7855477855477856, "cat": 6, "x": 0.846930846930847, "os": 0.6055320498485519, "ncat25k": 28, "term": "big", "bg": 9.132493847132173e-07, "y": 0.9222999222999223, "cat25k": 45}, {"ncat": 48, "s": 0.7327117327117327, "cat": 5, "x": 0.8477078477078477, "os": 0.5748873861209252, "ncat25k": 28, "term": "challenges", "bg": 6.552390287293143e-06, "y": 0.8834498834498834, "cat25k": 37}, {"ncat": 49, "s": 0.1763791763791764, "cat": 2, "x": 0.8484848484848485, "os": 0.09654331658257531, "ncat25k": 28, "term": "n't", "bg": 0.0, "y": 0.5306915306915307, "cat25k": 15}, {"ncat": 49, "s": 0.1763791763791764, "cat": 2, "x": 0.8492618492618492, "os": 0.09654331658257531, "ncat25k": 28, "term": "may", "bg": 1.7392570812656326e-07, "y": 0.5423465423465423, "cat25k": 15}, {"ncat": 49, "s": 0.36052836052836057, "cat": 3, "x": 0.85003885003885, "os": 0.14574924920726995, "ncat25k": 28, "term": "number", "bg": 3.438346379404983e-07, "y": 0.728049728049728, "cat25k": 22}, {"ncat": 49, "s": 0.36052836052836057, "cat": 3, "x": 0.8508158508158508, "os": 0.14574924920726995, "ncat25k": 28, "term": "my", "bg": 2.000167653675341e-07, "y": 0.777000777000777, "cat25k": 22}, {"ncat": 50, "s": 0.8197358197358198, "cat": 7, "x": 0.8515928515928516, "os": 0.6261488967814471, "ncat25k": 29, "term": "any", "bg": 2.0257182687386192e-07, "y": 0.9331779331779332, "cat25k": 52}, {"ncat": 50, "s": 0.7715617715617716, "cat": 6, "x": 0.8523698523698524, "os": 0.598394752533407, "ncat25k": 29, "term": "development", "bg": 5.656244939930259e-07, "y": 0.9106449106449106, "cat25k": 45}, {"ncat": 50, "s": 0.7715617715617716, "cat": 6, "x": 0.8531468531468531, "os": 0.598394752533407, "ncat25k": 29, "term": "demonstrate", "bg": 1.0184390398237136e-05, "y": 0.9145299145299145, "cat25k": 45}, {"ncat": 50, "s": 0.0, "cat": 0, "x": 0.853923853923854, "os": 0.0, "ncat25k": 29, "term": "scientists", "bg": 9.605899803084912e-06, "y": 0.05283605283605284, "cat25k": 0}, {"ncat": 50, "s": 0.17404817404817405, "cat": 2, "x": 0.8547008547008547, "os": 0.09458421485476975, "ncat25k": 29, "term": "start", "bg": 8.146585311245045e-07, "y": 0.5617715617715617, "cat25k": 15}, {"ncat": 50, "s": 0.17404817404817405, "cat": 2, "x": 0.8554778554778555, "os": 0.09458421485476975, "ncat25k": 29, "term": "text", "bg": 1.1680352449465172e-06, "y": 0.5641025641025641, "cat25k": 15}, {"ncat": 50, "s": 0.721056721056721, "cat": 5, "x": 0.8562548562548562, "os": 0.5684669196057267, "ncat25k": 29, "term": "level", "bg": 1.0065060352766155e-06, "y": 0.8764568764568764, "cat25k": 37}, {"ncat": 51, "s": 0.8088578088578089, "cat": 7, "x": 0.857031857031857, "os": 0.6224634501131726, "ncat25k": 29, "term": "research", "bg": 5.647250757577085e-07, "y": 0.9292929292929293, "cat25k": 52}, {"ncat": 51, "s": 0.08080808080808081, "cat": 1, "x": 0.8578088578088578, "os": 0.04597312637055506, "ncat25k": 29, "term": "go", "bg": 3.466253593700735e-07, "y": 0.2727272727272727, "cat25k": 7}, {"ncat": 52, "s": 0.972027972027972, "cat": 15, "x": 0.8585858585858586, "os": 0.7768453085917426, "ncat25k": 30, "term": "deep", "bg": 6.646107046531263e-06, "y": 0.9743589743589743, "cat25k": 111}, {"ncat": 52, "s": 0.31002331002331, "cat": 3, "x": 0.8593628593628594, "os": 0.1372064991649642, "ncat25k": 30, "term": "easy", "bg": 1.2891293325528107e-06, "y": 0.7031857031857032, "cat25k": 22}, {"ncat": 52, "s": 0.17094017094017094, "cat": 2, "x": 0.8601398601398601, "os": 0.09089449798626892, "ncat25k": 30, "term": "order", "bg": 4.098009418651039e-07, "y": 0.6192696192696193, "cat25k": 15}, {"ncat": 53, "s": 0.4731934731934732, "cat": 4, "x": 0.8609168609168609, "os": 0.18027276527042235, "ncat25k": 30, "term": "making", "bg": 1.206598899031595e-06, "y": 0.8065268065268065, "cat25k": 30}, {"ncat": 53, "s": 0.06526806526806528, "cat": 1, "x": 0.8616938616938616, "os": 0.0442229256019136, "ncat25k": 30, "term": "experience", "bg": 9.034455138713567e-07, "y": 0.27350427350427353, "cat25k": 7}, {"ncat": 53, "s": 0.4731934731934732, "cat": 4, "x": 0.8624708624708625, "os": 0.18027276527042235, "ncat25k": 30, "term": "while", "bg": 6.716371616682479e-07, "y": 0.8142968142968143, "cat25k": 30}, {"ncat": 53, "s": 0.4731934731934732, "cat": 4, "x": 0.8632478632478633, "os": 0.18027276527042235, "ncat25k": 30, "term": "part", "bg": 5.811515481604829e-07, "y": 0.8352758352758353, "cat25k": 30}, {"ncat": 54, "s": 0.46153846153846156, "cat": 4, "x": 0.864024864024864, "os": 0.1768827819767117, "ncat25k": 31, "term": "scale", "bg": 3.5756432609151415e-06, "y": 0.8088578088578089, "cat25k": 30}, {"ncat": 54, "s": 0.7521367521367522, "cat": 6, "x": 0.8648018648018648, "os": 0.5853694284591435, "ncat25k": 31, "term": "good", "bg": 4.5365840702009823e-07, "y": 0.912975912975913, "cat25k": 45}, {"ncat": 54, "s": 0.2944832944832945, "cat": 3, "x": 0.8655788655788655, "os": 0.13204312695866616, "ncat25k": 31, "term": "take", "bg": 5.218032974444336e-07, "y": 0.7334887334887334, "cat25k": 22}, {"ncat": 54, "s": 0.2944832944832945, "cat": 3, "x": 0.8663558663558664, "os": 0.13204312695866616, "ncat25k": 31, "term": "people", "bg": 3.455302291773165e-07, "y": 0.7536907536907537, "cat25k": 22}, {"ncat": 54, "s": 0.0644910644910645, "cat": 1, "x": 0.8671328671328671, "os": 0.04339683164303165, "ncat25k": 31, "term": "end", "bg": 7.785270718621857e-07, "y": 0.3791763791763792, "cat25k": 7}, {"ncat": 55, "s": 0.29059829059829057, "cat": 3, "x": 0.8679098679098679, "os": 0.12960352974891526, "ncat25k": 32, "term": "notebook", "bg": 9.819012769755304e-06, "y": 0.7008547008547008, "cat25k": 22}, {"ncat": 55, "s": 0.45532245532245536, "cat": 4, "x": 0.8686868686868687, "os": 0.17361653855005355, "ncat25k": 32, "term": "multiple", "bg": 2.5343553744912846e-06, "y": 0.8073038073038074, "cat25k": 30}, {"ncat": 55, "s": 0.29059829059829057, "cat": 3, "x": 0.8694638694638694, "os": 0.12960352974891526, "ncat25k": 32, "term": "give", "bg": 1.0368855780927213e-06, "y": 0.7195027195027195, "cat25k": 22}, {"ncat": 55, "s": 0.7894327894327895, "cat": 7, "x": 0.8702408702408703, "os": 0.6087319002803033, "ncat25k": 32, "term": "api", "bg": 1.519812675418975e-05, "y": 0.9355089355089355, "cat25k": 52}, {"ncat": 55, "s": 0.7024087024087025, "cat": 5, "x": 0.871017871017871, "os": 0.5541108530649196, "ncat25k": 32, "term": "does", "bg": 5.029660849109445e-07, "y": 0.8896658896658897, "cat25k": 37}, {"ncat": 56, "s": 0.7000777000777001, "cat": 5, "x": 0.8717948717948718, "os": 0.5514964483506724, "ncat25k": 32, "term": "built", "bg": 2.4991652544624907e-06, "y": 0.8593628593628594, "cat25k": 37}, {"ncat": 56, "s": 0.2843822843822844, "cat": 3, "x": 0.8725718725718725, "os": 0.12725187209045474, "ncat25k": 32, "term": "understanding", "bg": 3.0297494124558453e-06, "y": 0.7350427350427351, "cat25k": 22}, {"ncat": 56, "s": 0.7000777000777001, "cat": 5, "x": 0.8733488733488733, "os": 0.5514964483506724, "ncat25k": 32, "term": "user", "bg": 8.023654998503304e-07, "y": 0.8717948717948718, "cat25k": 37}, {"ncat": 56, "s": 0.738927738927739, "cat": 6, "x": 0.8741258741258742, "os": 0.5794129545029323, "ncat25k": 32, "term": "us", "bg": 1.4480606101876034e-07, "y": 0.9207459207459208, "cat25k": 45}, {"ncat": 57, "s": 0.2766122766122766, "cat": 3, "x": 0.8749028749028749, "os": 0.12498351009823372, "ncat25k": 33, "term": "tutorial", "bg": 1.8244753825859715e-05, "y": 0.6993006993006993, "cat25k": 22}, {"ncat": 57, "s": 0.16394716394716397, "cat": 2, "x": 0.8756798756798757, "os": 0.08281445410207355, "ncat25k": 33, "term": "look", "bg": 9.355472238337793e-07, "y": 0.5625485625485626, "cat25k": 15}, {"ncat": 58, "s": 0.06371406371406371, "cat": 1, "x": 0.8764568764568764, "os": 0.040379429184449145, "ncat25k": 33, "term": "help", "bg": 2.388854022073675e-07, "y": 0.2533022533022533, "cat25k": 7}, {"ncat": 58, "s": 0.27505827505827507, "cat": 3, "x": 0.8772338772338772, "os": 0.1227941197272267, "ncat25k": 33, "term": "web", "bg": 3.9051828069324415e-07, "y": 0.703962703962704, "cat25k": 22}, {"ncat": 58, "s": 0.13752913752913754, "cat": 2, "x": 0.878010878010878, "os": 0.0813673386889166, "ncat25k": 33, "term": "provides", "bg": 1.5366128328080449e-06, "y": 0.5229215229215229, "cat25k": 15}, {"ncat": 58, "s": 0.27505827505827507, "cat": 3, "x": 0.8787878787878788, "os": 0.1227941197272267, "ncat25k": 33, "term": "focus", "bg": 2.3728251650398767e-06, "y": 0.745920745920746, "cat25k": 22}, {"ncat": 59, "s": 0.5120435120435121, "cat": 5, "x": 0.8795648795648796, "os": 0.20277398684986087, "ncat25k": 34, "term": "systems", "bg": 9.299263686536699e-07, "y": 0.8686868686868687, "cat25k": 37}, {"ncat": 59, "s": 0.5120435120435121, "cat": 5, "x": 0.8803418803418803, "os": 0.20277398684986087, "ncat25k": 34, "term": "tool", "bg": 2.8252306052256598e-06, "y": 0.8702408702408703, "cat25k": 37}, {"ncat": 59, "s": 0.27039627039627046, "cat": 3, "x": 0.8811188811188811, "os": 0.12067966974003519, "ncat25k": 34, "term": "several", "bg": 1.4483702520125243e-06, "y": 0.7715617715617715, "cat25k": 22}, {"ncat": 59, "s": 0.27039627039627046, "cat": 3, "x": 0.8818958818958819, "os": 0.12067966974003519, "ncat25k": 34, "term": "project", "bg": 9.60148276802982e-07, "y": 0.7731157731157731, "cat25k": 22}, {"ncat": 60, "s": 0.804972804972805, "cat": 8, "x": 0.8826728826728827, "os": 0.617131067519683, "ncat25k": 34, "term": "only", "bg": 2.5379107902059883e-07, "y": 0.940947940947941, "cat25k": 59}, {"ncat": 60, "s": 0.2672882672882673, "cat": 3, "x": 0.8834498834498834, "os": 0.11863639739450554, "ncat25k": 34, "term": "jupyter", "bg": 0.002378040494632423, "y": 0.7024087024087025, "cat25k": 22}, {"ncat": 60, "s": 0.508935508935509, "cat": 5, "x": 0.8842268842268842, "os": 0.19934680665849713, "ncat25k": 34, "term": "scikit", "bg": 0.002378040494632423, "y": 0.8624708624708625, "cat25k": 37}, {"ncat": 60, "s": 0.721833721833722, "cat": 6, "x": 0.885003885003885, "os": 0.5684669201304308, "ncat25k": 34, "term": "best", "bg": 4.731558776552244e-07, "y": 0.9137529137529138, "cat25k": 45}, {"ncat": 60, "s": 0.508935508935509, "cat": 5, "x": 0.8857808857808858, "os": 0.19934680665849713, "ncat25k": 34, "term": "approach", "bg": 3.0182316124973956e-06, "y": 0.8787878787878788, "cat25k": 37}, {"ncat": 61, "s": 0.372960372960373, "cat": 4, "x": 0.8865578865578866, "os": 0.15627856564529163, "ncat25k": 35, "term": "often", "bg": 1.769736822652925e-06, "y": 0.8243978243978244, "cat25k": 30}, {"ncat": 61, "s": 0.22843822843822845, "cat": 3, "x": 0.8873348873348873, "os": 0.1166607865257942, "ncat25k": 35, "term": "better", "bg": 1.030553605190448e-06, "y": 0.7606837606837606, "cat25k": 22}, {"ncat": 62, "s": 0.22455322455322457, "cat": 3, "x": 0.8881118881118881, "os": 0.1147495477451948, "ncat25k": 36, "term": "overview", "bg": 3.027117577326731e-06, "y": 0.721056721056721, "cat25k": 22}, {"ncat": 62, "s": 0.5027195027195027, "cat": 5, "x": 0.8888888888888888, "os": 0.1928241050736898, "ncat25k": 36, "term": "was", "bg": 1.388564604941557e-07, "y": 0.8663558663558664, "cat25k": 37}, {"ncat": 62, "s": 0.22455322455322457, "cat": 3, "x": 0.8896658896658897, "os": 0.1147495477451948, "ncat25k": 36, "term": "should", "bg": 4.2273231064523634e-07, "y": 0.7327117327117327, "cat25k": 22}, {"ncat": 62, "s": 0.3706293706293706, "cat": 4, "x": 0.8904428904428905, "os": 0.15371707622131858, "ncat25k": 36, "term": "methods", "bg": 2.4768392522576816e-06, "y": 0.8282828282828283, "cat25k": 30}, {"ncat": 63, "s": 0.36752136752136755, "cat": 4, "x": 0.8912198912198912, "os": 0.15123747298652734, "ncat25k": 36, "term": "own", "bg": 7.668136295143223e-07, "y": 0.8228438228438228, "cat25k": 30}, {"ncat": 63, "s": 0.12820512820512822, "cat": 2, "x": 0.891996891996892, "os": 0.07482771545605238, "ncat25k": 36, "term": "present", "bg": 1.808346640922776e-06, "y": 0.574980574980575, "cat25k": 15}, {"ncat": 64, "s": 0.36363636363636365, "cat": 4, "x": 0.8927738927738927, "os": 0.14883592203684598, "ncat25k": 37, "term": "why", "bg": 1.0097941023058738e-06, "y": 0.7987567987567987, "cat25k": 30}, {"ncat": 64, "s": 0.36363636363636365, "cat": 4, "x": 0.8935508935508936, "os": 0.14883592203684598, "ncat25k": 37, "term": "information", "bg": 2.4444841959030867e-07, "y": 0.8313908313908314, "cat25k": 30}, {"ncat": 66, "s": 0.47630147630147635, "cat": 5, "x": 0.8943278943278943, "os": 0.18096620610148983, "ncat25k": 38, "term": "two", "bg": 4.1221596672161474e-07, "y": 0.8632478632478633, "cat25k": 37}, {"ncat": 66, "s": 0.735042735042735, "cat": 7, "x": 0.8951048951048951, "os": 0.5777730932326904, "ncat25k": 38, "term": "high", "bg": 6.540660097424464e-07, "y": 0.9370629370629371, "cat25k": 52}, {"ncat": 67, "s": 0.351981351981352, "cat": 4, "x": 0.8958818958818959, "os": 0.1420646494229037, "ncat25k": 39, "term": "basic", "bg": 2.5895166478848236e-06, "y": 0.8135198135198135, "cat25k": 30}, {"ncat": 67, "s": 0.7692307692307693, "cat": 8, "x": 0.8966588966588966, "os": 0.5975334961022069, "ncat25k": 39, "term": "applications", "bg": 2.2123471982070816e-06, "y": 0.9432789432789432, "cat25k": 59}, {"ncat": 68, "s": 0.2136752136752137, "cat": 3, "x": 0.8974358974358975, "os": 0.10447411057200734, "ncat25k": 39, "term": "including", "bg": 8.865531334903209e-07, "y": 0.6985236985236986, "cat25k": 22}, {"ncat": 68, "s": 0.2136752136752137, "cat": 3, "x": 0.8982128982128982, "os": 0.10447411057200734, "ncat25k": 39, "term": "first", "bg": 3.1126830860088444e-07, "y": 0.7086247086247086, "cat25k": 22}, {"ncat": 68, "s": 0.3123543123543124, "cat": 4, "x": 0.898989898989899, "os": 0.1399413941556546, "ncat25k": 39, "term": "where", "bg": 6.15664409472697e-07, "y": 0.8368298368298368, "cat25k": 30}, {"ncat": 69, "s": 0.12043512043512043, "cat": 2, "x": 0.8997668997668997, "os": 0.06824309492080094, "ncat25k": 40, "term": "common", "bg": 1.574701563485983e-06, "y": 0.5353535353535354, "cat25k": 15}, {"ncat": 69, "s": 0.8026418026418026, "cat": 9, "x": 0.9005439005439005, "os": 0.6131382142717342, "ncat25k": 40, "term": "performance", "bg": 1.9738487637935378e-06, "y": 0.9541569541569541, "cat25k": 67}, {"ncat": 69, "s": 0.12043512043512043, "cat": 2, "x": 0.9013209013209014, "os": 0.06824309492080094, "ncat25k": 40, "term": "libraries", "bg": 7.5119963392482744e-06, "y": 0.5563325563325563, "cat25k": 15}, {"ncat": 69, "s": 0.45376845376845376, "cat": 5, "x": 0.9020979020979021, "os": 0.1729775461957972, "ncat25k": 40, "term": "between", "bg": 8.60873417438788e-07, "y": 0.871017871017871, "cat25k": 37}, {"ncat": 69, "s": 0.20745920745920746, "cat": 3, "x": 0.9028749028749029, "os": 0.10293704467819248, "ncat25k": 40, "term": "system", "bg": 6.497222609838072e-07, "y": 0.7474747474747475, "cat25k": 22}, {"ncat": 69, "s": 0.7661227661227662, "cat": 8, "x": 0.9036519036519036, "os": 0.5925023615650595, "ncat25k": 40, "term": "features", "bg": 1.7258630341065737e-06, "y": 0.9463869463869464, "cat25k": 59}, {"ncat": 70, "s": 0.7226107226107227, "cat": 7, "x": 0.9044289044289044, "os": 0.5684669201305751, "ncat25k": 40, "term": "problems", "bg": 2.0131125239310815e-06, "y": 0.9324009324009324, "cat25k": 52}, {"ncat": 70, "s": 0.7226107226107227, "cat": 7, "x": 0.9052059052059052, "os": 0.5684669201305751, "ncat25k": 40, "term": "algorithms", "bg": 3.090902564089522e-05, "y": 0.9347319347319347, "cat25k": 52}, {"ncat": 71, "s": 0.054390054390054385, "cat": 1, "x": 0.905982905982906, "os": 0.03293540601732042, "ncat25k": 41, "term": "introduction", "bg": 3.533922308386847e-06, "y": 0.25874125874125875, "cat25k": 7}, {"ncat": 71, "s": 0.20512820512820515, "cat": 3, "x": 0.9067599067599068, "os": 0.0999941657038429, "ncat25k": 41, "term": "pandas", "bg": 0.000712278134986332, "y": 0.7070707070707071, "cat25k": 22}, {"ncat": 71, "s": 0.7886557886557887, "cat": 9, "x": 0.9075369075369075, "os": 0.6080132115593017, "ncat25k": 41, "term": "library", "bg": 1.713995360300876e-06, "y": 0.9526029526029526, "cat25k": 67}, {"ncat": 71, "s": 0.8205128205128206, "cat": 10, "x": 0.9083139083139083, "os": 0.627275684929429, "ncat25k": 41, "term": "been", "bg": 3.999046481266734e-07, "y": 0.9588189588189588, "cat25k": 74}, {"ncat": 72, "s": 0.17871017871017872, "cat": 3, "x": 0.9090909090909091, "os": 0.09858468330675946, "ncat25k": 41, "term": "create", "bg": 1.5890505958447712e-06, "y": 0.7163947163947164, "cat25k": 22}, {"ncat": 72, "s": 0.5073815073815074, "cat": 6, "x": 0.9098679098679099, "os": 0.19934680645805797, "ncat25k": 41, "term": "world", "bg": 5.184561537638187e-07, "y": 0.9052059052059052, "cat25k": 45}, {"ncat": 72, "s": 0.7871017871017871, "cat": 9, "x": 0.9106449106449106, "os": 0.6055320498487081, "ncat25k": 41, "term": "problem", "bg": 1.941475634144222e-06, "y": 0.9549339549339549, "cat25k": 67}, {"ncat": 72, "s": 0.5073815073815074, "cat": 6, "x": 0.9114219114219114, "os": 0.19934680645805797, "ncat25k": 41, "term": "very", "bg": 6.92452383631228e-07, "y": 0.916083916083916, "cat25k": 45}, {"ncat": 73, "s": 0.29292929292929293, "cat": 4, "x": 0.9121989121989122, "os": 0.13020498956289966, "ncat25k": 42, "term": "techniques", "bg": 6.782851130379469e-06, "y": 0.8329448329448329, "cat25k": 30}, {"ncat": 74, "s": 0.2867132867132867, "cat": 4, "x": 0.912975912975913, "os": 0.1284170025587003, "ncat25k": 43, "term": "over", "bg": 4.833015294076898e-07, "y": 0.818958818958819, "cat25k": 30}, {"ncat": 74, "s": 0.5034965034965035, "cat": 6, "x": 0.9137529137529138, "os": 0.19388182383806402, "ncat25k": 43, "term": "so", "bg": 3.3236279720415747e-07, "y": 0.9153069153069153, "cat25k": 45}, {"ncat": 74, "s": 0.17560217560217561, "cat": 3, "x": 0.9145299145299145, "os": 0.09588120520766502, "ncat25k": 43, "term": "cover", "bg": 2.1560052646689343e-06, "y": 0.7498057498057498, "cat25k": 22}, {"ncat": 74, "s": 0.2867132867132867, "cat": 4, "x": 0.9153069153069153, "os": 0.1284170025587003, "ncat25k": 43, "term": "then", "bg": 5.783054618745747e-07, "y": 0.8422688422688422, "cat25k": 30}, {"ncat": 75, "s": 0.38694638694638694, "cat": 5, "x": 0.916083916083916, "os": 0.15892602802041578, "ncat25k": 43, "term": "example", "bg": 1.7662113844204137e-06, "y": 0.8554778554778555, "cat25k": 37}, {"ncat": 75, "s": 0.2836052836052836, "cat": 4, "x": 0.9168609168609169, "os": 0.1266771442681741, "ncat25k": 43, "term": "than", "bg": 4.0180855222848784e-07, "y": 0.8290598290598291, "cat25k": 30}, {"ncat": 76, "s": 0.5003885003885005, "cat": 6, "x": 0.9176379176379177, "os": 0.18870470471776157, "ncat25k": 44, "term": "real", "bg": 8.395115168588923e-07, "y": 0.9036519036519036, "cat25k": 45}, {"ncat": 76, "s": 0.17171717171717174, "cat": 3, "x": 0.9184149184149184, "os": 0.09332151081264178, "ncat25k": 44, "term": "its", "bg": 4.260616809914024e-07, "y": 0.7466977466977467, "cat25k": 22}, {"ncat": 77, "s": 0.4941724941724942, "cat": 6, "x": 0.9191919191919192, "os": 0.18621716172442043, "ncat25k": 44, "term": "results", "bg": 9.317977286886194e-07, "y": 0.9044289044289044, "cat25k": 45}, {"ncat": 79, "s": 0.4770784770784771, "cat": 6, "x": 0.9199689199689199, "os": 0.1814314464970166, "ncat25k": 45, "term": "language", "bg": 2.0918126733278684e-06, "y": 0.8943278943278943, "cat25k": 45}, {"ncat": 79, "s": 0.787878787878788, "cat": 10, "x": 0.9207459207459208, "os": 0.6077627074022679, "ncat25k": 45, "term": "'ll", "bg": 0.0, "y": 0.9603729603729604, "cat25k": 74}, {"ncat": 80, "s": 0.6876456876456877, "cat": 7, "x": 0.9215229215229215, "os": 0.5486026348630092, "ncat25k": 46, "term": "if", "bg": 2.3257747120547086e-07, "y": 0.9300699300699301, "cat25k": 52}, {"ncat": 80, "s": 0.16783216783216787, "cat": 3, "x": 0.9222999222999223, "os": 0.08859004570540274, "ncat25k": 46, "term": "users", "bg": 1.9539878161097248e-06, "y": 0.7101787101787101, "cat25k": 22}, {"ncat": 80, "s": 0.47008547008547014, "cat": 6, "x": 0.9230769230769231, "os": 0.17912858154603745, "ncat25k": 46, "term": "set", "bg": 7.589592734609155e-07, "y": 0.9098679098679099, "cat25k": 45}, {"ncat": 80, "s": 0.16783216783216787, "cat": 3, "x": 0.9238539238539238, "os": 0.08859004570540274, "ncat25k": 46, "term": "provide", "bg": 1.3248051357925817e-06, "y": 0.7365967365967366, "cat25k": 22}, {"ncat": 81, "s": 0.4607614607614608, "cat": 6, "x": 0.9246309246309247, "os": 0.17688277268332048, "ncat25k": 47, "term": "examples", "bg": 7.376843463758618e-06, "y": 0.8951048951048951, "cat25k": 45}, {"ncat": 81, "s": 0.7195027195027195, "cat": 8, "x": 0.9254079254079254, "os": 0.5665454479277445, "ncat25k": 47, "term": "processing", "bg": 5.7549162451070625e-06, "y": 0.9401709401709402, "cat25k": 59}, {"ncat": 81, "s": 0.23620823620823622, "cat": 4, "x": 0.9261849261849262, "os": 0.11714852943251813, "ncat25k": 47, "term": "get", "bg": 4.1577143953723525e-07, "y": 0.8080808080808081, "cat25k": 30}, {"ncat": 82, "s": 0.5322455322455323, "cat": 7, "x": 0.9269619269619269, "os": 0.20427857376904235, "ncat25k": 47, "term": "simple", "bg": 2.8459573781958236e-06, "y": 0.9308469308469308, "cat25k": 52}, {"ncat": 82, "s": 0.4560994560994561, "cat": 6, "x": 0.9277389277389277, "os": 0.1746919476358087, "ncat25k": 47, "term": "out", "bg": 3.04697367368502e-07, "y": 0.905982905982906, "cat25k": 45}, {"ncat": 82, "s": 0.5322455322455323, "cat": 7, "x": 0.9285159285159286, "os": 0.20427857376904235, "ncat25k": 47, "term": "large", "bg": 1.6869349185990945e-06, "y": 0.9362859362859363, "cat25k": 52}, {"ncat": 83, "s": 0.1655011655011655, "cat": 3, "x": 0.9292929292929293, "os": 0.08534381761519727, "ncat25k": 48, "term": "need", "bg": 7.324066234059949e-07, "y": 0.7047397047397047, "cat25k": 22}, {"ncat": 83, "s": 0.3535353535353536, "cat": 5, "x": 0.9300699300699301, "os": 0.14336956142353113, "ncat25k": 48, "term": "each", "bg": 9.266573629094445e-07, "y": 0.8795648795648796, "cat25k": 37}, {"ncat": 84, "s": 0.35120435120435123, "cat": 5, "x": 0.9308469308469308, "os": 0.14163490004640705, "ncat25k": 48, "term": "process", "bg": 1.3563390974561347e-06, "y": 0.8679098679098679, "cat25k": 37}, {"ncat": 84, "s": 0.4467754467754468, "cat": 6, "x": 0.9316239316239316, "os": 0.17046744504689576, "ncat25k": 48, "term": "available", "bg": 6.425074000855636e-07, "y": 0.9090909090909091, "cat25k": 45}, {"ncat": 86, "s": 0.43356643356643354, "cat": 6, "x": 0.9324009324009324, "os": 0.16644035922144707, "ncat25k": 49, "term": "up", "bg": 3.2044829569700424e-07, "y": 0.9005439005439005, "cat25k": 45}, {"ncat": 86, "s": 0.710955710955711, "cat": 8, "x": 0.9331779331779332, "os": 0.5574924390631472, "ncat25k": 49, "term": "both", "bg": 1.1365311604606055e-06, "y": 0.9440559440559441, "cat25k": 59}, {"ncat": 88, "s": 0.21600621600621603, "cat": 4, "x": 0.933954933954934, "os": 0.1076895045614516, "ncat25k": 51, "term": "source", "bg": 1.621486823898024e-06, "y": 0.8259518259518259, "cat25k": 30}, {"ncat": 90, "s": 0.6993006993006993, "cat": 8, "x": 0.9347319347319347, "os": 0.5508550084906015, "ncat25k": 52, "term": "most", "bg": 6.581346917899512e-07, "y": 0.9417249417249417, "cat25k": 59}, {"ncat": 91, "s": 0.38539238539238546, "cat": 6, "x": 0.9355089355089355, "os": 0.15715129095497038, "ncat25k": 52, "term": "show", "bg": 1.106356706918279e-06, "y": 0.8966588966588966, "cat25k": 45}, {"ncat": 91, "s": 0.2937062937062937, "cat": 5, "x": 0.9362859362859363, "os": 0.13056853750542907, "ncat25k": 52, "term": "building", "bg": 2.3370106637562887e-06, "y": 0.857031857031857, "cat25k": 37}, {"ncat": 92, "s": 0.745920745920746, "cat": 10, "x": 0.9370629370629371, "os": 0.5817542689028017, "ncat25k": 53, "term": "there", "bg": 4.1922864564353083e-07, "y": 0.9572649572649573, "cat25k": 74}, {"ncat": 93, "s": 0.2851592851592852, "cat": 5, "x": 0.9378399378399378, "os": 0.127715392831881, "ncat25k": 53, "term": "/", "bg": 0.0, "y": 0.8578088578088578, "cat25k": 37}, {"ncat": 93, "s": 0.2851592851592852, "cat": 5, "x": 0.9386169386169386, "os": 0.127715392831881, "ncat25k": 53, "term": "build", "bg": 4.31966303110548e-06, "y": 0.8609168609168609, "cat25k": 37}, {"ncat": 95, "s": 0.7132867132867133, "cat": 9, "x": 0.9393939393939394, "os": 0.5602141131309066, "ncat25k": 55, "term": "other", "bg": 2.9838573247201815e-07, "y": 0.9487179487179487, "cat25k": 67}, {"ncat": 96, "s": 0.3620823620823621, "cat": 6, "x": 0.9401709401709402, "os": 0.14883592202313356, "ncat25k": 55, "term": "discuss", "bg": 6.647270025056746e-06, "y": 0.9230769230769231, "cat25k": 45}, {"ncat": 97, "s": 0.17793317793317795, "cat": 4, "x": 0.940947940947941, "os": 0.09755327127332142, "ncat25k": 56, "term": "way", "bg": 8.310611798938752e-07, "y": 0.8321678321678322, "cat25k": 30}, {"ncat": 99, "s": 0.26884226884226886, "cat": 5, "x": 0.9417249417249417, "os": 0.11985402135665196, "ncat25k": 57, "term": "code", "bg": 1.5737154561568826e-06, "y": 0.8616938616938616, "cat25k": 37}, {"ncat": 100, "s": 0.1732711732711733, "cat": 4, "x": 0.9425019425019425, "os": 0.09458410900756248, "ncat25k": 57, "term": "them", "bg": 7.491599869999408e-07, "y": 0.8096348096348096, "cat25k": 30}, {"ncat": 104, "s": 0.7342657342657343, "cat": 11, "x": 0.9432789432789432, "os": 0.5773304612646083, "ncat25k": 60, "term": "many", "bg": 1.0467456158564183e-06, "y": 0.9627039627039627, "cat25k": 82}, {"ncat": 104, "s": 0.30846930846930853, "cat": 6, "x": 0.9440559440559441, "os": 0.13720646714820428, "ncat25k": 60, "term": "based", "bg": 1.5602538865716413e-06, "y": 0.9020979020979021, "cat25k": 45}, {"ncat": 107, "s": 0.29759129759129765, "cat": 6, "x": 0.9448329448329449, "os": 0.1332974355719796, "ncat25k": 61, "term": "open", "bg": 1.701502007987142e-06, "y": 0.9067599067599068, "cat25k": 45}, {"ncat": 111, "s": 0.16472416472416473, "cat": 4, "x": 0.9456099456099456, "os": 0.08508397121223732, "ncat25k": 64, "term": "when", "bg": 5.163360822338973e-07, "y": 0.804972804972805, "cat25k": 30}, {"ncat": 111, "s": 0.8057498057498058, "cat": 15, "x": 0.9463869463869464, "os": 0.6195909873370917, "ncat25k": 64, "term": "models", "bg": 8.536049119656492e-06, "y": 0.9759129759129759, "cat25k": 111}, {"ncat": 113, "s": 0.21522921522921526, "cat": 5, "x": 0.9471639471639471, "os": 0.10478702328438583, "ncat25k": 65, "term": "make", "bg": 8.045361842851234e-07, "y": 0.8562548562548562, "cat25k": 37}, {"ncat": 113, "s": 0.7358197358197359, "cat": 12, "x": 0.947940947940948, "os": 0.5779766209571361, "ncat25k": 65, "term": "well", "bg": 9.663155272984771e-07, "y": 0.9673659673659674, "cat25k": 89}, {"ncat": 115, "s": 0.7754467754467754, "cat": 14, "x": 0.9487179487179487, "os": 0.6008926006210746, "ncat25k": 66, "term": "such", "bg": 1.0240421706237482e-06, "y": 0.9735819735819736, "cat25k": 104}, {"ncat": 116, "s": 0.35431235431235436, "cat": 7, "x": 0.9494949494949495, "os": 0.1436208167882868, "ncat25k": 67, "term": "like", "bg": 7.29782507278846e-07, "y": 0.933954933954934, "cat25k": 52}, {"ncat": 117, "s": 0.35275835275835277, "cat": 7, "x": 0.9502719502719502, "os": 0.14237320067722675, "ncat25k": 67, "term": "different", "bg": 2.378940260152236e-06, "y": 0.9316239316239316, "cat25k": 52}, {"ncat": 117, "s": 0.27195027195027194, "cat": 6, "x": 0.951048951048951, "os": 0.12172776778026034, "ncat25k": 67, "term": "through", "bg": 1.0628013740818817e-06, "y": 0.9013209013209014, "cat25k": 45}, {"ncat": 118, "s": 0.26961926961926963, "cat": 6, "x": 0.9518259518259519, "os": 0.1206796693052497, "ncat25k": 68, "term": "science", "bg": 2.913668233916192e-06, "y": 0.9028749028749029, "cat25k": 45}, {"ncat": 119, "s": 0.5112665112665112, "cat": 10, "x": 0.9526029526029526, "os": 0.20104600464539424, "ncat25k": 68, "term": "new", "bg": 2.7717321393780433e-07, "y": 0.958041958041958, "cat25k": 74}, {"ncat": 119, "s": 0.7047397047397048, "cat": 11, "x": 0.9533799533799534, "os": 0.5565573141219188, "ncat25k": 68, "term": "one", "bg": 3.542479116514484e-07, "y": 0.9634809634809635, "cat25k": 82}, {"ncat": 121, "s": 0.236985236985237, "cat": 6, "x": 0.9541569541569541, "os": 0.11764034479749286, "ncat25k": 70, "term": "work", "bg": 9.485184440996292e-07, "y": 0.898989898989899, "cat25k": 45}, {"ncat": 122, "s": 0.3076923076923077, "cat": 7, "x": 0.9549339549339549, "os": 0.13644444143625822, "ncat25k": 70, "term": "do", "bg": 4.5011432377989517e-07, "y": 0.9285159285159286, "cat25k": 52}, {"ncat": 125, "s": 0.7148407148407149, "cat": 12, "x": 0.9557109557109557, "os": 0.562209421069813, "ncat25k": 72, "term": "model", "bg": 4.239922958368308e-06, "y": 0.9665889665889665, "cat25k": 89}, {"ncat": 126, "s": 0.4467754467754468, "cat": 9, "x": 0.9564879564879565, "os": 0.17046744504689576, "ncat25k": 72, "term": "all", "bg": 1.7897953289588266e-07, "y": 0.9533799533799534, "cat25k": 67}, {"ncat": 127, "s": 0.4374514374514375, "cat": 9, "x": 0.9572649572649573, "os": 0.16910383511885202, "ncat25k": 73, "term": "tools", "bg": 2.606723234715177e-06, "y": 0.9494949494949495, "cat25k": 67}, {"ncat": 127, "s": 0.49961149961149964, "cat": 10, "x": 0.958041958041958, "os": 0.1882019624820765, "ncat25k": 73, "term": "they", "bg": 4.845237488262666e-07, "y": 0.9611499611499611, "cat25k": 74}, {"ncat": 131, "s": 0.1250971250971251, "cat": 4, "x": 0.9588189588189588, "os": 0.07193601126146221, "ncat25k": 75, "term": "their", "bg": 5.594103826827572e-07, "y": 0.8104118104118104, "cat25k": 30}, {"ncat": 134, "s": 0.46386946386946387, "cat": 10, "x": 0.9595959595959596, "os": 0.17822352958576887, "ncat25k": 77, "term": "time", "bg": 6.711974482340591e-07, "y": 0.9595959595959596, "cat25k": 74}, {"ncat": 135, "s": 0.34965034965034963, "cat": 8, "x": 0.9603729603729604, "os": 0.140995091103589, "ncat25k": 78, "term": "'s", "bg": 0.0, "y": 0.9393939393939394, "cat25k": 59}, {"ncat": 137, "s": 0.7241647241647242, "cat": 14, "x": 0.9611499611499611, "os": 0.5718535499309726, "ncat25k": 79, "term": "machine", "bg": 9.801687220021398e-06, "y": 0.9704739704739704, "cat25k": 104}, {"ncat": 139, "s": 0.5353535353535354, "cat": 12, "x": 0.9619269619269619, "os": 0.2066201899105818, "ncat25k": 80, "term": "has", "bg": 4.31941577371171e-07, "y": 0.9681429681429682, "cat25k": 89}, {"ncat": 140, "s": 0.3053613053613054, "cat": 8, "x": 0.9627039627039627, "os": 0.13587841343727408, "ncat25k": 80, "term": "learn", "bg": 3.219227516949832e-06, "y": 0.9425019425019425, "cat25k": 59}, {"ncat": 141, "s": 0.10334110334110334, "cat": 4, "x": 0.9634809634809635, "os": 0.0667737611155399, "ncat25k": 81, "term": "analysis", "bg": 5.261160109951211e-06, "y": 0.8041958041958042, "cat25k": 30}, {"ncat": 142, "s": 0.20435120435120438, "cat": 6, "x": 0.9642579642579643, "os": 0.0999941657037659, "ncat25k": 82, "term": "about", "bg": 3.9450566032972005e-07, "y": 0.8982128982128982, "cat25k": 45}, {"ncat": 144, "s": 0.3620823620823621, "cat": 9, "x": 0.965034965034965, "os": 0.14883592202313356, "ncat25k": 83, "term": "our", "bg": 7.708664762744603e-07, "y": 0.9518259518259519, "cat25k": 67}, {"ncat": 147, "s": 0.35975135975135974, "cat": 9, "x": 0.9658119658119658, "os": 0.1457490780543782, "ncat25k": 84, "term": "your", "bg": 3.151997348272153e-07, "y": 0.9502719502719502, "cat25k": 67}, {"ncat": 149, "s": 0.7125097125097125, "cat": 14, "x": 0.9665889665889665, "os": 0.5589819590508601, "ncat25k": 86, "term": "not", "bg": 1.837785098331453e-07, "y": 0.9712509712509713, "cat25k": 104}, {"ncat": 150, "s": 0.4584304584304585, "cat": 11, "x": 0.9673659673659674, "os": 0.1750862800114658, "ncat25k": 86, "term": "also", "bg": 7.391227702598925e-07, "y": 0.9619269619269619, "cat25k": 82}, {"ncat": 152, "s": 0.7187257187257188, "cat": 15, "x": 0.9681429681429682, "os": 0.5664187275161708, "ncat25k": 87, "term": "i", "bg": 3.3696833594830423e-07, "y": 0.9751359751359752, "cat25k": 111}, {"ncat": 152, "s": 0.34887334887334887, "cat": 9, "x": 0.9689199689199689, "os": 0.1408772368359391, "ncat25k": 87, "term": "into", "bg": 1.0731127980718686e-06, "y": 0.951048951048951, "cat25k": 67}, {"ncat": 152, "s": 0.7031857031857033, "cat": 14, "x": 0.9696969696969697, "os": 0.556027338574156, "ncat25k": 87, "term": "used", "bg": 1.2003150347789149e-06, "y": 0.9728049728049728, "cat25k": 104}, {"ncat": 154, "s": 0.3115773115773116, "cat": 9, "x": 0.9704739704739704, "os": 0.1390178090156678, "ncat25k": 89, "term": "what", "bg": 7.138343143000375e-07, "y": 0.947940947940948, "cat25k": 67}, {"ncat": 160, "s": 0.7676767676767677, "cat": 19, "x": 0.9712509712509713, "os": 0.5965892745497255, "ncat25k": 92, "term": "learning", "bg": 9.015970492817022e-06, "y": 0.9813519813519813, "cat25k": 141}, {"ncat": 164, "s": 0.5322455322455323, "cat": 14, "x": 0.972027972027972, "os": 0.20427857376904235, "ncat25k": 94, "term": "but", "bg": 5.759898936813255e-07, "y": 0.972027972027972, "cat25k": 104}, {"ncat": 167, "s": 0.44910644910644915, "cat": 12, "x": 0.9728049728049728, "os": 0.17150451179916903, "ncat25k": 96, "term": "these", "bg": 1.0570631674633178e-06, "y": 0.9658119658119658, "cat25k": 89}, {"ncat": 167, "s": 0.44910644910644915, "cat": 12, "x": 0.9735819735819736, "os": 0.17150451179916903, "ncat25k": 96, "term": "more", "bg": 3.3788823547590103e-07, "y": 0.9689199689199689, "cat25k": 89}, {"ncat": 168, "s": 0.714063714063714, "cat": 16, "x": 0.9743589743589743, "os": 0.5610070125915643, "ncat25k": 97, "term": "some", "bg": 1.158581000303388e-06, "y": 0.9774669774669774, "cat25k": 119}, {"ncat": 171, "s": 0.36985236985236986, "cat": 11, "x": 0.9751359751359752, "os": 0.15326026410514082, "ncat25k": 98, "term": "which", "bg": 7.993762762981334e-07, "y": 0.9642579642579643, "cat25k": 82}, {"ncat": 177, "s": 0.7156177156177157, "cat": 17, "x": 0.9759129759129759, "os": 0.562280644420073, "ncat25k": 102, "term": "have", "bg": 4.781628131696022e-07, "y": 0.9782439782439782, "cat25k": 126}, {"ncat": 183, "s": 0.29215229215229216, "cat": 10, "x": 0.9766899766899767, "os": 0.12984344687841642, "ncat25k": 105, "term": "you", "bg": 3.717919776519941e-07, "y": 0.9564879564879565, "cat25k": 74}, {"ncat": 190, "s": 0.3108003108003108, "cat": 11, "x": 0.9774669774669774, "os": 0.13769580675642085, "ncat25k": 109, "term": "at", "bg": 3.027648343855233e-07, "y": 0.965034965034965, "cat25k": 82}, {"ncat": 200, "s": 0.501942501942502, "cat": 16, "x": 0.9782439782439782, "os": 0.19125870390506977, "ncat25k": 115, "term": "or", "bg": 2.979707999014197e-07, "y": 0.9766899766899767, "cat25k": 119}, {"ncat": 217, "s": 0.703962703962704, "cat": 20, "x": 0.9790209790209791, "os": 0.5561242639908197, "ncat25k": 125, "term": "by", "bg": 2.674493601643144e-07, "y": 0.9836829836829837, "cat25k": 149}, {"ncat": 230, "s": 0.30458430458430463, "cat": 13, "x": 0.9797979797979798, "os": 0.13437741110127593, "ncat25k": 132, "term": "use", "bg": 1.2414971540246977e-06, "y": 0.9696969696969697, "cat25k": 97}, {"ncat": 249, "s": 0.7334887334887334, "cat": 26, "x": 0.9805749805749806, "os": 0.5752709994231655, "ncat25k": 143, "term": "using", "bg": 4.266099887226948e-06, "y": 0.9914529914529915, "cat25k": 193}, {"ncat": 267, "s": 0.43822843822843827, "cat": 19, "x": 0.9813519813519813, "os": 0.1698188232743661, "ncat25k": 153, "term": "an", "bg": 7.600181130625271e-07, "y": 0.9821289821289821, "cat25k": 141}, {"ncat": 272, "s": 0.3861693861693862, "cat": 18, "x": 0.9821289821289821, "os": 0.15773849149066133, "ncat25k": 156, "term": "can", "bg": 1.1718858490365816e-06, "y": 0.9805749805749806, "cat25k": 134}, {"ncat": 274, "s": 0.47863247863247865, "cat": 21, "x": 0.9829059829059829, "os": 0.18311248368710353, "ncat25k": 157, "term": "it", "bg": 4.955066407442759e-07, "y": 0.9844599844599845, "cat25k": 156}, {"ncat": 274, "s": 0.6884226884226885, "cat": 24, "x": 0.9836829836829837, "os": 0.5487509515140999, "ncat25k": 157, "term": "talk", "bg": 1.2044622550160776e-05, "y": 0.9891219891219891, "cat25k": 178}, {"ncat": 278, "s": 0.5058275058275059, "cat": 23, "x": 0.9844599844599845, "os": 0.1978920918939157, "ncat25k": 160, "term": "are", "bg": 6.082549211346904e-07, "y": 0.9875679875679876, "cat25k": 171}, {"ncat": 278, "s": 0.7117327117327118, "cat": 26, "x": 0.9852369852369852, "os": 0.5582914160739482, "ncat25k": 160, "term": "as", "bg": 6.771817885679573e-07, "y": 0.993006993006993, "cat25k": 193}, {"ncat": 282, "s": 0.494949494949495, "cat": 22, "x": 0.986013986013986, "os": 0.18644062334760259, "ncat25k": 162, "term": "python", "bg": 0.00010085127955907008, "y": 0.986013986013986, "cat25k": 163}, {"ncat": 282, "s": 0.3682983682983683, "cat": 18, "x": 0.9867909867909868, "os": 0.1520551505478948, "ncat25k": 162, "term": "from", "bg": 5.949781984806026e-07, "y": 0.9797979797979798, "cat25k": 134}, {"ncat": 304, "s": 0.4755244755244755, "cat": 23, "x": 0.9875679875679876, "os": 0.1807244064613941, "ncat25k": 175, "term": "be", "bg": 6.411425416883397e-07, "y": 0.9867909867909868, "cat25k": 171}, {"ncat": 309, "s": 0.43900543900543904, "cat": 22, "x": 0.9883449883449883, "os": 0.1699069840531362, "ncat25k": 178, "term": "we", "bg": 1.8277502995157498e-06, "y": 0.9852369852369852, "cat25k": 163}, {"ncat": 312, "s": 0.30846930846930853, "cat": 18, "x": 0.9891219891219891, "os": 0.13720646714820428, "ncat25k": 179, "term": "how", "bg": 3.01066559758316e-06, "y": 0.9790209790209791, "cat25k": 134}, {"ncat": 313, "s": 0.4794094794094795, "cat": 24, "x": 0.98989898989899, "os": 0.1831973434978073, "ncat25k": 180, "term": "that", "bg": 5.158596575438194e-07, "y": 0.9883449883449883, "cat25k": 178}, {"ncat": 337, "s": 0.35042735042735046, "cat": 20, "x": 0.9906759906759907, "os": 0.14120772254929675, "ncat25k": 194, "term": "on", "bg": 4.6553275522114204e-07, "y": 0.9829059829059829, "cat25k": 149}, {"ncat": 344, "s": 0.45454545454545453, "cat": 25, "x": 0.9914529914529915, "os": 0.17348836286381236, "ncat25k": 198, "term": "will", "bg": 1.6558386321054247e-06, "y": 0.98989898989899, "cat25k": 186}, {"ncat": 360, "s": 0.5066045066045066, "cat": 30, "x": 0.9922299922299922, "os": 0.19934680645802766, "ncat25k": 207, "term": "is", "bg": 5.614273397694921e-07, "y": 0.9953379953379954, "cat25k": 223}, {"ncat": 365, "s": 0.3916083916083916, "cat": 25, "x": 0.993006993006993, "os": 0.16335178781668047, "ncat25k": 210, "term": "data", "bg": 1.0102499388813528e-05, "y": 0.9906759906759907, "cat25k": 186}, {"ncat": 370, "s": 0.4366744366744367, "cat": 26, "x": 0.9937839937839937, "os": 0.1676592650875341, "ncat25k": 213, "term": "with", "bg": 7.043161890877233e-07, "y": 0.9922299922299922, "cat25k": 193}, {"ncat": 386, "s": 0.47241647241647244, "cat": 29, "x": 0.9945609945609946, "os": 0.1794427779729733, "ncat25k": 222, "term": "this", "bg": 7.037130908008734e-07, "y": 0.9937839937839937, "cat25k": 215}, {"ncat": 393, "s": 0.4778554778554779, "cat": 30, "x": 0.9953379953379954, "os": 0.18236905207905496, "ncat25k": 226, "term": "for", "bg": 4.975190795655549e-07, "y": 0.9945609945609946, "cat25k": 223}, {"ncat": 417, "s": 0.46231546231546233, "cat": 31, "x": 0.9961149961149961, "os": 0.17752902749312333, "ncat25k": 240, "term": "in", "bg": 5.192735268336555e-07, "y": 0.9968919968919969, "cat25k": 230}, {"ncat": 427, "s": 0.4358974358974359, "cat": 30, "x": 0.9968919968919969, "os": 0.16762857720481594, "ncat25k": 245, "term": "a", "bg": 6.153307365225168e-07, "y": 0.9961149961149961, "cat25k": 223}, {"ncat": 443, "s": 0.4630924630924631, "cat": 33, "x": 0.9976689976689976, "os": 0.1778966565111234, "ncat25k": 255, "term": "of", "bg": 5.098819573975981e-07, "y": 0.9992229992229992, "cat25k": 245}, {"ncat": 447, "s": 0.45998445998446, "cat": 33, "x": 0.9984459984459985, "os": 0.17627990679550526, "ncat25k": 257, "term": "to", "bg": 6.090415903047885e-07, "y": 0.9984459984459985, "cat25k": 245}, {"ncat": 449, "s": 0.4397824397824398, "cat": 32, "x": 0.9992229992229992, "os": 0.17008173612879957, "ncat25k": 258, "term": "the", "bg": 4.438110124251134e-07, "y": 0.9976689976689976, "cat25k": 238}, {"ncat": 450, "s": 0.4584304584304585, "cat": 33, "x": 1.0, "os": 0.1750862800114658, "ncat25k": 259, "term": "and", "bg": 6.32570550755669e-07, "y": 1.0, "cat25k": 245}], "info": {"not_category_terms": ["scientists", "cases", "packages", "case", "community", "visualization", "creating", "become", "matplotlib", "database"], "category_internal_name": "Experienced", "category_name": "Experienced", "not_category_name": "Not Experienced", "category_terms": ["scientists", "cases", "packages", "case", "community", "visualization", "creating", "become", "matplotlib", "database"]}}; }
buildViz(1000,undefined,null,null,true,true,false,false,false,true,false,false,true,0.05,false,undefined,undefined);
</script>

